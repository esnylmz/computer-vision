{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5cb0bd3",
   "metadata": {},
   "source": [
    "# üéπ Automatic Piano Fingering Detection from Video\n",
    "\n",
    "**Master's Thesis Project ‚Äì Sapienza University of Rome**\n",
    "\n",
    "---\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "Given a video of a piano performance with synchronized MIDI data, automatically determine the finger assignment (1‚Äì5, thumb to pinky) for each played note.\n",
    "\n",
    "**Input**: Video + MIDI ‚Üí **Output**: Per-note finger labels (L1‚ÄìL5 for left hand, R1‚ÄìR5 for right hand)\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "Video ‚Üí Keyboard Detection ‚Üí Hand Processing ‚Üí Finger-Key Assignment ‚Üí Neural Refinement ‚Üí Fingering Labels\n",
    "         (OpenCV)            (MediaPipe)       (Gaussian Prob.)         (BiLSTM)\n",
    "```\n",
    "\n",
    "| Stage | Method | Input | Output |\n",
    "|-------|--------|-------|--------|\n",
    "| 1. Keyboard Detection | Corner annotations + Homography | Video frame | 88 key bounding boxes |\n",
    "| 2. Hand Processing | Hampel + SavGol filters | MediaPipe skeleton JSON | Filtered landmarks (T√ó21√ó3) |\n",
    "| 3. Finger Assignment | Gaussian probability | MIDI events + fingertips + keys | FingerAssignment per note |\n",
    "| 4. Neural Refinement | BiLSTM + Attention | Initial assignments | Refined predictions |\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**PianoVAM** (KAIST) ‚Äì [HuggingFace](https://huggingface.co/datasets/PianoVAM/PianoVAM_v1.0)\n",
    "- 107 piano performances with synchronized video, audio, MIDI\n",
    "- Pre-extracted 21-keypoint hand skeletons (MediaPipe)\n",
    "- Multiple skill levels: Beginner, Intermediate, Advanced\n",
    "- Top-view camera angle (1920√ó1080, 60fps)\n",
    "- Metadata loaded from `metadata_v1.json`; individual files downloaded on demand\n",
    "\n",
    "| Split | Samples |\n",
    "|-------|---------|\n",
    "| Train | 73 |\n",
    "| Validation | 19 |\n",
    "| Test | 14 |\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "0. [Environment Setup](#0)\n",
    "1. [Data Exploration](#1)\n",
    "2. [Stage 1: Keyboard Detection](#2)\n",
    "3. [Stage 2: Hand Processing](#3)\n",
    "4. [Stage 3: Finger-Key Assignment](#4)\n",
    "5. [Baseline Pipeline on Multiple Samples](#5)\n",
    "6. [Stage 4: Neural Refinement (BiLSTM)](#6)\n",
    "7. [Evaluation & Results](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa42beb",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='0'></a>\n",
    "## 0. Environment Setup\n",
    "\n",
    "Clone the repository and install dependencies. This cell handles both Colab and local setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92023f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 0.1 - Environment Setup\n",
    "# ==============================================================================\n",
    "import os, sys, subprocess\n",
    "\n",
    "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone repository\n",
    "    if not os.path.exists('computer-vision'):\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/esnylmz/computer-vision.git'], check=True)\n",
    "    os.chdir('computer-vision')\n",
    "\n",
    "    # Install the project package + extra deps\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-e', '.'], check=True)\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'torchcodec'], check=True)\n",
    "\n",
    "    print(\"\\n‚úÖ Colab environment ready\")\n",
    "else:\n",
    "    # Local development ‚Äì make sure project root is on path\n",
    "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    if PROJECT_ROOT not in sys.path:\n",
    "        sys.path.insert(0, PROJECT_ROOT)\n",
    "    print(\"‚úÖ Local environment ready\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json, time, warnings\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"NumPy  : {np.__version__}\")\n",
    "print(f\"Pandas : {pd.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device : {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ad697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 0.2 - Import Project Modules\n",
    "# ==============================================================================\n",
    "from src.data.dataset import PianoVAMDataset, PianoVAMSample\n",
    "from src.data.midi_utils import MidiProcessor, MidiEvent\n",
    "from src.data.video_utils import VideoProcessor\n",
    "from src.utils.config import load_config, Config\n",
    "\n",
    "from src.keyboard.detector import KeyboardDetector, KeyboardRegion\n",
    "from src.keyboard.homography import HomographyComputer\n",
    "from src.keyboard.key_localization import KeyLocalizer\n",
    "\n",
    "from src.hand.skeleton_loader import SkeletonLoader, HandLandmarks\n",
    "from src.hand.temporal_filter import TemporalFilter\n",
    "from src.hand.fingertip_extractor import FingertipExtractor, FingertipData\n",
    "\n",
    "from src.assignment.gaussian_assignment import GaussianFingerAssigner, FingerAssignment\n",
    "from src.assignment.midi_sync import MidiVideoSync\n",
    "from src.assignment.hand_separation import HandSeparator\n",
    "\n",
    "from src.refinement.model import FingeringRefiner, FeatureExtractor, SequenceDataset\n",
    "from src.refinement.constraints import BiomechanicalConstraints\n",
    "from src.refinement.train import train_refiner, collate_fn\n",
    "\n",
    "from src.evaluation.metrics import FingeringMetrics, EvaluationResult, aggregate_results\n",
    "from src.evaluation.visualization import ResultVisualizer\n",
    "\n",
    "from src.pipeline import FingeringPipeline\n",
    "\n",
    "# Load configuration (use colab config when on Colab)\n",
    "config_path = 'configs/colab.yaml' if IN_COLAB else 'configs/default.yaml'\n",
    "config = load_config(config_path)\n",
    "print(f\"‚úÖ All modules imported  |  Config: {config_path}\")\n",
    "print(f\"   Project: {config.project_name} v{config.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d89d8",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='1'></a>\n",
    "## 1. Data Exploration\n",
    "\n",
    "Load the PianoVAM dataset from HuggingFace and explore its structure.\n",
    "\n",
    "**How it works**: We download `metadata_v1.json` (~15 KB) from the HuggingFace repo, which\n",
    "contains all sample metadata (composer, piece, skill level, keyboard corners, etc.).\n",
    "Individual files (skeleton JSON, TSV annotations, video, audio) are downloaded on demand.\n",
    "No bulk download of the full dataset (~7 GB) is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c443c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1.1 - Load Dataset Splits\n",
    "# ==============================================================================\n",
    "# PianoVAMDataset downloads metadata_v1.json (~15 KB) from HuggingFace,\n",
    "# then filters by the 'split' column:\n",
    "#   train: 73  |  valid: 19  |  test: 14\n",
    "# No audio/video is downloaded here ‚Äî only metadata.\n",
    "# ==============================================================================\n",
    "\n",
    "MAX_EXPLORE = 5  # limit for quick exploration (keep small on Colab)\n",
    "\n",
    "print(\"Loading PianoVAM dataset splits (streaming mode)...\\n\")\n",
    "\n",
    "train_dataset = PianoVAMDataset(split='train', streaming=True, max_samples=MAX_EXPLORE)\n",
    "print(\"  ‚úÖ Train split ready\")\n",
    "\n",
    "val_dataset = PianoVAMDataset(split='validation', streaming=True, max_samples=MAX_EXPLORE)\n",
    "print(\"  ‚úÖ Validation split ready\")\n",
    "\n",
    "test_dataset = PianoVAMDataset(split='test', streaming=True, max_samples=MAX_EXPLORE)\n",
    "print(\"  ‚úÖ Test split ready\")\n",
    "\n",
    "print(\"\\nAll splits loaded (streaming ‚Äì no bulk download).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d105e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1.2 - Verify & Preview Samples\n",
    "# ==============================================================================\n",
    "\n",
    "for name, ds in [('Train', train_dataset), ('Validation', val_dataset), ('Test', test_dataset)]:\n",
    "    try:\n",
    "        s = next(iter(ds))\n",
    "        print(f\"‚úì {name:12s} | ID={s.id:>4s}  Composer={s.metadata['composer']:<20s}  \"\n",
    "              f\"Piece={s.metadata['piece']:<30s}  Skill={s.metadata['skill_level']}\")\n",
    "    except StopIteration:\n",
    "        print(f\"‚úó {name:12s} | EMPTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14945e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1.3 - Explore a Single Sample in Detail\n",
    "# ==============================================================================\n",
    "\n",
    "sample = next(iter(train_dataset))\n",
    "\n",
    "print(f\"Sample ID      : {sample.id}\")\n",
    "print(f\"Composer       : {sample.metadata['composer']}\")\n",
    "print(f\"Piece          : {sample.metadata['piece']}\")\n",
    "print(f\"Performer      : {sample.metadata['performer']}\")\n",
    "print(f\"Skill Level    : {sample.metadata['skill_level']}\")\n",
    "print(f\"Duration       : {sample.metadata.get('duration', 'N/A')}s\")\n",
    "print(f\"\\nKeyboard Corners: {sample.metadata['keyboard_corners']}\")\n",
    "print(f\"\\nFile URLs:\")\n",
    "print(f\"  Video    : {sample.video_path[:90]}...\")\n",
    "print(f\"  MIDI     : {sample.midi_path[:90]}...\")\n",
    "print(f\"  Skeleton : {sample.skeleton_path[:90]}...\")\n",
    "print(f\"  TSV      : {sample.tsv_path[:90]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ae2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1.4 - Dataset Statistics (Full Train Split)\n",
    "# ==============================================================================\n",
    "# Load all 73 train samples (metadata only ‚Äì no audio/video download).\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Collecting dataset statistics from full train split ...\")\n",
    "stats_ds = PianoVAMDataset(split='train', max_samples=None)\n",
    "\n",
    "composers, skill_levels, pieces = [], [], []\n",
    "for s in stats_ds:\n",
    "    composers.append(s.metadata['composer'])\n",
    "    skill_levels.append(s.metadata['skill_level'])\n",
    "    pieces.append(s.metadata['piece'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Skill level distribution\n",
    "pd.Series(skill_levels).value_counts().plot.bar(ax=axes[0], color='steelblue')\n",
    "axes[0].set_title(f'Skill Level Distribution (Train, n={len(skill_levels)})')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Composer distribution (top 10)\n",
    "comp_counts = pd.Series(composers).value_counts().head(10)\n",
    "comp_counts.plot.barh(ax=axes[1], color='darkorange')\n",
    "axes[1].set_title('Top 10 Composers (Train)')\n",
    "axes[1].set_xlabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrain samples    : {len(skill_levels)}\")\n",
    "print(f\"Unique composers : {len(set(composers))}\")\n",
    "print(f\"Unique pieces    : {len(set(pieces))}\")\n",
    "print(f\"Skill levels     : {dict(pd.Series(skill_levels).value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407dec8d",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='2'></a>\n",
    "## 2. Stage 1 ‚Äì Keyboard Detection\n",
    "\n",
    "**Goal**: Detect piano key boundaries from corner annotations.\n",
    "\n",
    "PianoVAM provides 4-point corner annotations (`Point_LT`, `Point_RT`, `Point_RB`, `Point_LB`). We use these to:\n",
    "1. Compute a homography matrix for perspective normalization.\n",
    "2. Map all 88 keys to pixel bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2.1 - Detect Keyboard from Corner Annotations\n",
    "# ==============================================================================\n",
    "\n",
    "sample = next(iter(train_dataset))\n",
    "corners = sample.metadata['keyboard_corners']\n",
    "print(f\"Sample: {sample.id}  |  Corners: {corners}\")\n",
    "\n",
    "# Initialize detector\n",
    "detector = KeyboardDetector({\n",
    "    'canny_low': config.keyboard.canny_low,\n",
    "    'canny_high': config.keyboard.canny_high,\n",
    "    'hough_threshold': config.keyboard.hough_threshold\n",
    "})\n",
    "\n",
    "# Use corner-based detection (preferred when annotations are available)\n",
    "keyboard_region = detector.detect_from_corners(corners)\n",
    "\n",
    "print(f\"\\n‚úÖ Keyboard detected\")\n",
    "print(f\"   Bounding box    : {keyboard_region.bbox}\")\n",
    "print(f\"   Keys localised  : {len(keyboard_region.key_boundaries)}\")\n",
    "print(f\"   White key width : {keyboard_region.white_key_width:.1f} px\")\n",
    "print(f\"   Black key width : {keyboard_region.black_key_width:.1f} px\")\n",
    "print(f\"   Homography shape: {keyboard_region.homography.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2.2 - Visualise Key Boundaries\n",
    "# ==============================================================================\n",
    "\n",
    "localizer = KeyLocalizer(keyboard_region.key_boundaries)\n",
    "\n",
    "# Gather key info\n",
    "white_keys = localizer.get_white_keys()\n",
    "black_keys = localizer.get_black_keys()\n",
    "\n",
    "print(f\"White keys: {len(white_keys)}  |  Black keys: {len(black_keys)}\")\n",
    "\n",
    "# Plot key layout\n",
    "fig, ax = plt.subplots(figsize=(18, 3))\n",
    "\n",
    "for ki in white_keys:\n",
    "    x1, y1, x2, y2 = ki.bbox\n",
    "    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=0.8,\n",
    "                          edgecolor='black', facecolor='white')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "for ki in black_keys:\n",
    "    x1, y1, x2, y2 = ki.bbox\n",
    "    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=0.5,\n",
    "                          edgecolor='black', facecolor='#333333')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# Mark some reference keys\n",
    "for note_name in ['A0', 'C4', 'C8']:\n",
    "    ki = localizer.get_key_by_name(note_name)\n",
    "    if ki:\n",
    "        ax.annotate(ki.note_name, xy=ki.center, fontsize=7, color='red',\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlim(keyboard_region.bbox[0] - 10,\n",
    "            keyboard_region.bbox[2] + 10)\n",
    "ax.set_ylim(keyboard_region.bbox[3] + 10,\n",
    "            keyboard_region.bbox[1] - 10)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(f'Keyboard Layout ‚Äì 88 Keys (Sample {sample.id})')\n",
    "ax.set_xlabel('x (pixels)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2.3 - Homography Demonstration\n",
    "# ==============================================================================\n",
    "\n",
    "homography_computer = HomographyComputer()\n",
    "\n",
    "# If real corners are available, compute homography\n",
    "if keyboard_region.corners:\n",
    "    H = homography_computer.compute_from_corners(keyboard_region.corners)\n",
    "    print(f\"Homography matrix:\\n{H}\")\n",
    "\n",
    "    # Show how a point transforms\n",
    "    test_point = keyboard_region.corners.get('LT', (0, 0))\n",
    "    warped = homography_computer.warp_point(test_point, H)\n",
    "    print(f\"\\nCorner LT {test_point} -> normalised {warped}\")\n",
    "else:\n",
    "    print(\"Corner annotations not parsed as tuples; using homography from bbox.\")\n",
    "    H = keyboard_region.homography\n",
    "    print(f\"Homography matrix:\\n{H}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee17f0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='3'></a>\n",
    "## 3. Stage 2 ‚Äì Hand Processing\n",
    "\n",
    "**Goal**: Load pre-extracted MediaPipe 21-keypoint skeletons, apply temporal filtering, extract fingertip positions.\n",
    "\n",
    "Filtering pipeline:\n",
    "1. **Hampel filter** (window=20) ‚Äì outlier detection via Median Absolute Deviation\n",
    "2. **Linear interpolation** ‚Äì fill gaps < 30 frames\n",
    "3. **Savitzky-Golay filter** (window=11, order=3) ‚Äì smooth high-frequency noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3.1 - Download & Load Skeleton Data\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"Downloading skeleton for sample {sample.id} ...\")\n",
    "skeleton_data = train_dataset.load_skeleton(sample)\n",
    "\n",
    "# Inspect structure\n",
    "print(f\"\\nSkeleton data type : {type(skeleton_data).__name__}\")\n",
    "if isinstance(skeleton_data, dict):\n",
    "    print(f\"Top-level keys     : {list(skeleton_data.keys())[:10]}\")\n",
    "elif isinstance(skeleton_data, list):\n",
    "    print(f\"Number of frames   : {len(skeleton_data)}\")\n",
    "    if skeleton_data:\n",
    "        print(f\"First frame keys   : {list(skeleton_data[0].keys()) if isinstance(skeleton_data[0], dict) else type(skeleton_data[0]).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12434352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3.2 - Parse Skeleton into Arrays\n",
    "# ==============================================================================\n",
    "\n",
    "loader = SkeletonLoader()\n",
    "hands = loader._parse_json(skeleton_data)\n",
    "\n",
    "print(f\"Left  hand frames with data: {len(hands['left'])}\")\n",
    "print(f\"Right hand frames with data: {len(hands['right'])}\")\n",
    "\n",
    "left_raw  = loader.to_array(hands['left'])\n",
    "right_raw = loader.to_array(hands['right'])\n",
    "\n",
    "print(f\"\\nLeft  landmarks shape : {left_raw.shape}\")\n",
    "print(f\"Right landmarks shape : {right_raw.shape}\")\n",
    "\n",
    "# Count valid frames (non-NaN) ‚Äì guard against empty arrays\n",
    "if left_raw.size > 0:\n",
    "    left_valid = int(np.sum(~np.any(np.isnan(left_raw.reshape(len(left_raw), -1)), axis=1)))\n",
    "else:\n",
    "    left_valid = 0\n",
    "\n",
    "if right_raw.size > 0:\n",
    "    right_valid = int(np.sum(~np.any(np.isnan(right_raw.reshape(len(right_raw), -1)), axis=1)))\n",
    "else:\n",
    "    right_valid = 0\n",
    "\n",
    "print(f\"\\nValid frames ‚Äì Left: {left_valid}  Right: {right_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ac303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3.3 - Temporal Filtering\n",
    "# ==============================================================================\n",
    "\n",
    "temporal_filter = TemporalFilter(\n",
    "    hampel_window=config.hand.hampel_window,\n",
    "    hampel_threshold=config.hand.hampel_threshold,\n",
    "    max_interpolation_gap=config.hand.interpolation_max_gap,\n",
    "    savgol_window=config.hand.savgol_window,\n",
    "    savgol_order=config.hand.savgol_order\n",
    ")\n",
    "\n",
    "print(\"Applying temporal filtering ...\")\n",
    "if left_raw.size > 0 and left_raw.shape[0] > 0:\n",
    "    left_filtered = temporal_filter.process(left_raw)\n",
    "    print(f\"  Left  hand filtered: {left_filtered.shape}\")\n",
    "else:\n",
    "    left_filtered = left_raw\n",
    "    print(\"  Left  hand: no data to filter\")\n",
    "\n",
    "if right_raw.size > 0 and right_raw.shape[0] > 0:\n",
    "    right_filtered = temporal_filter.process(right_raw)\n",
    "    print(f\"  Right hand filtered: {right_filtered.shape}\")\n",
    "else:\n",
    "    right_filtered = right_raw\n",
    "    print(\"  Right hand: no data to filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3.4 - Visualise Filtering Effect (Index Fingertip X-Coordinate)\n",
    "# ==============================================================================\n",
    "\n",
    "# Pick the hand with more data\n",
    "if right_raw.shape[0] >= left_raw.shape[0] and right_raw.size > 0:\n",
    "    raw_data, filt_data, hand_label = right_raw, right_filtered, 'Right'\n",
    "else:\n",
    "    raw_data, filt_data, hand_label = left_raw, left_filtered, 'Left'\n",
    "\n",
    "LM_IDX = 8  # Index finger tip\n",
    "COORD  = 0  # X-coordinate\n",
    "MAX_FRAMES = min(600, len(raw_data))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "frames = np.arange(MAX_FRAMES)\n",
    "ax.plot(frames, raw_data[:MAX_FRAMES, LM_IDX, COORD],\n",
    "        alpha=0.4, label='Raw', linewidth=0.8)\n",
    "ax.plot(frames, filt_data[:MAX_FRAMES, LM_IDX, COORD],\n",
    "        label='Filtered', linewidth=1.2)\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('X position (px)')\n",
    "ax.set_title(f'{hand_label} Hand ‚Äì Index Fingertip (landmark 8) X-Coordinate')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9bcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3.5 - Fingertip Extraction\n",
    "# ==============================================================================\n",
    "\n",
    "extractor = FingertipExtractor()\n",
    "\n",
    "right_fingertips = extractor.extract_sequence(right_filtered, hand_type='right')\n",
    "left_fingertips  = extractor.extract_sequence(left_filtered, hand_type='left')\n",
    "\n",
    "print(f\"Right fingertip data points : {len(right_fingertips)}\")\n",
    "print(f\"Left  fingertip data points : {len(left_fingertips)}\")\n",
    "\n",
    "# Show one frame\n",
    "if right_fingertips:\n",
    "    ft = right_fingertips[0]\n",
    "    print(f\"\\nSample fingertip data (frame {ft.frame_idx}, {ft.hand_type} hand):\")\n",
    "    finger_names = {1: 'Thumb', 2: 'Index', 3: 'Middle', 4: 'Ring', 5: 'Pinky'}\n",
    "    for fnum, pos in ft.positions.items():\n",
    "        print(f\"  {finger_names[fnum]:6s} : ({pos[0]:.1f}, {pos[1]:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ffd6b",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='4'></a>\n",
    "## 4. Stage 3 ‚Äì Finger-Key Assignment\n",
    "\n",
    "**Goal**: Synchronise MIDI note events with video frames and assign fingers to pressed keys using a Gaussian probability model.\n",
    "\n",
    "$$P(\\text{finger}_i \\rightarrow \\text{key}_k) = \\exp\\left(-\\frac{(\\Delta x_{ik})^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "where $\\Delta x_{ik}$ is the **horizontal (x-only) pixel distance** from fingertip $i$ to key centre $k$.\n",
    "\n",
    "> **Why pixel space & x-only?** The keyboard homography correctly maps the key *surface* but distorts hand landmarks floating *above* it (parallax). We avoid this by projecting key centres back to image-pixel space via $H^{-1}$ and comparing with landmarks scaled to pixels. The y-axis measures key depth and varies systematically with finger length, so only the x-component (along the keyboard) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aefb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4.1 - Load & Parse MIDI / TSV Annotations\n",
    "# ==============================================================================\n",
    "\n",
    "# Download TSV (contains onset, key_offset, frame_offset, note, velocity)\n",
    "print(f\"Downloading TSV annotations for sample {sample.id} ...\")\n",
    "tsv_df = train_dataset.load_tsv_annotations(sample)\n",
    "\n",
    "print(f\"\\nTSV shape: {tsv_df.shape}\")\n",
    "print(tsv_df.head(10))\n",
    "\n",
    "# Convert to MIDI event dicts for our pipeline\n",
    "midi_events = []\n",
    "for _, row in tsv_df.iterrows():\n",
    "    midi_events.append({\n",
    "        'onset': float(row['onset']),\n",
    "        'offset': float(row['onset']) + 0.3,  # approximate offset\n",
    "        'pitch': int(row['note']),\n",
    "        'velocity': int(row['velocity']) if 'velocity' in row and pd.notna(row['velocity']) else 64\n",
    "    })\n",
    "\n",
    "print(f\"\\nTotal MIDI events: {len(midi_events)}\")\n",
    "print(f\"Pitch range: {min(e['pitch'] for e in midi_events)} ‚Äì {max(e['pitch'] for e in midi_events)}\")\n",
    "print(f\"Time range : {midi_events[0]['onset']:.2f}s ‚Äì {midi_events[-1]['onset']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4.2 - MIDI-Video Synchronisation\n",
    "# ==============================================================================\n",
    "\n",
    "midi_sync = MidiVideoSync(fps=config.video_fps)\n",
    "synced_events = midi_sync.sync_events(midi_events)\n",
    "\n",
    "print(f\"Synced events: {len(synced_events)}\")\n",
    "print(f\"\\nFirst 5 events:\")\n",
    "for ev in synced_events[:5]:\n",
    "    print(f\"  Frame {ev.frame_idx:>5d}  |  {ev.note_name:4s}  \"\n",
    "          f\"(MIDI {ev.midi_pitch})  |  key_idx {ev.key_idx}  |  vel {ev.velocity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4.3 - Gaussian Finger Assignment\n",
    "# ==============================================================================\n",
    "# STRATEGY: work in IMAGE-PIXEL space, NOT keyboard-homography space.\n",
    "#\n",
    "# The homography correctly maps points on the keyboard SURFACE but distorts\n",
    "# hand landmarks that float ABOVE it (parallax error).  This caused the\n",
    "# thumb (shortest finger) to appear closest to every key in the warped view.\n",
    "#\n",
    "# Fix:\n",
    "#   1. Project key centres from warped space BACK to pixel space (H‚Åª¬π)\n",
    "#   2. Scale landmarks from [0,1] to pixels (simple multiplication)\n",
    "#   3. Compare in pixel space using x-distance only\n",
    "#\n",
    "# Why x-only?  In a top-down piano view the y-axis measures depth into the\n",
    "# keyboard.  It varies with finger length and carries no useful signal.\n",
    "# œÉ auto-scales to the mean key width when config.assignment.sigma is None.\n",
    "# ==============================================================================\n",
    "\n",
    "FRAME_W, FRAME_H = 1920, 1080  # PianoVAM video resolution\n",
    "\n",
    "\n",
    "def project_keys_to_pixel_space(key_boundaries_warped, homography):\n",
    "    \"\"\"\n",
    "    Project key bounding boxes from warped (rectified) space back to\n",
    "    original image-pixel space using the inverse homography.\n",
    "\n",
    "    The inverse transform is exact for key centres (which lie ON the\n",
    "    keyboard plane), so pixel-space distances between keys and hands\n",
    "    are free of the parallax bias that affects the forward warp.\n",
    "    \"\"\"\n",
    "    H_inv = np.linalg.inv(homography)\n",
    "\n",
    "    result = {}\n",
    "    for k, (x1, y1, x2, y2) in key_boundaries_warped.items():\n",
    "        cy = (y1 + y2) / 2.0\n",
    "        # Three points at key mid-height: left edge, right edge, centre\n",
    "        pts_w = np.array([[x1, cy, 1.0],\n",
    "                          [x2, cy, 1.0],\n",
    "                          [(x1 + x2) / 2.0, cy, 1.0]], dtype=np.float64)\n",
    "        pts_p = (H_inv @ pts_w.T).T           # (3, 3)\n",
    "        pts_p = pts_p[:, :2] / pts_p[:, 2:3]  # dehomogenise\n",
    "\n",
    "        lx, rx = pts_p[0, 0], pts_p[1, 0]\n",
    "        cy_px  = pts_p[2, 1]\n",
    "        result[k] = (lx, cy_px - 5.0, rx, cy_px + 5.0)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- Project key boundaries into pixel space ---\n",
    "key_boundaries_px = project_keys_to_pixel_space(\n",
    "    keyboard_region.key_boundaries, keyboard_region.homography\n",
    ")\n",
    "\n",
    "# --- Scale landmarks from [0,1] ‚Üí pixels (NO homography warp!) ---\n",
    "left_px  = left_filtered.copy()\n",
    "left_px[:, :, 0] *= FRAME_W\n",
    "left_px[:, :, 1] *= FRAME_H\n",
    "\n",
    "right_px = right_filtered.copy()\n",
    "right_px[:, :, 0] *= FRAME_W\n",
    "right_px[:, :, 1] *= FRAME_H\n",
    "\n",
    "# --- Initialise assigner with pixel-space keys ---\n",
    "assigner = GaussianFingerAssigner(\n",
    "    key_boundaries=key_boundaries_px,\n",
    "    sigma=config.assignment.sigma,          # None ‚Üí auto-scale to key width\n",
    "    candidate_range=config.assignment.candidate_keys\n",
    ")\n",
    "\n",
    "# Sanity check: coordinate ranges\n",
    "key_xs = [c[0] for c in assigner.key_centers.values()]\n",
    "print(f\"Key centres  x: [{min(key_xs):.0f} .. {max(key_xs):.0f}] px\")\n",
    "if right_px.size > 0:\n",
    "    print(f\"Right hand   x: [{np.nanmin(right_px[:,:,0]):.0f} .. {np.nanmax(right_px[:,:,0]):.0f}] px\")\n",
    "if left_px.size > 0:\n",
    "    print(f\"Left  hand   x: [{np.nanmin(left_px[:,:,0]):.0f} .. {np.nanmax(left_px[:,:,0]):.0f}] px\")\n",
    "print(f\"œÉ (auto)     : {assigner.sigma:.1f} px  (‚âà 1 key width in image)\")\n",
    "print(f\"Distance mode: x-only (horizontal along keyboard)\")\n",
    "print(f\"‚Üí Both keys & hands in image-pixel space (no warp on hands).\\n\")\n",
    "\n",
    "# Keyboard center for hand separation (median key x in pixel space)\n",
    "key_cx_sorted = sorted(assigner.key_centers.values(), key=lambda c: c[0])\n",
    "keyboard_center_x = key_cx_sorted[len(key_cx_sorted) // 2][0]\n",
    "\n",
    "# Run assignment on all synced events\n",
    "assignments = []\n",
    "skipped = 0\n",
    "\n",
    "for event in synced_events:\n",
    "    frame_idx = event.frame_idx\n",
    "    key_idx = event.key_idx\n",
    "\n",
    "    if key_idx not in assigner.key_centers:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    key_center_x = assigner.key_centers[key_idx][0]\n",
    "\n",
    "    # Determine hand by key position (left half = left hand, right half = right hand)\n",
    "    assignment = None\n",
    "\n",
    "    if key_center_x >= keyboard_center_x:  # Right side ‚Üí try right hand first\n",
    "        if frame_idx < len(right_px):\n",
    "            lm = right_px[frame_idx]\n",
    "            if not np.any(np.isnan(lm)):\n",
    "                assignment = assigner.assign_from_landmarks(\n",
    "                    lm, key_idx, 'right', frame_idx, event.onset_time)\n",
    "\n",
    "    if assignment is None and key_center_x < keyboard_center_x:  # Left side\n",
    "        if frame_idx < len(left_px):\n",
    "            lm = left_px[frame_idx]\n",
    "            if not np.any(np.isnan(lm)):\n",
    "                assignment = assigner.assign_from_landmarks(\n",
    "                    lm, key_idx, 'left', frame_idx, event.onset_time)\n",
    "\n",
    "    # Fallback: try the other hand\n",
    "    if assignment is None:\n",
    "        if key_center_x >= keyboard_center_x and frame_idx < len(left_px):\n",
    "            lm = left_px[frame_idx]\n",
    "            if not np.any(np.isnan(lm)):\n",
    "                assignment = assigner.assign_from_landmarks(\n",
    "                    lm, key_idx, 'left', frame_idx, event.onset_time)\n",
    "        elif key_center_x < keyboard_center_x and frame_idx < len(right_px):\n",
    "            lm = right_px[frame_idx]\n",
    "            if not np.any(np.isnan(lm)):\n",
    "                assignment = assigner.assign_from_landmarks(\n",
    "                    lm, key_idx, 'right', frame_idx, event.onset_time)\n",
    "\n",
    "    if assignment:\n",
    "        assignments.append(assignment)\n",
    "    else:\n",
    "        skipped += 1\n",
    "\n",
    "print(f\"‚úÖ Assignments completed\")\n",
    "print(f\"   Total events     : {len(synced_events)}\")\n",
    "print(f\"   Assigned         : {len(assignments)}\")\n",
    "print(f\"   Skipped (no data): {skipped}\")\n",
    "print(f\"   Coverage         : {len(assignments)/max(1,len(synced_events))*100:.1f}%\")\n",
    "\n",
    "# Diagnostic: show fingertip x-positions for first few events\n",
    "print(\"\\nDiagnostic ‚Äì fingertip x vs key centre (pixel space):\")\n",
    "for a in assignments[:5]:\n",
    "    f = a.frame_idx\n",
    "    hand_arr = right_px if a.hand == 'right' else left_px\n",
    "    if f < len(hand_arr):\n",
    "        lm = hand_arr[f]\n",
    "        tip_x = {fn: lm[idx, 0] for fn, idx in [(1,4),(2,8),(3,12),(4,16),(5,20)]}\n",
    "        kx = assigner.key_centers[a.key_idx][0]\n",
    "        dists = \" \".join(f\"F{fn}:{abs(tip_x[fn]-kx):>4.0f}\" for fn in range(1, 6))\n",
    "        print(f\"  Frame {f:>5d} | key_x={kx:>7.0f} | dx: {dists} ‚Üí {a.finger_name}(F{a.assigned_finger})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc00e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4.4 - Assignment Statistics & Visualisation\n",
    "# ==============================================================================\n",
    "\n",
    "if assignments:\n",
    "    fingers = [a.assigned_finger for a in assignments]\n",
    "    hands   = [a.hand for a in assignments]\n",
    "    confs   = [a.confidence for a in assignments]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "    # Finger distribution\n",
    "    finger_names = {1: 'Thumb', 2: 'Index', 3: 'Middle', 4: 'Ring', 5: 'Pinky'}\n",
    "    finger_counts = pd.Series(fingers).value_counts().sort_index()\n",
    "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "    finger_counts.plot.bar(ax=axes[0], color=[colors[i-1] for i in finger_counts.index])\n",
    "    axes[0].set_xticklabels([finger_names[i] for i in finger_counts.index], rotation=45)\n",
    "    axes[0].set_title('Finger Distribution')\n",
    "    axes[0].set_ylabel('Count')\n",
    "\n",
    "    # Hand distribution\n",
    "    pd.Series(hands).value_counts().plot.bar(ax=axes[1], color=['coral', 'skyblue'])\n",
    "    axes[1].set_title('Hand Distribution')\n",
    "    axes[1].set_ylabel('Count')\n",
    "\n",
    "    # Confidence distribution\n",
    "    axes[2].hist(confs, bins=30, color='mediumseagreen', edgecolor='white')\n",
    "    axes[2].set_title('Assignment Confidence')\n",
    "    axes[2].set_xlabel('Confidence')\n",
    "    axes[2].set_ylabel('Count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print some sample assignments\n",
    "    print(\"\\nSample assignments:\")\n",
    "    for a in assignments[:10]:\n",
    "        print(f\"  Frame {a.frame_idx:>5d} | {a.label} | \"\n",
    "              f\"MIDI {a.midi_pitch} ({a.finger_name:6s}) | conf={a.confidence:.3f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No assignments produced. Check skeleton data alignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726b5721",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='5'></a>\n",
    "## 5. Baseline Pipeline on Multiple Samples\n",
    "\n",
    "Run the Gaussian assignment baseline on several samples and collect statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc340366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5.1 - Helper: Process One Sample End-to-End (Stages 1‚Äì3)\n",
    "# ==============================================================================\n",
    "\n",
    "def process_sample_baseline(sample, dataset, config):\n",
    "    \"\"\"\n",
    "    Run the full baseline (Stages 1-3) on a single PianoVAMSample.\n",
    "    Returns a dict with 'sample_id', 'assignments' list, and 'error' string.\n",
    "    \"\"\"\n",
    "    result = {'sample_id': sample.id, 'assignments': [], 'error': None}\n",
    "\n",
    "    try:\n",
    "        # Stage 1: Keyboard Detection\n",
    "        corners = sample.metadata['keyboard_corners']\n",
    "        det = KeyboardDetector()\n",
    "        kb = det.detect_from_corners(corners)\n",
    "\n",
    "        # Stage 2: Hand Processing\n",
    "        skel = dataset.load_skeleton(sample)\n",
    "        ldr = SkeletonLoader()\n",
    "        h = ldr._parse_json(skel)\n",
    "\n",
    "        left_arr  = ldr.to_array(h['left'])\n",
    "        right_arr = ldr.to_array(h['right'])\n",
    "\n",
    "        tf = TemporalFilter(\n",
    "            hampel_window=config.hand.hampel_window,\n",
    "            hampel_threshold=config.hand.hampel_threshold,\n",
    "            max_interpolation_gap=config.hand.interpolation_max_gap,\n",
    "            savgol_window=config.hand.savgol_window,\n",
    "            savgol_order=config.hand.savgol_order\n",
    "        )\n",
    "        if left_arr.size > 0 and left_arr.shape[0] > 0:\n",
    "            left_arr = tf.process(left_arr)\n",
    "        if right_arr.size > 0 and right_arr.shape[0] > 0:\n",
    "            right_arr = tf.process(right_arr)\n",
    "\n",
    "        # Scale landmarks from [0,1] ‚Üí pixel space (NO homography warp!)\n",
    "        if left_arr.size > 0:\n",
    "            left_arr[:, :, 0] *= FRAME_W\n",
    "            left_arr[:, :, 1] *= FRAME_H\n",
    "        if right_arr.size > 0:\n",
    "            right_arr[:, :, 0] *= FRAME_W\n",
    "            right_arr[:, :, 1] *= FRAME_H\n",
    "\n",
    "        # Project key boundaries to pixel space\n",
    "        kb_px = project_keys_to_pixel_space(kb.key_boundaries, kb.homography)\n",
    "\n",
    "        # Stage 3: MIDI Sync + Assignment\n",
    "        tsv = dataset.load_tsv_annotations(sample)\n",
    "        midi_evts = []\n",
    "        for _, row in tsv.iterrows():\n",
    "            midi_evts.append({\n",
    "                'onset': float(row['onset']),\n",
    "                'offset': float(row['onset']) + 0.3,\n",
    "                'pitch': int(row['note']),\n",
    "                'velocity': int(row['velocity']) if 'velocity' in row and pd.notna(row['velocity']) else 64\n",
    "            })\n",
    "\n",
    "        sync = MidiVideoSync(fps=config.video_fps)\n",
    "        synced = sync.sync_events(midi_evts)\n",
    "\n",
    "        asgn = GaussianFingerAssigner(\n",
    "            key_boundaries=kb_px,\n",
    "            sigma=config.assignment.sigma,          # None ‚Üí auto-scale\n",
    "            candidate_range=config.assignment.candidate_keys\n",
    "        )\n",
    "\n",
    "        key_cx = sorted(asgn.key_centers.values(), key=lambda c: c[0])\n",
    "        center_x = key_cx[len(key_cx) // 2][0]\n",
    "\n",
    "        for ev in synced:\n",
    "            fidx, kidx = ev.frame_idx, ev.key_idx\n",
    "            if kidx not in asgn.key_centers:\n",
    "                continue\n",
    "            kx = asgn.key_centers[kidx][0]\n",
    "            assignment = None\n",
    "\n",
    "            # Try primary hand based on keyboard position\n",
    "            if kx >= center_x and fidx < len(right_arr):\n",
    "                lm = right_arr[fidx]\n",
    "                if not np.any(np.isnan(lm)):\n",
    "                    assignment = asgn.assign_from_landmarks(lm, kidx, 'right', fidx, ev.onset_time)\n",
    "            if assignment is None and kx < center_x and fidx < len(left_arr):\n",
    "                lm = left_arr[fidx]\n",
    "                if not np.any(np.isnan(lm)):\n",
    "                    assignment = asgn.assign_from_landmarks(lm, kidx, 'left', fidx, ev.onset_time)\n",
    "\n",
    "            # Fallback: other hand\n",
    "            if assignment is None:\n",
    "                if kx >= center_x and fidx < len(left_arr):\n",
    "                    lm = left_arr[fidx]\n",
    "                    if not np.any(np.isnan(lm)):\n",
    "                        assignment = asgn.assign_from_landmarks(lm, kidx, 'left', fidx, ev.onset_time)\n",
    "                elif kx < center_x and fidx < len(right_arr):\n",
    "                    lm = right_arr[fidx]\n",
    "                    if not np.any(np.isnan(lm)):\n",
    "                        assignment = asgn.assign_from_landmarks(lm, kidx, 'right', fidx, ev.onset_time)\n",
    "\n",
    "            if assignment:\n",
    "                result['assignments'].append(assignment)\n",
    "\n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e26058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5.2 - Process Multiple Samples from Train Split\n",
    "# ==============================================================================\n",
    "\n",
    "NUM_SAMPLES = 3  # Process first N samples (increase for thorough evaluation)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for i, samp in enumerate(train_dataset):\n",
    "    if i >= NUM_SAMPLES:\n",
    "        break\n",
    "    print(f\"\\nProcessing sample {i+1}/{NUM_SAMPLES}: ID={samp.id} ‚Äì {samp.metadata['piece'][:40]}\")\n",
    "    res = process_sample_baseline(samp, train_dataset, config)\n",
    "\n",
    "    if res['error']:\n",
    "        print(f\"  ‚ö†Ô∏è Error: {res['error'][:100]}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Assigned {len(res['assignments'])} notes\")\n",
    "    all_results.append(res)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE PROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "total_assigned = sum(len(r['assignments']) for r in all_results)\n",
    "successful = sum(1 for r in all_results if r['error'] is None)\n",
    "print(f\"Samples processed  : {len(all_results)}\")\n",
    "print(f\"Successful         : {successful}\")\n",
    "print(f\"Total assignments  : {total_assigned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5.3 - Baseline Statistics Across Samples\n",
    "# ==============================================================================\n",
    "\n",
    "# Aggregate finger distributions\n",
    "all_fingers = []\n",
    "all_hands   = []\n",
    "all_confs   = []\n",
    "\n",
    "for res in all_results:\n",
    "    for a in res['assignments']:\n",
    "        all_fingers.append(a.assigned_finger)\n",
    "        all_hands.append(a.hand)\n",
    "        all_confs.append(a.confidence)\n",
    "\n",
    "if all_fingers:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "    finger_names = {1: 'Thumb', 2: 'Index', 3: 'Middle', 4: 'Ring', 5: 'Pinky'}\n",
    "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "\n",
    "    fc = pd.Series(all_fingers).value_counts().sort_index()\n",
    "    fc.plot.bar(ax=axes[0], color=[colors[i-1] for i in fc.index])\n",
    "    axes[0].set_xticklabels([finger_names.get(i, str(i)) for i in fc.index], rotation=45)\n",
    "    axes[0].set_title(f'Finger Distribution (n={len(all_fingers)})')\n",
    "\n",
    "    pd.Series(all_hands).value_counts().plot.bar(ax=axes[1], color=['coral', 'skyblue'])\n",
    "    axes[1].set_title('Hand Distribution')\n",
    "\n",
    "    axes[2].hist(all_confs, bins=30, color='mediumseagreen', edgecolor='white')\n",
    "    axes[2].set_title('Confidence Distribution')\n",
    "    axes[2].axvline(np.mean(all_confs), color='red', ls='--', label=f'mean={np.mean(all_confs):.3f}')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Check biomechanical validity\n",
    "    constraints = BiomechanicalConstraints()\n",
    "    for res in all_results:\n",
    "        asgns = res['assignments']\n",
    "        if len(asgns) > 1:\n",
    "            violations = constraints.validate_sequence(\n",
    "                [a.assigned_finger for a in asgns],\n",
    "                [a.midi_pitch for a in asgns],\n",
    "                [a.hand for a in asgns]\n",
    "            )\n",
    "            ifr = len(violations) / max(1, len(asgns) - 1)\n",
    "            print(f\"  Sample {res['sample_id']} ‚Äì {len(asgns)} notes, \"\n",
    "                  f\"{len(violations)} violations, IFR={ifr:.3f}\")\n",
    "else:\n",
    "    print(\"No assignments to visualise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ddd5d",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='6'></a>\n",
    "## 6. Stage 4 ‚Äì Neural Refinement (BiLSTM)\n",
    "\n",
    "**Goal**: Refine Gaussian baseline predictions using a BiLSTM model with attention that captures temporal context and applies biomechanical constraints.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input(20) ‚Üí Linear(128) ‚Üí BiLSTM(128 √ó 2 layers) ‚Üí Self-Attention ‚Üí Linear(128) ‚Üí Linear(5)\n",
    "```\n",
    "\n",
    "**Input features per note** (20-dim):\n",
    "- Normalised pitch (1)\n",
    "- One-hot initial finger (5)\n",
    "- Time delta to previous note (1)\n",
    "- Hand encoding (1)\n",
    "- Pitch class one-hot (12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bbc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6.1 - Prepare Training Data from Baseline Assignments\n",
    "# ==============================================================================\n",
    "# We create training sequences from the baseline assignments.\n",
    "# Since PianoVAM doesn't provide per-note finger ground truth in a simple column,\n",
    "# we use the baseline Gaussian assignments as \"soft labels\" to train the BiLSTM\n",
    "# to learn temporal consistency and biomechanical constraints.\n",
    "# In a full setup, you would replace these with ground-truth finger annotations.\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Preparing training sequences from baseline assignments ...\")\n",
    "\n",
    "MAX_TRAIN_SAMPLES = 8  # Number of samples to use for training data (keep small on Colab)\n",
    "\n",
    "train_sequences = []\n",
    "train_ds_full = PianoVAMDataset(split='train', streaming=True, max_samples=MAX_TRAIN_SAMPLES)\n",
    "\n",
    "for i, samp in enumerate(train_ds_full):\n",
    "    if i >= MAX_TRAIN_SAMPLES:\n",
    "        break\n",
    "    res = process_sample_baseline(samp, train_ds_full, config)\n",
    "    asgns = res['assignments']\n",
    "\n",
    "    if len(asgns) < 10:  # Need minimum sequence length\n",
    "        continue\n",
    "\n",
    "    # Create sequence dict\n",
    "    seq = {\n",
    "        'pitches': [a.midi_pitch for a in asgns],\n",
    "        'fingers': [a.assigned_finger for a in asgns],\n",
    "        'onsets':  [a.note_onset for a in asgns],\n",
    "        'hands':   [a.hand for a in asgns],\n",
    "        'labels':  [a.assigned_finger for a in asgns],  # Using baseline as labels\n",
    "    }\n",
    "    train_sequences.append(seq)\n",
    "    print(f\"  Sample {samp.id}: {len(asgns)} notes\")\n",
    "\n",
    "print(f\"\\nTotal training sequences: {len(train_sequences)}\")\n",
    "print(f\"Total training notes   : {sum(len(s['pitches']) for s in train_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6.2 - Create Model & Dataset\n",
    "# ==============================================================================\n",
    "\n",
    "feature_extractor = FeatureExtractor(normalize_pitch=True)\n",
    "input_size = feature_extractor.get_input_size()  # 20\n",
    "\n",
    "print(f\"Feature input size: {input_size}\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "if len(train_sequences) > 2:\n",
    "    # Split into train/val (80/20)\n",
    "    split_idx = max(1, int(0.8 * len(train_sequences)))\n",
    "    train_seqs = train_sequences[:split_idx]\n",
    "    val_seqs   = train_sequences[split_idx:]\n",
    "\n",
    "    train_torch_ds = SequenceDataset(train_seqs, feature_extractor, max_len=256)\n",
    "    val_torch_ds   = SequenceDataset(val_seqs,   feature_extractor, max_len=256)\n",
    "\n",
    "    print(f\"Training sequences  : {len(train_torch_ds)}\")\n",
    "    print(f\"Validation sequences: {len(val_torch_ds)}\")\n",
    "\n",
    "    # Create model\n",
    "    model = FingeringRefiner(\n",
    "        input_size=input_size,\n",
    "        hidden_size=config.refinement.hidden_size,\n",
    "        num_layers=config.refinement.num_layers,\n",
    "        dropout=config.refinement.dropout,\n",
    "        bidirectional=config.refinement.bidirectional\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nModel parameters: {total_params:,} (trainable: {trainable_params:,})\")\n",
    "    print(model)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough training sequences. Need at least 3 samples with >=10 notes.\")\n",
    "    print(\"   Try increasing MAX_TRAIN_SAMPLES or check data loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6.3 - Train the Refinement Model\n",
    "# ==============================================================================\n",
    "\n",
    "if 'train_torch_ds' in dir() and len(train_torch_ds) > 0:\n",
    "    print(\"Training BiLSTM refinement model ...\\n\")\n",
    "\n",
    "    training_config = {\n",
    "        'hidden_size': config.refinement.hidden_size,\n",
    "        'num_layers': config.refinement.num_layers,\n",
    "        'dropout': config.refinement.dropout,\n",
    "        'batch_size': min(config.refinement.batch_size, len(train_torch_ds)),\n",
    "        'learning_rate': config.refinement.learning_rate,\n",
    "        'epochs': config.refinement.epochs,\n",
    "        'early_stopping_patience': config.refinement.early_stopping_patience,\n",
    "        'device': DEVICE,\n",
    "        'checkpoint_dir': '/content/checkpoints' if IN_COLAB else './outputs/checkpoints'\n",
    "    }\n",
    "\n",
    "    trained_model = train_refiner(\n",
    "        train_dataset=train_torch_ds,\n",
    "        val_dataset=val_torch_ds if len(val_torch_ds) > 0 else None,\n",
    "        config=training_config\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping training ‚Äì insufficient data.\")\n",
    "    trained_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6.4 - Apply Refinement to Baseline Predictions\n",
    "# ==============================================================================\n",
    "\n",
    "def refine_assignments(model, assignments, feature_extractor, device='cpu'):\n",
    "    \"\"\"\n",
    "    Apply trained BiLSTM model to refine baseline finger assignments.\n",
    "    Returns new list of FingerAssignment with refined fingers.\n",
    "    \"\"\"\n",
    "    if not assignments or model is None:\n",
    "        return assignments\n",
    "\n",
    "    pitches = [a.midi_pitch for a in assignments]\n",
    "    fingers = [a.assigned_finger for a in assignments]\n",
    "    onsets  = [a.note_onset for a in assignments]\n",
    "    hands   = [a.hand for a in assignments]\n",
    "\n",
    "    features = feature_extractor.extract(pitches, fingers, onsets, hands)\n",
    "    features = features.unsqueeze(0).to(device)  # (1, seq_len, 20)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds, confs = model.predict_with_confidence(features)\n",
    "\n",
    "    preds = preds.squeeze(0).cpu().numpy()\n",
    "    confs = confs.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Create refined assignments\n",
    "    refined = []\n",
    "    for i, a in enumerate(assignments):\n",
    "        refined.append(FingerAssignment(\n",
    "            note_onset=a.note_onset,\n",
    "            frame_idx=a.frame_idx,\n",
    "            midi_pitch=a.midi_pitch,\n",
    "            key_idx=a.key_idx,\n",
    "            assigned_finger=int(preds[i]),\n",
    "            hand=a.hand,\n",
    "            confidence=float(confs[i]),\n",
    "            fingertip_position=a.fingertip_position\n",
    "        ))\n",
    "\n",
    "    return refined\n",
    "\n",
    "\n",
    "if trained_model is not None and all_results:\n",
    "    print(\"Refining baseline predictions with BiLSTM ...\\n\")\n",
    "\n",
    "    for res in all_results:\n",
    "        if res['assignments']:\n",
    "            original = res['assignments']\n",
    "            refined  = refine_assignments(trained_model, original, feature_extractor, DEVICE)\n",
    "            res['refined_assignments'] = refined\n",
    "\n",
    "            # Count how many changed\n",
    "            changed = sum(1 for o, r in zip(original, refined)\n",
    "                         if o.assigned_finger != r.assigned_finger)\n",
    "            print(f\"  Sample {res['sample_id']}: {len(original)} notes, \"\n",
    "                  f\"{changed} changed ({changed/max(1,len(original))*100:.1f}%)\")\n",
    "\n",
    "    print(\"\\n‚úÖ Refinement applied.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping refinement (no trained model or no results).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d36fde",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='7'></a>\n",
    "## 7. Evaluation & Results\n",
    "\n",
    "Evaluate both the **baseline (Gaussian)** and **refined (BiLSTM)** predictions.\n",
    "\n",
    "Metrics:\n",
    "- **Accuracy**: Exact match rate\n",
    "- **M_gen**: General match rate (across annotators)\n",
    "- **M_high**: Highest match rate with any annotator\n",
    "- **IFR**: Irrational Fingering Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fdd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7.1 - Evaluate Baseline Predictions (Self-Consistency & IFR)\n",
    "# ==============================================================================\n",
    "# Note: Without ground-truth finger labels, we evaluate:\n",
    "#   - IFR (biomechanical constraint violations)\n",
    "#   - Confidence statistics\n",
    "#   - Finger distribution analysis\n",
    "# When ground-truth labels are available, Accuracy/M_gen/M_high are used.\n",
    "# ==============================================================================\n",
    "\n",
    "metrics = FingeringMetrics()\n",
    "constraints = BiomechanicalConstraints()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "baseline_ifrs = []\n",
    "refined_ifrs  = []\n",
    "\n",
    "for res in all_results:\n",
    "    if not res['assignments']:\n",
    "        continue\n",
    "\n",
    "    asgns = res['assignments']\n",
    "    pitches = [a.midi_pitch for a in asgns]\n",
    "    fingers = [a.assigned_finger for a in asgns]\n",
    "    hands_list = [a.hand for a in asgns]\n",
    "\n",
    "    # Baseline IFR\n",
    "    violations = constraints.validate_sequence(fingers, pitches, hands_list)\n",
    "    ifr = len(violations) / max(1, len(asgns) - 1)\n",
    "    baseline_ifrs.append(ifr)\n",
    "    mean_conf = np.mean([a.confidence for a in asgns])\n",
    "\n",
    "    print(f\"\\nSample {res['sample_id']}:\")\n",
    "    print(f\"  Baseline  ‚Äì {len(asgns)} notes | IFR={ifr:.3f} | \"\n",
    "          f\"Mean Confidence={mean_conf:.3f} | Violations={len(violations)}\")\n",
    "\n",
    "    # Refined (if available)\n",
    "    if 'refined_assignments' in res:\n",
    "        ref = res['refined_assignments']\n",
    "        ref_fingers = [a.assigned_finger for a in ref]\n",
    "        ref_violations = constraints.validate_sequence(ref_fingers, pitches, hands_list)\n",
    "        ref_ifr = len(ref_violations) / max(1, len(ref) - 1)\n",
    "        refined_ifrs.append(ref_ifr)\n",
    "        ref_conf = np.mean([a.confidence for a in ref])\n",
    "        print(f\"  Refined   ‚Äì {len(ref)} notes | IFR={ref_ifr:.3f} | \"\n",
    "              f\"Mean Confidence={ref_conf:.3f} | Violations={len(ref_violations)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if baseline_ifrs:\n",
    "    print(f\"BASELINE  ‚Äì Mean IFR: {np.mean(baseline_ifrs):.3f} ¬± {np.std(baseline_ifrs):.3f}\")\n",
    "if refined_ifrs:\n",
    "    print(f\"REFINED   ‚Äì Mean IFR: {np.mean(refined_ifrs):.3f} ¬± {np.std(refined_ifrs):.3f}\")\n",
    "    improvement = np.mean(baseline_ifrs) - np.mean(refined_ifrs)\n",
    "    print(f\"\\nIFR Improvement: {improvement:+.3f} ({'better' if improvement > 0 else 'worse'})\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ba359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7.2 - Visualise Results\n",
    "# ==============================================================================\n",
    "\n",
    "viz = ResultVisualizer(output_dir='./outputs' if not IN_COLAB else '/content/outputs')\n",
    "\n",
    "if all_fingers:\n",
    "    # --- Finger Distribution Comparison ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    finger_names = ['Thumb', 'Index', 'Middle', 'Ring', 'Pinky']\n",
    "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "\n",
    "    # Baseline\n",
    "    base_fc = pd.Series(all_fingers).value_counts().sort_index()\n",
    "    base_fc.plot.bar(ax=axes[0], color=[colors[i-1] for i in base_fc.index])\n",
    "    axes[0].set_title('Baseline Finger Distribution')\n",
    "    axes[0].set_xticklabels([finger_names[i-1] for i in base_fc.index], rotation=45)\n",
    "\n",
    "    # Refined (if available)\n",
    "    refined_fingers_all = []\n",
    "    for res in all_results:\n",
    "        if 'refined_assignments' in res:\n",
    "            refined_fingers_all.extend([a.assigned_finger for a in res['refined_assignments']])\n",
    "\n",
    "    if refined_fingers_all:\n",
    "        ref_fc = pd.Series(refined_fingers_all).value_counts().sort_index()\n",
    "        ref_fc.plot.bar(ax=axes[1], color=[colors[i-1] for i in ref_fc.index])\n",
    "        axes[1].set_title('Refined Finger Distribution')\n",
    "        axes[1].set_xticklabels([finger_names[i-1] for i in ref_fc.index], rotation=45)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No refined predictions', ha='center', va='center',\n",
    "                     transform=axes[1].transAxes, fontsize=14)\n",
    "        axes[1].set_title('Refined (not available)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- IFR Comparison ---\n",
    "if baseline_ifrs:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    x = np.arange(len(baseline_ifrs))\n",
    "    width = 0.35\n",
    "\n",
    "    ax.bar(x - width/2, baseline_ifrs, width, label='Baseline', color='steelblue')\n",
    "    if refined_ifrs:\n",
    "        ax.bar(x + width/2, refined_ifrs, width, label='Refined', color='coral')\n",
    "\n",
    "    ax.set_xlabel('Sample Index')\n",
    "    ax.set_ylabel('IFR (lower is better)')\n",
    "    ax.set_title('Irrational Fingering Rate Comparison')\n",
    "    ax.legend()\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([r['sample_id'] for r in all_results if r['assignments']])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32aa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7.3 - Run on Test Split (Final Evaluation)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Processing test split for final evaluation ...\\n\")\n",
    "\n",
    "test_ds_eval = PianoVAMDataset(split='test', streaming=True, max_samples=5)\n",
    "test_results = []\n",
    "\n",
    "for i, samp in enumerate(test_ds_eval):\n",
    "    print(f\"  Test sample {i+1}: ID={samp.id}\")\n",
    "    res = process_sample_baseline(samp, test_ds_eval, config)\n",
    "\n",
    "    if res['error']:\n",
    "        print(f\"    ‚ö†Ô∏è Error: {res['error'][:80]}\")\n",
    "    else:\n",
    "        n = len(res['assignments'])\n",
    "        # Apply refinement if model is available\n",
    "        if trained_model is not None and n > 0:\n",
    "            res['refined_assignments'] = refine_assignments(\n",
    "                trained_model, res['assignments'], feature_extractor, DEVICE)\n",
    "        print(f\"    ‚úÖ {n} notes assigned\")\n",
    "\n",
    "    test_results.append(res)\n",
    "\n",
    "# Evaluate test results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_baseline_ifrs = []\n",
    "test_refined_ifrs  = []\n",
    "\n",
    "for res in test_results:\n",
    "    if not res['assignments']:\n",
    "        continue\n",
    "\n",
    "    asgns = res['assignments']\n",
    "    pitches = [a.midi_pitch for a in asgns]\n",
    "    fingers = [a.assigned_finger for a in asgns]\n",
    "    hands_list = [a.hand for a in asgns]\n",
    "\n",
    "    viols = constraints.validate_sequence(fingers, pitches, hands_list)\n",
    "    ifr = len(viols) / max(1, len(asgns) - 1)\n",
    "    test_baseline_ifrs.append(ifr)\n",
    "\n",
    "    msg = f\"  {res['sample_id']} ‚Äì {len(asgns)} notes | Baseline IFR={ifr:.3f}\"\n",
    "\n",
    "    if 'refined_assignments' in res:\n",
    "        ref = res['refined_assignments']\n",
    "        ref_f = [a.assigned_finger for a in ref]\n",
    "        ref_viols = constraints.validate_sequence(ref_f, pitches, hands_list)\n",
    "        ref_ifr = len(ref_viols) / max(1, len(ref) - 1)\n",
    "        test_refined_ifrs.append(ref_ifr)\n",
    "        msg += f\" | Refined IFR={ref_ifr:.3f}\"\n",
    "\n",
    "    print(msg)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "if test_baseline_ifrs:\n",
    "    print(f\"TEST Baseline Mean IFR: {np.mean(test_baseline_ifrs):.3f} ¬± {np.std(test_baseline_ifrs):.3f}\")\n",
    "if test_refined_ifrs:\n",
    "    print(f\"TEST Refined  Mean IFR: {np.mean(test_refined_ifrs):.3f} ¬± {np.std(test_refined_ifrs):.3f}\")\n",
    "    improvement = np.mean(test_baseline_ifrs) - np.mean(test_refined_ifrs)\n",
    "    print(f\"IFR Improvement: {improvement:+.3f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7.4 - Summary Figure\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Generating summary figures ...\\n\")\n",
    "\n",
    "# Collect all test fingers for distribution plot\n",
    "test_all_fingers = []\n",
    "for res in test_results:\n",
    "    test_all_fingers.extend([a.assigned_finger for a in res.get('assignments', [])])\n",
    "\n",
    "if test_all_fingers:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    finger_names = ['Thumb', 'Index', 'Middle', 'Ring', 'Pinky']\n",
    "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "\n",
    "    # (0,0) Overall metrics text\n",
    "    axes[0, 0].axis('off')\n",
    "    total_notes = sum(len(r.get('assignments', [])) for r in test_results)\n",
    "    info_text = (\n",
    "        f\"Test Set Summary\\n\"\n",
    "        f\"{'='*30}\\n\"\n",
    "        f\"Samples processed: {sum(1 for r in test_results if r.get('assignments'))}\\n\"\n",
    "        f\"Total notes      : {total_notes}\\n\"\n",
    "        f\"Baseline IFR     : {np.mean(test_baseline_ifrs):.3f} +/- {np.std(test_baseline_ifrs):.3f}\\n\"\n",
    "    )\n",
    "    if test_refined_ifrs:\n",
    "        info_text += f\"Refined IFR      : {np.mean(test_refined_ifrs):.3f} +/- {np.std(test_refined_ifrs):.3f}\\n\"\n",
    "    axes[0, 0].text(0.5, 0.5, info_text, transform=axes[0, 0].transAxes,\n",
    "                    fontsize=13, verticalalignment='center', horizontalalignment='center',\n",
    "                    family='monospace', bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "    # (0,1) Finger distribution\n",
    "    fc = pd.Series(test_all_fingers).value_counts().sort_index()\n",
    "    fc.plot.bar(ax=axes[0, 1], color=[colors[i-1] for i in fc.index])\n",
    "    axes[0, 1].set_xticklabels([finger_names[i-1] for i in fc.index], rotation=45)\n",
    "    axes[0, 1].set_title('Test Set Finger Distribution')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "    # (1,0) IFR per sample\n",
    "    sample_ids = [r['sample_id'] for r in test_results if r.get('assignments')]\n",
    "    x = np.arange(len(test_baseline_ifrs))\n",
    "    w = 0.35\n",
    "    axes[1, 0].bar(x - w/2, test_baseline_ifrs, w, label='Baseline', color='steelblue')\n",
    "    if test_refined_ifrs:\n",
    "        axes[1, 0].bar(x + w/2, test_refined_ifrs, w, label='Refined', color='coral')\n",
    "    axes[1, 0].set_xlabel('Sample')\n",
    "    axes[1, 0].set_ylabel('IFR')\n",
    "    axes[1, 0].set_title('IFR per Test Sample')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(sample_ids[:len(test_baseline_ifrs)], rotation=45, fontsize=8)\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # (1,1) Confidence distribution\n",
    "    test_confs = [a.confidence for r in test_results for a in r.get('assignments', [])]\n",
    "    if test_confs:\n",
    "        axes[1, 1].hist(test_confs, bins=30, color='mediumseagreen', edgecolor='white')\n",
    "        axes[1, 1].axvline(np.mean(test_confs), color='red', ls='--',\n",
    "                           label=f'mean={np.mean(test_confs):.3f}')\n",
    "        axes[1, 1].set_title('Assignment Confidence (Test)')\n",
    "        axes[1, 1].set_xlabel('Confidence')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "        axes[1, 1].legend()\n",
    "\n",
    "    fig.suptitle('Piano Fingering Detection ‚Äì Evaluation Summary', fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No test results to visualise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7.5 - Save Results\n",
    "# ==============================================================================\n",
    "\n",
    "output_dir = Path('/content/outputs' if IN_COLAB else './outputs')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save assignments as JSON\n",
    "results_summary = {\n",
    "    'pipeline': 'piano-fingering-detection',\n",
    "    'baseline_method': 'Gaussian Assignment',\n",
    "    'refinement_method': 'BiLSTM + Attention',\n",
    "    'test_results': []\n",
    "}\n",
    "\n",
    "for i, res in enumerate(test_results):\n",
    "    entry = {\n",
    "        'sample_id': res['sample_id'],\n",
    "        'num_assignments': len(res.get('assignments', [])),\n",
    "        'error': res.get('error'),\n",
    "    }\n",
    "    if i < len(test_baseline_ifrs):\n",
    "        entry['baseline_ifr'] = float(test_baseline_ifrs[i])\n",
    "    if i < len(test_refined_ifrs):\n",
    "        entry['refined_ifr'] = float(test_refined_ifrs[i])\n",
    "    results_summary['test_results'].append(entry)\n",
    "\n",
    "if test_baseline_ifrs:\n",
    "    results_summary['mean_baseline_ifr'] = float(np.mean(test_baseline_ifrs))\n",
    "if test_refined_ifrs:\n",
    "    results_summary['mean_refined_ifr'] = float(np.mean(test_refined_ifrs))\n",
    "\n",
    "results_path = output_dir / 'evaluation_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to {results_path}\")\n",
    "\n",
    "# Save model checkpoint\n",
    "if trained_model is not None:\n",
    "    model_path = output_dir / 'refinement_model.pt'\n",
    "    torch.save(trained_model.state_dict(), model_path)\n",
    "    print(f\"‚úÖ Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd9327",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Conclusions\n",
    "\n",
    "### What was implemented\n",
    "\n",
    "| Stage | Description | Status |\n",
    "|-------|-------------|--------|\n",
    "| **Data Loading** | PianoVAM dataset via HuggingFace (streaming) | ‚úÖ |\n",
    "| **Stage 1** | Keyboard detection from corner annotations | ‚úÖ |\n",
    "| **Stage 2** | Hand skeleton loading + temporal filtering | ‚úÖ |\n",
    "| **Stage 3** | Gaussian finger-key assignment | ‚úÖ |\n",
    "| **Stage 4** | BiLSTM neural refinement | ‚úÖ |\n",
    "| **Evaluation** | IFR, confidence analysis, visualisation | ‚úÖ |\n",
    "\n",
    "### Metrics\n",
    "\n",
    "| Metric | Baseline (Gaussian) | + Refinement (BiLSTM) |\n",
    "|--------|--------------------|-----------------------|\n",
    "| **IFR** | ~ measured above | ~ measured above |\n",
    "| **Accuracy** | (requires GT labels) | (requires GT labels) |\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. The Gaussian assignment provides a reasonable baseline using only fingertip proximity.\n",
    "2. Temporal filtering (Hampel + SavGol) is crucial for cleaning MediaPipe skeleton noise.\n",
    "3. The BiLSTM refinement learns temporal consistency and reduces biomechanical violations (IFR).\n",
    "4. Full Accuracy/M_gen/M_high evaluation requires per-note ground-truth finger labels.\n",
    "\n",
    "### References\n",
    "\n",
    "1. Moryossef et al. (2023) ‚Äì \"At Your Fingertips: Extracting Piano Fingering Instructions from Videos\"\n",
    "2. Kim et al. (2025) ‚Äì \"PianoVAM: A Multimodal Piano Performance Dataset\" (ISMIR 2025)\n",
    "3. Ramoneda et al. (2022) ‚Äì \"Automatic Piano Fingering from Partially Annotated Scores\"\n",
    "4. Lee et al. (2019) ‚Äì \"Observing Pianist Accuracy and Form with Computer Vision\" (WACV 2019)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
