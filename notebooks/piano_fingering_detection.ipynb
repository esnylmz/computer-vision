{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automatic Piano Fingering Detection from Video\n",
        "\n",
        "**Computer Vision Final Project**\n",
        "\n",
        "---\n",
        "\n",
        "## Project Goal\n",
        "\n",
        "Given a video of a piano performance with synchronized MIDI data, automatically determine the finger assignment (1-5, thumb to pinky) for each played note.\n",
        "\n",
        "**Input**: Video + MIDI -> **Output**: Per-note finger labels (L1-L5 for left hand, R1-R5 for right hand)\n",
        "\n",
        "## Pipeline Architecture\n",
        "\n",
        "```\n",
        "Video -> Keyboard Detection -> Hand Pose Estimation -> Temporal Filtering -> Finger-Key Assignment -> Neural Refinement -> Fingering Labels\n",
        "          (OpenCV)              (MediaPipe)             (Hampel+SavGol)      (Gaussian Prob.)          (BiLSTM)\n",
        "```\n",
        "\n",
        "| Stage | Method | Input | Output |\n",
        "|-------|--------|-------|--------|\n",
        "| 1. Keyboard Detection | Canny + Hough + Clustering + Black-Key Analysis | Video frames | 88 key bounding boxes |\n",
        "| 2. Hand Pose Estimation | MediaPipe Hands | Video frame | 21-keypoint hand skeletons |\n",
        "| 3. Temporal Filtering | Hampel + SavGol filters | Raw landmarks | Filtered landmarks (T x 21 x 3) |\n",
        "| 4. Finger Assignment | Gaussian probability | MIDI events + fingertips + keys | FingerAssignment per note |\n",
        "| 5. Neural Refinement | BiLSTM + Attention | Initial assignments | Refined predictions |\n",
        "\n",
        "## Dataset\n",
        "\n",
        "**PianoVAM** (KAIST)\n",
        "- 107 piano performances with synchronized video, audio, MIDI\n",
        "- Pre-extracted 21-keypoint hand skeletons (MediaPipe)\n",
        "- Top-view camera angle (1920 x 1080, 60fps)\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "0. [Environment Setup](#0)\n",
        "1. [Data Exploration](#1)\n",
        "2. [Stage 1: Keyboard Detection (OpenCV)](#2)\n",
        "3. [Stage 2: Hand Pose Estimation (MediaPipe)](#3)\n",
        "4. [Stage 3: Temporal Filtering](#4)\n",
        "5. [Stage 4: Finger-Key Assignment](#5)\n",
        "6. [Baseline Pipeline on Multiple Samples](#6)\n",
        "7. [Stage 5: Neural Refinement (BiLSTM)](#7)\n",
        "8. [Evaluation & Results](#8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='0'></a>\n",
        "## 0. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, subprocess\n",
        "\n",
        "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
        "\n",
        "if IN_COLAB:\n",
        "    REPO_URL = 'https://github.com/esnylmz/computer-vision.git'\n",
        "    BRANCH = 'v4'\n",
        "    if not os.path.exists('computer-vision'):\n",
        "        subprocess.run(['git', 'clone', '--branch', BRANCH, '--single-branch', REPO_URL], check=True)\n",
        "    os.chdir('computer-vision')\n",
        "    subprocess.run(['git', 'fetch', 'origin', BRANCH], check=True)\n",
        "    subprocess.run(['git', 'checkout', BRANCH], check=True)\n",
        "    subprocess.run(['git', 'pull', '--ff-only', 'origin', BRANCH], check=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-e', '.'], check=True)\n",
        "    # mediapipe-numpy2 keeps mp.solutions API and works with numpy 2.x on Colab\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'mediapipe-numpy2'], check=True)\n",
        "    print('\\nColab environment ready')\n",
        "else:\n",
        "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "    if PROJECT_ROOT not in sys.path:\n",
        "        sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "    # make sure we have a compatible mediapipe (solutions API removed in 0.10.31+)\n",
        "    try:\n",
        "        import mediapipe as _mp\n",
        "        if not hasattr(_mp, 'solutions'):\n",
        "            print('WARNING: mediapipe version too new, reinstalling compatible version...')\n",
        "            subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'mediapipe-numpy2'], check=True)\n",
        "    except ImportError:\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'mediapipe-numpy2'], check=True)\n",
        "\n",
        "    print('Local environment ready')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import json, time, warnings\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(f'NumPy  : {np.__version__}')\n",
        "print(f'Pandas : {pd.__version__}')\n",
        "print(f'OpenCV : {cv2.__version__}')\n",
        "\n",
        "import torch\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device : {DEVICE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data.dataset import PianoVAMDataset, PianoVAMSample\n",
        "from src.data.midi_utils import MidiProcessor, MidiEvent\n",
        "from src.data.video_utils import VideoProcessor\n",
        "from src.utils.config import load_config, Config\n",
        "\n",
        "from src.keyboard.detector import KeyboardDetector, KeyboardRegion\n",
        "from src.keyboard.homography import HomographyComputer\n",
        "from src.keyboard.key_localization import KeyLocalizer\n",
        "from src.keyboard.auto_detector import AutoKeyboardDetector, AutoDetectionResult\n",
        "\n",
        "from src.hand.skeleton_loader import SkeletonLoader, HandLandmarks\n",
        "from src.hand.temporal_filter import TemporalFilter\n",
        "from src.hand.fingertip_extractor import FingertipExtractor, FingertipData\n",
        "\n",
        "from src.assignment.gaussian_assignment import GaussianFingerAssigner, FingerAssignment\n",
        "from src.assignment.midi_sync import MidiVideoSync\n",
        "from src.assignment.hand_separation import HandSeparator\n",
        "\n",
        "from src.refinement.model import FingeringRefiner, FeatureExtractor, SequenceDataset\n",
        "from src.refinement.constraints import BiomechanicalConstraints\n",
        "from src.refinement.decoding import constrained_viterbi_decode\n",
        "from src.refinement.train import train_refiner, collate_fn\n",
        "\n",
        "from src.evaluation.metrics import FingeringMetrics, EvaluationResult, aggregate_results\n",
        "from src.evaluation.visualization import ResultVisualizer\n",
        "\n",
        "from src.pipeline import FingeringPipeline\n",
        "\n",
        "config_path = 'configs/colab.yaml' if IN_COLAB else 'configs/default.yaml'\n",
        "config = load_config(config_path)\n",
        "print(f'All modules imported | Config: {config_path}')\n",
        "print(f'Project: {config.project_name} v{config.version}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='1'></a>\n",
        "## 1. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_EXPLORE = 20\n",
        "\n",
        "print('Loading PianoVAM dataset splits ...\\n')\n",
        "train_dataset = PianoVAMDataset(split='train', streaming=True, max_samples=MAX_EXPLORE)\n",
        "val_dataset = PianoVAMDataset(split='validation', streaming=True, max_samples=MAX_EXPLORE)\n",
        "test_dataset = PianoVAMDataset(split='test', streaming=True, max_samples=MAX_EXPLORE)\n",
        "\n",
        "sample = next(iter(train_dataset))\n",
        "print(f'\\nSample ID      : {sample.id}')\n",
        "print(f'Composer       : {sample.metadata[\"composer\"]}')\n",
        "print(f'Piece          : {sample.metadata[\"piece\"]}')\n",
        "print(f'Skill Level    : {sample.metadata[\"skill_level\"]}')\n",
        "print(f'Keyboard Corners: {sample.metadata[\"keyboard_corners\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Collecting dataset statistics ...')\n",
        "stats_ds = PianoVAMDataset(split='train', max_samples=None)\n",
        "\n",
        "composers, skill_levels = [], []\n",
        "for s in stats_ds:\n",
        "    composers.append(s.metadata['composer'])\n",
        "    skill_levels.append(s.metadata['skill_level'])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "pd.Series(skill_levels).value_counts().plot.bar(ax=axes[0], color='steelblue')\n",
        "axes[0].set_title(f'Skill Level Distribution (n={len(skill_levels)})')\n",
        "pd.Series(composers).value_counts().head(10).plot.barh(ax=axes[1], color='darkorange')\n",
        "axes[1].set_title('Top 10 Composers')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Train samples    : {len(skill_levels)}')\n",
        "print(f'Unique composers : {len(set(composers))}')\n",
        "print(f'Skill levels     : {dict(pd.Series(skill_levels).value_counts())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='2'></a>\n",
        "## 2. Stage 1 — Keyboard Detection (Classical Computer Vision)\n",
        "\n",
        "We detect the piano keyboard from raw video frames using classical computer vision:\n",
        "\n",
        "**Automatic Detection Pipeline (Canny + Hough)**\n",
        "1. Preprocessing — CLAHE contrast enhancement + Gaussian blur\n",
        "2. Canny edge detection (Otsu-adaptive + fixed thresholds, merged)\n",
        "3. Morphological closing to connect nearby edge fragments\n",
        "4. Hough line transform — detect horizontal and vertical lines\n",
        "5. Horizontal line clustering — group nearby lines, select keyboard top/bottom pair\n",
        "6. Black-key contour analysis — refine x-boundaries using dark-region segmentation\n",
        "7. Multi-frame consensus — sample multiple frames, take median bbox\n",
        "8. Homography computation for perspective normalization\n",
        "9. 88-key localization\n",
        "\n",
        "**Corner-based Detection (Ground Truth)**\n",
        "- Uses 4-point corner annotations from PianoVAM metadata\n",
        "- Serves as ground truth for evaluating automatic detection (IoU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download a video frame from PianoVAM\n",
        "print(f'Downloading video for sample {sample.id} ...')\n",
        "video_path = train_dataset.download_file(sample.video_path)\n",
        "print(f'Video saved to: {video_path}')\n",
        "\n",
        "vp = VideoProcessor()\n",
        "vp.open(video_path)\n",
        "print(f'Resolution: {vp.info.width}x{vp.info.height}')\n",
        "print(f'FPS: {vp.info.fps}')\n",
        "print(f'Total frames: {vp.info.frame_count}')\n",
        "print(f'Duration: {vp.info.duration:.1f}s')\n",
        "\n",
        "# grab a frame from the middle of the video\n",
        "mid_frame_idx = vp.info.frame_count // 2\n",
        "frame_bgr = vp.get_frame(mid_frame_idx)\n",
        "frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.imshow(frame_rgb)\n",
        "plt.title(f'Raw Video Frame (frame {mid_frame_idx})')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "vp.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Image Preprocessing Pipeline ──\n",
        "gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# CLAHE (Contrast-Limited Adaptive Histogram Equalisation) for lighting normalisation\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "enhanced = clahe.apply(gray)\n",
        "enhanced_blur = cv2.GaussianBlur(enhanced, (5, 5), 0)\n",
        "\n",
        "# Canny edge detection with different thresholds\n",
        "edges_low = cv2.Canny(blurred, 30, 100)\n",
        "edges_mid = cv2.Canny(blurred, 50, 150)\n",
        "edges_high = cv2.Canny(blurred, 100, 200)\n",
        "\n",
        "# Otsu-based automatic threshold on CLAHE-enhanced image\n",
        "otsu_thresh, _ = cv2.threshold(enhanced_blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "otsu_low, otsu_high = max(10, int(otsu_thresh * 0.5)), min(255, int(otsu_thresh * 1.0))\n",
        "edges_otsu = cv2.Canny(enhanced_blur, otsu_low, otsu_high)\n",
        "edges_merged = cv2.bitwise_or(edges_mid, edges_otsu)\n",
        "\n",
        "# Morphological closing to connect fragmented edges\n",
        "kernel_h = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 1))\n",
        "edges_closed = cv2.morphologyEx(edges_merged, cv2.MORPH_CLOSE, kernel_h)\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
        "\n",
        "axes[0, 0].imshow(frame_rgb);               axes[0, 0].set_title('Original (RGB)')\n",
        "axes[0, 1].imshow(gray, cmap='gray');        axes[0, 1].set_title('Grayscale')\n",
        "axes[0, 2].imshow(enhanced, cmap='gray');    axes[0, 2].set_title('CLAHE Enhanced')\n",
        "\n",
        "axes[1, 0].imshow(edges_low, cmap='gray');   axes[1, 0].set_title('Canny (30, 100)')\n",
        "axes[1, 1].imshow(edges_mid, cmap='gray');   axes[1, 1].set_title('Canny (50, 150)')\n",
        "axes[1, 2].imshow(edges_high, cmap='gray');  axes[1, 2].set_title('Canny (100, 200)')\n",
        "\n",
        "axes[2, 0].imshow(edges_otsu, cmap='gray');  axes[2, 0].set_title(f'Otsu-adaptive Canny ({otsu_low}, {otsu_high})')\n",
        "axes[2, 1].imshow(edges_merged, cmap='gray');axes[2, 1].set_title('Merged Edges (fixed + Otsu)')\n",
        "axes[2, 2].imshow(edges_closed, cmap='gray');axes[2, 2].set_title('After Morphological Close')\n",
        "\n",
        "for ax in axes.flat:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Image Preprocessing & Edge Detection Pipeline', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Otsu threshold: {otsu_thresh:.0f}  →  Canny range: ({otsu_low}, {otsu_high})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hough Line Transform on the edge map\n",
        "edges = edges_mid\n",
        "\n",
        "lines = cv2.HoughLinesP(\n",
        "    edges, rho=1, theta=np.pi/180, threshold=100,\n",
        "    minLineLength=100, maxLineGap=10\n",
        ")\n",
        "\n",
        "print(f'Total lines detected: {len(lines) if lines is not None else 0}')\n",
        "\n",
        "# separate horizontal and vertical lines\n",
        "line_vis = frame_rgb.copy()\n",
        "horizontal_lines = []\n",
        "vertical_lines = []\n",
        "\n",
        "if lines is not None:\n",
        "    for line in lines:\n",
        "        x1, y1, x2, y2 = line[0]\n",
        "        angle = np.abs(np.arctan2(y2 - y1, x2 - x1))\n",
        "        if angle < np.pi / 18:  # within 10 degrees of horizontal\n",
        "            horizontal_lines.append((x1, y1, x2, y2))\n",
        "            cv2.line(line_vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        elif angle > np.pi / 2 - np.pi / 18:  # within 10 degrees of vertical\n",
        "            vertical_lines.append((x1, y1, x2, y2))\n",
        "            cv2.line(line_vis, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
        "\n",
        "print(f'Horizontal lines: {len(horizontal_lines)}')\n",
        "print(f'Vertical lines  : {len(vertical_lines)}')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "axes[0].imshow(edges, cmap='gray')\n",
        "axes[0].set_title('Canny Edge Map')\n",
        "axes[1].imshow(line_vis)\n",
        "axes[1].set_title('Hough Lines (green=horizontal, red=vertical)')\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Automatic keyboard detection (Canny + Hough + clustering) ──\n",
        "corners = sample.metadata['keyboard_corners']\n",
        "\n",
        "auto_detector = AutoKeyboardDetector({\n",
        "    'canny_low': config.keyboard.canny_low,\n",
        "    'canny_high': config.keyboard.canny_high,\n",
        "    'hough_threshold': config.keyboard.hough_threshold,\n",
        "})\n",
        "\n",
        "# Run full single-frame auto-detection with intermediate results\n",
        "auto_result = auto_detector.detect_single_frame(frame_bgr, return_intermediates=True)\n",
        "print(f'Auto-detection success: {auto_result.success}')\n",
        "if auto_result.success:\n",
        "    print(f'  Auto bbox           : {auto_result.consensus_bbox}')\n",
        "    print(f'  Top line y          : {auto_result.top_line_y:.0f}')\n",
        "    print(f'  Bottom line y       : {auto_result.bottom_line_y:.0f}')\n",
        "    n_horiz = len(auto_result.horizontal_lines) if auto_result.horizontal_lines else 0\n",
        "    n_vert  = len(auto_result.vertical_lines) if auto_result.vertical_lines else 0\n",
        "    n_bk    = len(auto_result.black_key_contours) if auto_result.black_key_contours else 0\n",
        "    print(f'  Horizontal lines    : {n_horiz}')\n",
        "    print(f'  Vertical lines      : {n_vert}')\n",
        "    print(f'  Black key contours  : {n_bk}')\n",
        "    print(f'  Line clusters       : {len(auto_result.line_clusters)}')\n",
        "else:\n",
        "    print('  (auto-detection failed on this frame)')\n",
        "\n",
        "# Corner-based detection (ground truth)\n",
        "detector = KeyboardDetector({\n",
        "    'canny_low': config.keyboard.canny_low,\n",
        "    'canny_high': config.keyboard.canny_high,\n",
        "    'hough_threshold': config.keyboard.hough_threshold\n",
        "})\n",
        "keyboard_region = detector.detect_from_corners(corners)\n",
        "print(f'\\nCorner-based detection: {len(keyboard_region.key_boundaries)} keys')\n",
        "print(f'  Bounding box   : {keyboard_region.bbox}')\n",
        "print(f'  White key width: {keyboard_region.white_key_width:.1f} px')\n",
        "\n",
        "# Compute IoU between auto-detected and corner-based\n",
        "if auto_result.success:\n",
        "    iou = auto_detector.evaluate_against_corners(auto_result, corners)\n",
        "    print(f'\\n>>> IoU (auto vs corners): {iou:.3f} <<<')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Visualize auto-detection intermediates ──\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "# 1. Hough lines on frame\n",
        "line_vis = auto_detector.visualize_lines(frame_bgr, auto_result)\n",
        "axes[0, 0].imshow(cv2.cvtColor(line_vis, cv2.COLOR_BGR2RGB))\n",
        "axes[0, 0].set_title('Hough Lines (green=horizontal, red=vertical)')\n",
        "\n",
        "# 2. Line clusters & selected top/bottom edges\n",
        "cluster_vis = auto_detector.visualize_clusters(frame_bgr, auto_result)\n",
        "axes[0, 1].imshow(cv2.cvtColor(cluster_vis, cv2.COLOR_BGR2RGB))\n",
        "axes[0, 1].set_title('Line Clusters & Selected Edges (cyan)')\n",
        "\n",
        "# 3. Black key contours\n",
        "bk_vis = auto_detector.visualize_black_keys(frame_bgr, auto_result)\n",
        "axes[1, 0].imshow(cv2.cvtColor(bk_vis, cv2.COLOR_BGR2RGB))\n",
        "axes[1, 0].set_title('Black Key Contour Detection')\n",
        "\n",
        "# 4. Final comparison: auto (green) vs corner GT (red)\n",
        "compare_vis = auto_detector.visualize_detection(\n",
        "    frame_bgr, auto_result, corner_bbox=keyboard_region.bbox\n",
        ")\n",
        "axes[1, 1].imshow(cv2.cvtColor(compare_vis, cv2.COLOR_BGR2RGB))\n",
        "axes[1, 1].set_title('Auto-detected (green) vs Corner GT (red)')\n",
        "\n",
        "for ax in axes.flat:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Automatic Keyboard Detection — Intermediate Stages', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Multi-frame consensus auto-detection from video ──\n",
        "print(f'Running multi-frame auto-detection on {video_path} ...')\n",
        "video_auto_result = auto_detector.detect_from_video(str(video_path), return_intermediates=False)\n",
        "\n",
        "n_valid = sum(1 for b in (video_auto_result.per_frame_bboxes or []) if b is not None)\n",
        "n_total = len(video_auto_result.per_frame_bboxes or [])\n",
        "print(f'  Sampled frames   : {n_total}')\n",
        "print(f'  Successful frames: {n_valid}/{n_total}')\n",
        "print(f'  Consensus bbox   : {video_auto_result.consensus_bbox}')\n",
        "\n",
        "if video_auto_result.success:\n",
        "    video_iou = auto_detector.evaluate_against_corners(video_auto_result, corners)\n",
        "    print(f'  IoU (multi-frame vs corners): {video_iou:.3f}')\n",
        "\n",
        "    # Show per-frame bboxes\n",
        "    if video_auto_result.per_frame_bboxes:\n",
        "        valid_bboxes = [b for b in video_auto_result.per_frame_bboxes if b is not None]\n",
        "        arr = np.array(valid_bboxes)\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
        "\n",
        "        axes[0].plot(arr[:, 0], 'o-', label='x1')\n",
        "        axes[0].plot(arr[:, 2], 's-', label='x2')\n",
        "        axes[0].set_title('Per-frame x-coordinates')\n",
        "        axes[0].legend()\n",
        "        axes[0].set_xlabel('Sampled frame #')\n",
        "\n",
        "        axes[1].plot(arr[:, 1], 'o-', label='y_top')\n",
        "        axes[1].plot(arr[:, 3], 's-', label='y_bottom')\n",
        "        axes[1].set_title('Per-frame y-coordinates')\n",
        "        axes[1].legend()\n",
        "        axes[1].set_xlabel('Sampled frame #')\n",
        "\n",
        "        plt.suptitle('Multi-Frame Consensus: per-frame bbox coordinates', fontsize=13)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print('  Multi-frame auto-detection failed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Homography warping: perspective correction\n",
        "hc = HomographyComputer()\n",
        "H = keyboard_region.homography\n",
        "H_inv = np.linalg.inv(H)\n",
        "\n",
        "# warp the frame to get a top-down view of the keyboard\n",
        "warped = cv2.warpPerspective(frame_bgr, H, (1718, 213))\n",
        "warped_rgb = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# draw keyboard corners on the original frame\n",
        "corner_vis = frame_rgb.copy()\n",
        "if keyboard_region.corners:\n",
        "    pts = [keyboard_region.corners[k] for k in ['LT', 'RT', 'RB', 'LB']]\n",
        "    for i in range(4):\n",
        "        p1 = pts[i]\n",
        "        p2 = pts[(i + 1) % 4]\n",
        "        cv2.line(corner_vis, p1, p2, (0, 255, 0), 3)\n",
        "        cv2.circle(corner_vis, p1, 8, (255, 0, 0), -1)\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
        "axes[0].imshow(corner_vis)\n",
        "axes[0].set_title('Detected Keyboard Corners')\n",
        "axes[1].imshow(warped_rgb)\n",
        "axes[1].set_title('Perspective-Corrected Keyboard (Homography Warp)')\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize 88-key layout in warped space\n",
        "localizer = KeyLocalizer(keyboard_region.key_boundaries)\n",
        "white_keys = localizer.get_white_keys()\n",
        "black_keys = localizer.get_black_keys()\n",
        "\n",
        "print(f'White keys: {len(white_keys)}  |  Black keys: {len(black_keys)}')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 3))\n",
        "for ki in white_keys:\n",
        "    x1, y1, x2, y2 = ki.bbox\n",
        "    ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=0.8,\n",
        "                               edgecolor='black', facecolor='white'))\n",
        "for ki in black_keys:\n",
        "    x1, y1, x2, y2 = ki.bbox\n",
        "    ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=0.5,\n",
        "                               edgecolor='black', facecolor='#333'))\n",
        "\n",
        "for note_name in ['A0', 'C4', 'C8']:\n",
        "    ki = localizer.get_key_by_name(note_name)\n",
        "    if ki:\n",
        "        ax.annotate(ki.note_name, xy=ki.center, fontsize=7, color='red', ha='center', va='bottom')\n",
        "\n",
        "ax.set_xlim(keyboard_region.bbox[0] - 10, keyboard_region.bbox[2] + 10)\n",
        "ax.set_ylim(keyboard_region.bbox[3] + 10, keyboard_region.bbox[1] - 10)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_title('88-Key Layout (Warped Space)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='3'></a>\n",
        "## 3. Stage 2 - Hand Pose Estimation (MediaPipe)\n",
        "\n",
        "We run MediaPipe Hands on actual video frames to detect 21 hand landmarks per hand.\n",
        "Then compare with pre-extracted skeleton data from the PianoVAM dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "print(f'MediaPipe version: {mp.__version__}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run MediaPipe hand detection on video frames\n",
        "vp = VideoProcessor()\n",
        "vp.open(video_path)\n",
        "\n",
        "# pick several frames spread across the video\n",
        "sample_frame_indices = [300, 600, 1200, 2400, 4800]\n",
        "sample_frames = []\n",
        "for idx in sample_frame_indices:\n",
        "    f = vp.get_frame(idx)\n",
        "    if f is not None:\n",
        "        sample_frames.append((idx, f))\n",
        "\n",
        "vp.close()\n",
        "\n",
        "# run mediapipe on each frame\n",
        "hands_detector = mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, len(sample_frames), figsize=(5 * len(sample_frames), 6))\n",
        "if len(sample_frames) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, (fidx, frame) in enumerate(sample_frames):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = hands_detector.process(frame_rgb)\n",
        "\n",
        "    annotated = frame_rgb.copy()\n",
        "    n_hands = 0\n",
        "    if results.multi_hand_landmarks:\n",
        "        n_hands = len(results.multi_hand_landmarks)\n",
        "        for hand_lm in results.multi_hand_landmarks:\n",
        "            mp_drawing.draw_landmarks(\n",
        "                annotated, hand_lm, mp_hands.HAND_CONNECTIONS,\n",
        "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "                mp_drawing_styles.get_default_hand_connections_style()\n",
        "            )\n",
        "\n",
        "    axes[i].imshow(annotated)\n",
        "    axes[i].set_title(f'Frame {fidx} ({n_hands} hands)')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('MediaPipe Hand Detection on Video Frames', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "hands_detector.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract landmarks from MediaPipe and visualize\n",
        "# Focus on one frame to show the 21-keypoint structure\n",
        "\n",
        "demo_frame_bgr = sample_frames[2][1] if len(sample_frames) > 2 else sample_frames[0][1]\n",
        "demo_frame_rgb = cv2.cvtColor(demo_frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "hands_detector = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
        "results = hands_detector.process(demo_frame_rgb)\n",
        "hands_detector.close()\n",
        "\n",
        "h, w = demo_frame_rgb.shape[:2]\n",
        "annotated = demo_frame_rgb.copy()\n",
        "\n",
        "fingertip_indices = [4, 8, 12, 16, 20]\n",
        "finger_names = {4: 'Thumb', 8: 'Index', 12: 'Middle', 16: 'Ring', 20: 'Pinky'}\n",
        "\n",
        "if results.multi_hand_landmarks:\n",
        "    for hand_lm, hand_info in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
        "        label = hand_info.classification[0].label\n",
        "        mp_drawing.draw_landmarks(annotated, hand_lm, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "        # mark fingertips\n",
        "        for tip_idx in fingertip_indices:\n",
        "            lm = hand_lm.landmark[tip_idx]\n",
        "            px, py = int(lm.x * w), int(lm.y * h)\n",
        "            cv2.circle(annotated, (px, py), 8, (255, 0, 0), -1)\n",
        "            cv2.putText(annotated, finger_names[tip_idx], (px + 10, py - 5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
        "\n",
        "        print(f'{label} hand detected:')\n",
        "        for tip_idx in fingertip_indices:\n",
        "            lm = hand_lm.landmark[tip_idx]\n",
        "            print(f'  {finger_names[tip_idx]:6s} tip: ({lm.x:.4f}, {lm.y:.4f}, {lm.z:.4f})')\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.imshow(annotated)\n",
        "plt.title('MediaPipe 21-Keypoint Hand Skeleton with Fingertip Labels')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare our MediaPipe detection with pre-extracted skeleton JSON\n",
        "print(f'Downloading skeleton JSON for sample {sample.id} ...')\n",
        "skeleton_data = train_dataset.load_skeleton(sample)\n",
        "\n",
        "loader = SkeletonLoader()\n",
        "hands_parsed = loader._parse_json(skeleton_data)\n",
        "\n",
        "left_raw = loader.to_array(hands_parsed['left'])\n",
        "right_raw = loader.to_array(hands_parsed['right'])\n",
        "\n",
        "print(f'\\nPre-extracted skeleton:')\n",
        "print(f'  Left  hand frames: {len(hands_parsed[\"left\"])}, array shape: {left_raw.shape}')\n",
        "print(f'  Right hand frames: {len(hands_parsed[\"right\"])}, array shape: {right_raw.shape}')\n",
        "\n",
        "left_valid = int(np.sum(~np.any(np.isnan(left_raw.reshape(len(left_raw), -1)), axis=1))) if left_raw.size > 0 else 0\n",
        "right_valid = int(np.sum(~np.any(np.isnan(right_raw.reshape(len(right_raw), -1)), axis=1))) if right_raw.size > 0 else 0\n",
        "print(f'  Valid frames - Left: {left_valid}  Right: {right_valid}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overlay pre-extracted skeleton on the same frame for comparison\n",
        "demo_fidx = sample_frames[2][0] if len(sample_frames) > 2 else sample_frames[0][0]\n",
        "comparison = demo_frame_rgb.copy()\n",
        "\n",
        "# draw pre-extracted landmarks in green\n",
        "for hand_key, color in [('right', (0, 255, 0)), ('left', (0, 200, 255))]:\n",
        "    arr = right_raw if hand_key == 'right' else left_raw\n",
        "    if demo_fidx < len(arr) and not np.any(np.isnan(arr[demo_fidx])):\n",
        "        lm = arr[demo_fidx]\n",
        "        for j in range(21):\n",
        "            px = int(lm[j, 0] * w)\n",
        "            py = int(lm[j, 1] * h)\n",
        "            cv2.circle(comparison, (px, py), 4, color, -1)\n",
        "        # connect landmarks\n",
        "        connections = [(0,1),(1,2),(2,3),(3,4),(0,5),(5,6),(6,7),(7,8),\n",
        "                       (5,9),(9,10),(10,11),(11,12),(9,13),(13,14),(14,15),(15,16),\n",
        "                       (13,17),(17,18),(18,19),(19,20),(0,17)]\n",
        "        for c1, c2 in connections:\n",
        "            p1 = (int(lm[c1, 0] * w), int(lm[c1, 1] * h))\n",
        "            p2 = (int(lm[c2, 0] * w), int(lm[c2, 1] * h))\n",
        "            cv2.line(comparison, p1, p2, color, 2)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.imshow(comparison)\n",
        "plt.title(f'Pre-extracted Skeleton Overlay (green=right, cyan=left) - Frame {demo_fidx}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='4'></a>\n",
        "## 4. Stage 3 - Temporal Filtering\n",
        "\n",
        "MediaPipe landmarks are noisy. We apply a 3-stage filtering pipeline:\n",
        "1. Hampel filter (outlier detection via Median Absolute Deviation)\n",
        "2. Linear interpolation (fill gaps < 30 frames)\n",
        "3. Savitzky-Golay filter (smoothing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf = TemporalFilter(\n",
        "    hampel_window=config.hand.hampel_window,\n",
        "    hampel_threshold=config.hand.hampel_threshold,\n",
        "    max_interpolation_gap=config.hand.interpolation_max_gap,\n",
        "    savgol_window=config.hand.savgol_window,\n",
        "    savgol_order=config.hand.savgol_order\n",
        ")\n",
        "\n",
        "left_filtered = tf.process(left_raw) if left_raw.size > 0 else left_raw\n",
        "right_filtered = tf.process(right_raw) if right_raw.size > 0 else right_raw\n",
        "\n",
        "print('Filtering complete')\n",
        "print(f'Left  filtered shape: {left_filtered.shape}')\n",
        "print(f'Right filtered shape: {right_filtered.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize filtering effect: index fingertip x-coordinate\n",
        "hand_arr_raw = right_raw if right_raw.size > 0 else left_raw\n",
        "hand_arr_filt = right_filtered if right_filtered.size > 0 else left_filtered\n",
        "hand_label = 'Right' if right_raw.size > 0 else 'Left'\n",
        "\n",
        "lm_idx = 8  # index fingertip\n",
        "T = min(3000, len(hand_arr_raw))\n",
        "\n",
        "raw_signal = hand_arr_raw[:T, lm_idx, 0]\n",
        "filt_signal = hand_arr_filt[:T, lm_idx, 0]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 4))\n",
        "ax.plot(raw_signal, alpha=0.5, label='Raw', linewidth=0.5)\n",
        "ax.plot(filt_signal, label='Filtered', linewidth=1)\n",
        "ax.set_title(f'{hand_label} Hand - Index Fingertip X-Coordinate')\n",
        "ax.set_xlabel('Frame')\n",
        "ax.set_ylabel('X (normalized)')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optical flow: track fingertip motion between frames\n",
        "vp = VideoProcessor()\n",
        "vp.open(video_path)\n",
        "\n",
        "flow_start = 1000\n",
        "frame1_bgr = vp.get_frame(flow_start)\n",
        "frame2_bgr = vp.get_frame(flow_start + 5)\n",
        "vp.close()\n",
        "\n",
        "if frame1_bgr is not None and frame2_bgr is not None:\n",
        "    gray1 = cv2.cvtColor(frame1_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(frame2_bgr, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # crop to keyboard region for clearer visualization\n",
        "    y1, y2 = keyboard_region.bbox[1], keyboard_region.bbox[3]\n",
        "    x1, x2 = keyboard_region.bbox[0], keyboard_region.bbox[2]\n",
        "    gray1_crop = gray1[y1:y2, x1:x2]\n",
        "    gray2_crop = gray2[y1:y2, x1:x2]\n",
        "\n",
        "    flow = cv2.calcOpticalFlowFarneback(\n",
        "        gray1_crop, gray2_crop, None,\n",
        "        pyr_scale=0.5, levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.2, flags=0\n",
        "    )\n",
        "\n",
        "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    # HSV visualization\n",
        "    hsv = np.zeros((*gray1_crop.shape, 3), dtype=np.uint8)\n",
        "    hsv[..., 0] = angle * 180 / np.pi / 2\n",
        "    hsv[..., 1] = 255\n",
        "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    flow_vis = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    axes[0].imshow(cv2.cvtColor(frame1_bgr[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title(f'Frame {flow_start}')\n",
        "    axes[1].imshow(cv2.cvtColor(frame2_bgr[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
        "    axes[1].set_title(f'Frame {flow_start + 5}')\n",
        "    axes[2].imshow(flow_vis)\n",
        "    axes[2].set_title('Dense Optical Flow (Farneback)')\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "    plt.suptitle('Optical Flow: Hand Motion Between Frames', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Mean flow magnitude: {np.mean(magnitude):.2f} px/frame')\n",
        "    print(f'Max flow magnitude : {np.max(magnitude):.2f} px/frame')\n",
        "else:\n",
        "    print('Could not read frames for optical flow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fingertip extraction\n",
        "extractor = FingertipExtractor()\n",
        "\n",
        "sample_fidx = 500\n",
        "if sample_fidx < len(right_filtered) and not np.any(np.isnan(right_filtered[sample_fidx])):\n",
        "    ftips = extractor.extract(right_filtered[sample_fidx], frame_idx=sample_fidx, hand_type='right')\n",
        "    print(f'Frame {sample_fidx} - Right hand fingertips:')\n",
        "    for f_num in range(1, 6):\n",
        "        pos = ftips.get_position_2d(f_num)\n",
        "        if pos:\n",
        "            print(f'  {extractor.FINGER_NAMES[f_num]:6s}: ({pos[0]:.4f}, {pos[1]:.4f})')\n",
        "\n",
        "    span = extractor.compute_hand_span(ftips)\n",
        "    print(f'  Hand span: {span:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='5'></a>\n",
        "## 5. Stage 4 - Finger-Key Assignment\n",
        "\n",
        "Gaussian probability model in image-pixel space (not homography-warped space).\n",
        "Uses x-distance only to avoid y-bias from different finger lengths.\n",
        "Tries both hands for each key, picks the higher-confidence assignment.\n",
        "Max-distance gate rejects assignments when the hand is too far from the key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MIDI/TSV annotations\n",
        "print(f'Downloading TSV annotations for sample {sample.id} ...')\n",
        "tsv_df = train_dataset.load_tsv_annotations(sample)\n",
        "\n",
        "midi_events = []\n",
        "for _, row in tsv_df.iterrows():\n",
        "    midi_events.append({\n",
        "        'onset': float(row['onset']),\n",
        "        'offset': float(row['onset']) + 0.3,\n",
        "        'pitch': int(row['note']),\n",
        "        'velocity': int(row['velocity']) if 'velocity' in row and pd.notna(row['velocity']) else 64\n",
        "    })\n",
        "\n",
        "print(f'Total MIDI events: {len(midi_events)}')\n",
        "print(f'Pitch range: {min(e[\"pitch\"] for e in midi_events)} - {max(e[\"pitch\"] for e in midi_events)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synchronize MIDI events with video frames\n",
        "midi_sync = MidiVideoSync(fps=config.video_fps)\n",
        "synced_events = midi_sync.sync_events(midi_events)\n",
        "print(f'Synced events: {len(synced_events)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FRAME_W, FRAME_H = 1920, 1080\n",
        "\n",
        "def project_keys_to_pixel_space(key_boundaries_warped, homography):\n",
        "    H_inv = np.linalg.inv(homography)\n",
        "    result = {}\n",
        "    for k, (x1, y1, x2, y2) in key_boundaries_warped.items():\n",
        "        cy = (y1 + y2) / 2.0\n",
        "        pts_w = np.array([[x1, cy, 1.0], [x2, cy, 1.0], [(x1+x2)/2.0, cy, 1.0]], dtype=np.float64)\n",
        "        pts_p = (H_inv @ pts_w.T).T\n",
        "        pts_p = pts_p[:, :2] / pts_p[:, 2:3]\n",
        "        lx, rx = pts_p[0, 0], pts_p[1, 0]\n",
        "        cy_px = pts_p[2, 1]\n",
        "        result[k] = (lx, cy_px - 5.0, rx, cy_px + 5.0)\n",
        "    return result\n",
        "\n",
        "key_boundaries_px = project_keys_to_pixel_space(keyboard_region.key_boundaries, keyboard_region.homography)\n",
        "\n",
        "# scale landmarks from [0,1] to pixels\n",
        "left_px = left_filtered.copy()\n",
        "left_px[:, :, 0] *= FRAME_W\n",
        "left_px[:, :, 1] *= FRAME_H\n",
        "\n",
        "right_px = right_filtered.copy()\n",
        "right_px[:, :, 0] *= FRAME_W\n",
        "right_px[:, :, 1] *= FRAME_H\n",
        "\n",
        "assigner = GaussianFingerAssigner(\n",
        "    key_boundaries=key_boundaries_px,\n",
        "    sigma=config.assignment.sigma,\n",
        "    candidate_range=config.assignment.candidate_keys\n",
        ")\n",
        "\n",
        "print(f'Sigma (auto): {assigner.sigma:.1f} px')\n",
        "print(f'Max distance: {assigner.max_distance_px:.0f} px ({assigner.max_distance_sigma} sigma)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run assignment: try BOTH hands for every key, pick higher confidence\n",
        "assignments = []\n",
        "skipped = 0\n",
        "\n",
        "for event in synced_events:\n",
        "    frame_idx = event.frame_idx\n",
        "    key_idx = event.key_idx\n",
        "\n",
        "    if key_idx not in assigner.key_centers:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    asgn_right = None\n",
        "    if frame_idx < len(right_px):\n",
        "        lm = right_px[frame_idx]\n",
        "        if not np.any(np.isnan(lm)):\n",
        "            asgn_right = assigner.assign_from_landmarks(lm, key_idx, 'right', frame_idx, event.onset_time)\n",
        "\n",
        "    asgn_left = None\n",
        "    if frame_idx < len(left_px):\n",
        "        lm = left_px[frame_idx]\n",
        "        if not np.any(np.isnan(lm)):\n",
        "            asgn_left = assigner.assign_from_landmarks(lm, key_idx, 'left', frame_idx, event.onset_time)\n",
        "\n",
        "    candidates = [a for a in (asgn_right, asgn_left) if a is not None]\n",
        "    if candidates:\n",
        "        assignments.append(max(candidates, key=lambda a: a.confidence))\n",
        "    else:\n",
        "        skipped += 1\n",
        "\n",
        "print(f'Total events  : {len(synced_events)}')\n",
        "print(f'Assigned      : {len(assignments)}')\n",
        "print(f'Skipped       : {skipped}')\n",
        "print(f'Coverage      : {len(assignments)/max(1,len(synced_events))*100:.1f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assignment statistics\n",
        "if assignments:\n",
        "    fingers = [a.assigned_finger for a in assignments]\n",
        "    hands_list = [a.hand for a in assignments]\n",
        "    confs = [a.confidence for a in assignments]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "    finger_names_map = {1: 'Thumb', 2: 'Index', 3: 'Middle', 4: 'Ring', 5: 'Pinky'}\n",
        "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
        "    fc = pd.Series(fingers).value_counts().sort_index()\n",
        "    fc.plot.bar(ax=axes[0], color=[colors[i-1] for i in fc.index])\n",
        "    axes[0].set_xticklabels([finger_names_map[i] for i in fc.index], rotation=45)\n",
        "    axes[0].set_title('Finger Distribution')\n",
        "\n",
        "    pd.Series(hands_list).value_counts().plot.bar(ax=axes[1], color=['coral', 'skyblue'])\n",
        "    axes[1].set_title('Hand Distribution')\n",
        "\n",
        "    axes[2].hist(confs, bins=30, color='mediumseagreen', edgecolor='white')\n",
        "    axes[2].set_title('Assignment Confidence')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print('\\nSample assignments:')\n",
        "    for a in assignments[:10]:\n",
        "        print(f'  Frame {a.frame_idx:>5d} | {a.label} | MIDI {a.midi_pitch} ({a.finger_name:6s}) | conf={a.confidence:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='6'></a>\n",
        "## 6. Baseline Pipeline on Multiple Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_SAMPLE_CACHE = {'skeleton': {}, 'tsv': {}, 'filtered_landmarks': {}, 'keys_px': {}}\n",
        "\n",
        "def process_sample_baseline(sample, dataset, config, max_duration_sec=120, cache=_SAMPLE_CACHE):\n",
        "    result = {'sample_id': sample.id, 'assignments': [], 'error': None}\n",
        "    try:\n",
        "        corners = sample.metadata['keyboard_corners']\n",
        "        det = KeyboardDetector()\n",
        "        kb = det.detect_from_corners(corners)\n",
        "\n",
        "        if sample.id not in cache['keys_px']:\n",
        "            cache['keys_px'][sample.id] = project_keys_to_pixel_space(kb.key_boundaries, kb.homography)\n",
        "        kb_px = cache['keys_px'][sample.id]\n",
        "\n",
        "        if sample.id not in cache['filtered_landmarks']:\n",
        "            if sample.id not in cache['skeleton']:\n",
        "                cache['skeleton'][sample.id] = dataset.load_skeleton(sample)\n",
        "            skel = cache['skeleton'][sample.id]\n",
        "\n",
        "            ldr = SkeletonLoader()\n",
        "            h = ldr._parse_json(skel)\n",
        "            la = ldr.to_array(h['left'])\n",
        "            ra = ldr.to_array(h['right'])\n",
        "\n",
        "            t = TemporalFilter(\n",
        "                hampel_window=config.hand.hampel_window,\n",
        "                hampel_threshold=config.hand.hampel_threshold,\n",
        "                max_interpolation_gap=config.hand.interpolation_max_gap,\n",
        "                savgol_window=config.hand.savgol_window,\n",
        "                savgol_order=config.hand.savgol_order\n",
        "            )\n",
        "            if la.size > 0: la = t.process(la)\n",
        "            if ra.size > 0: ra = t.process(ra)\n",
        "            if la.size > 0: la = la.copy(); la[:,:,0] *= FRAME_W; la[:,:,1] *= FRAME_H\n",
        "            if ra.size > 0: ra = ra.copy(); ra[:,:,0] *= FRAME_W; ra[:,:,1] *= FRAME_H\n",
        "            cache['filtered_landmarks'][sample.id] = (la, ra)\n",
        "\n",
        "        la, ra = cache['filtered_landmarks'][sample.id]\n",
        "        if max_duration_sec:\n",
        "            mf = int(max_duration_sec * config.video_fps)\n",
        "            if la.size > 0: la = la[:mf]\n",
        "            if ra.size > 0: ra = ra[:mf]\n",
        "\n",
        "        if sample.id not in cache['tsv']:\n",
        "            cache['tsv'][sample.id] = dataset.load_tsv_annotations(sample)\n",
        "        tsv = cache['tsv'][sample.id]\n",
        "        if max_duration_sec:\n",
        "            tsv = tsv[tsv['onset'] <= float(max_duration_sec)].copy()\n",
        "\n",
        "        midi_evts = [{'onset': float(r['onset']), 'offset': float(r['onset'])+0.3,\n",
        "                      'pitch': int(r['note']),\n",
        "                      'velocity': int(r['velocity']) if 'velocity' in r and pd.notna(r['velocity']) else 64}\n",
        "                     for _, r in tsv.iterrows()]\n",
        "\n",
        "        sync = MidiVideoSync(fps=config.video_fps)\n",
        "        synced = sync.sync_events(midi_evts)\n",
        "\n",
        "        asgn = GaussianFingerAssigner(key_boundaries=kb_px, sigma=config.assignment.sigma,\n",
        "                                      candidate_range=config.assignment.candidate_keys)\n",
        "\n",
        "        for ev in synced:\n",
        "            fidx, kidx = ev.frame_idx, ev.key_idx\n",
        "            if kidx not in asgn.key_centers: continue\n",
        "            ar = None\n",
        "            if fidx < len(ra):\n",
        "                lm = ra[fidx]\n",
        "                if not np.any(np.isnan(lm)):\n",
        "                    ar = asgn.assign_from_landmarks(lm, kidx, 'right', fidx, ev.onset_time)\n",
        "            al = None\n",
        "            if fidx < len(la):\n",
        "                lm = la[fidx]\n",
        "                if not np.any(np.isnan(lm)):\n",
        "                    al = asgn.assign_from_landmarks(lm, kidx, 'left', fidx, ev.onset_time)\n",
        "            cands = [a for a in (ar, al) if a is not None]\n",
        "            if cands:\n",
        "                result['assignments'].append(max(cands, key=lambda a: a.confidence))\n",
        "    except Exception as e:\n",
        "        result['error'] = str(e)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_SAMPLES = 10\n",
        "MAX_DURATION_SEC = 120\n",
        "\n",
        "all_results = []\n",
        "for i, samp in enumerate(train_dataset):\n",
        "    if i >= NUM_SAMPLES: break\n",
        "    print(f'Processing {i+1}/{NUM_SAMPLES}: {samp.id} - {samp.metadata[\"piece\"][:40]}')\n",
        "    res = process_sample_baseline(samp, train_dataset, config, max_duration_sec=MAX_DURATION_SEC)\n",
        "    if res['error']:\n",
        "        print(f'  Error: {res[\"error\"][:100]}')\n",
        "    else:\n",
        "        print(f'  Assigned {len(res[\"assignments\"])} notes')\n",
        "    all_results.append(res)\n",
        "\n",
        "total_assigned = sum(len(r['assignments']) for r in all_results)\n",
        "print(f'\\nTotal assignments: {total_assigned}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_fingers = [a.assigned_finger for r in all_results for a in r['assignments']]\n",
        "all_hands = [a.hand for r in all_results for a in r['assignments']]\n",
        "all_confs = [a.confidence for r in all_results for a in r['assignments']]\n",
        "\n",
        "if all_fingers:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
        "    fc = pd.Series(all_fingers).value_counts().sort_index()\n",
        "    fc.plot.bar(ax=axes[0], color=[colors[i-1] for i in fc.index])\n",
        "    axes[0].set_title(f'Finger Distribution (n={len(all_fingers)})')\n",
        "    pd.Series(all_hands).value_counts().plot.bar(ax=axes[1], color=['coral', 'skyblue'])\n",
        "    axes[1].set_title('Hand Distribution')\n",
        "    axes[2].hist(all_confs, bins=30, color='mediumseagreen', edgecolor='white')\n",
        "    axes[2].axvline(np.mean(all_confs), color='red', ls='--', label=f'mean={np.mean(all_confs):.3f}')\n",
        "    axes[2].set_title('Confidence Distribution')\n",
        "    axes[2].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Keyboard Auto-Detection IoU Across Multiple Samples ──\n",
        "print('Evaluating auto-detection (Canny/Hough) IoU across samples ...\\n')\n",
        "\n",
        "auto_det = AutoKeyboardDetector({\n",
        "    'canny_low': config.keyboard.canny_low,\n",
        "    'canny_high': config.keyboard.canny_high,\n",
        "    'hough_threshold': config.keyboard.hough_threshold,\n",
        "    'num_sample_frames': 5,\n",
        "})\n",
        "\n",
        "iou_scores = []\n",
        "eval_ds = PianoVAMDataset(split='train', streaming=True, max_samples=NUM_SAMPLES)\n",
        "\n",
        "for i, samp in enumerate(eval_ds):\n",
        "    if i >= NUM_SAMPLES:\n",
        "        break\n",
        "    try:\n",
        "        vpath = eval_ds.download_file(samp.video_path)\n",
        "        res = auto_det.detect_from_video(str(vpath))\n",
        "        if res.success:\n",
        "            iou = auto_det.evaluate_against_corners(res, samp.metadata['keyboard_corners'])\n",
        "            iou_scores.append(iou)\n",
        "            print(f'  {samp.id}: IoU = {iou:.3f}  (bbox={res.consensus_bbox})')\n",
        "        else:\n",
        "            iou_scores.append(0.0)\n",
        "            print(f'  {samp.id}: FAILED')\n",
        "    except Exception as e:\n",
        "        iou_scores.append(0.0)\n",
        "        print(f'  {samp.id}: Error - {str(e)[:60]}')\n",
        "\n",
        "print(f'\\nAuto-detection IoU summary ({len(iou_scores)} samples):')\n",
        "print(f'  Mean IoU : {np.mean(iou_scores):.3f}')\n",
        "print(f'  Median   : {np.median(iou_scores):.3f}')\n",
        "print(f'  Min / Max: {np.min(iou_scores):.3f} / {np.max(iou_scores):.3f}')\n",
        "print(f'  Success  : {sum(1 for s in iou_scores if s > 0)}/{len(iou_scores)}')\n",
        "\n",
        "if iou_scores:\n",
        "    fig, ax = plt.subplots(figsize=(10, 4))\n",
        "    ax.bar(range(len(iou_scores)), iou_scores, color='steelblue', edgecolor='white')\n",
        "    ax.axhline(np.mean(iou_scores), color='red', ls='--', label=f'Mean = {np.mean(iou_scores):.3f}')\n",
        "    ax.set_xlabel('Sample')\n",
        "    ax.set_ylabel('IoU')\n",
        "    ax.set_title('Auto-Detection IoU vs Corner Annotations')\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='7'></a>\n",
        "## 7. Stage 5 - Neural Refinement (BiLSTM)\n",
        "\n",
        "Architecture: Input(20) -> Linear(128) -> BiLSTM(128 x 2 layers) -> Self-Attention -> Linear(128) -> Linear(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Preparing training sequences from baseline assignments ...')\n",
        "\n",
        "MAX_TRAIN_SAMPLES = 20\n",
        "train_sequences = []\n",
        "train_ds_full = PianoVAMDataset(split='train', streaming=True, max_samples=MAX_TRAIN_SAMPLES)\n",
        "\n",
        "for i, samp in enumerate(train_ds_full):\n",
        "    if i >= MAX_TRAIN_SAMPLES: break\n",
        "    res = process_sample_baseline(samp, train_ds_full, config, max_duration_sec=120)\n",
        "    asgns = res['assignments']\n",
        "    if len(asgns) < 10: continue\n",
        "    seq = {\n",
        "        'pitches': [a.midi_pitch for a in asgns],\n",
        "        'fingers': [a.assigned_finger for a in asgns],\n",
        "        'onsets': [a.note_onset for a in asgns],\n",
        "        'hands': [a.hand for a in asgns],\n",
        "        'labels': [a.assigned_finger for a in asgns],\n",
        "    }\n",
        "    train_sequences.append(seq)\n",
        "\n",
        "print(f'Training sequences: {len(train_sequences)}')\n",
        "print(f'Total notes: {sum(len(s[\"pitches\"]) for s in train_sequences)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor(normalize_pitch=True)\n",
        "input_size = feature_extractor.get_input_size()\n",
        "\n",
        "trained_model = None\n",
        "if len(train_sequences) > 2:\n",
        "    split_idx = max(1, int(0.8 * len(train_sequences)))\n",
        "    train_seqs = train_sequences[:split_idx]\n",
        "    val_seqs = train_sequences[split_idx:]\n",
        "\n",
        "    train_torch_ds = SequenceDataset(train_seqs, feature_extractor, max_len=256)\n",
        "    val_torch_ds = SequenceDataset(val_seqs, feature_extractor, max_len=256)\n",
        "\n",
        "    model = FingeringRefiner(\n",
        "        input_size=input_size,\n",
        "        hidden_size=config.refinement.hidden_size,\n",
        "        num_layers=config.refinement.num_layers,\n",
        "        dropout=config.refinement.dropout,\n",
        "        bidirectional=config.refinement.bidirectional\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "    print(model)\n",
        "\n",
        "    training_config = {\n",
        "        'hidden_size': config.refinement.hidden_size,\n",
        "        'num_layers': config.refinement.num_layers,\n",
        "        'dropout': config.refinement.dropout,\n",
        "        'batch_size': min(config.refinement.batch_size, len(train_torch_ds)),\n",
        "        'learning_rate': config.refinement.learning_rate,\n",
        "        'epochs': config.refinement.epochs,\n",
        "        'early_stopping_patience': config.refinement.early_stopping_patience,\n",
        "        'device': DEVICE,\n",
        "        'checkpoint_dir': '/content/checkpoints' if IN_COLAB else './outputs/checkpoints'\n",
        "    }\n",
        "\n",
        "    print('\\nTraining BiLSTM refinement model ...')\n",
        "    trained_model = train_refiner(\n",
        "        train_dataset=train_torch_ds,\n",
        "        val_dataset=val_torch_ds if len(val_torch_ds) > 0 else None,\n",
        "        config=training_config\n",
        "    )\n",
        "    print('Training complete')\n",
        "else:\n",
        "    print('Not enough data for training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def refine_assignments(model, assignments, feature_extractor, device='cpu', use_constraints=True):\n",
        "    if not assignments or model is None:\n",
        "        return assignments\n",
        "\n",
        "    pitches = [a.midi_pitch for a in assignments]\n",
        "    fingers = [a.assigned_finger for a in assignments]\n",
        "    onsets = [a.note_onset for a in assignments]\n",
        "    hands = [a.hand for a in assignments]\n",
        "\n",
        "    x = feature_extractor.extract(pitches, fingers, onsets, hands)\n",
        "    x = x.unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    if use_constraints:\n",
        "        decoded = constrained_viterbi_decode(\n",
        "            probs=probs, pitches=pitches, hands=hands,\n",
        "            constraints=BiomechanicalConstraints(strict=False)\n",
        "        )\n",
        "        pred_fingers = decoded.fingers\n",
        "    else:\n",
        "        pred_fingers = (np.argmax(probs, axis=-1) + 1).tolist()\n",
        "\n",
        "    confs = [float(probs[i, f - 1]) for i, f in enumerate(pred_fingers)]\n",
        "\n",
        "    return [FingerAssignment(\n",
        "        note_onset=a.note_onset, frame_idx=a.frame_idx, midi_pitch=a.midi_pitch,\n",
        "        key_idx=a.key_idx, assigned_finger=int(pred_fingers[i]), hand=a.hand,\n",
        "        confidence=float(confs[i]), fingertip_position=a.fingertip_position\n",
        "    ) for i, a in enumerate(assignments)]\n",
        "\n",
        "\n",
        "if trained_model is not None and all_results:\n",
        "    print('Refining baseline predictions ...')\n",
        "    for res in all_results:\n",
        "        if res['assignments']:\n",
        "            original = res['assignments']\n",
        "            refined = refine_assignments(trained_model, original, feature_extractor, DEVICE)\n",
        "            res['refined_assignments'] = refined\n",
        "            changed = sum(1 for o, r in zip(original, refined) if o.assigned_finger != r.assigned_finger)\n",
        "            print(f'  {res[\"sample_id\"]}: {changed}/{len(original)} changed')\n",
        "    print('Refinement done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='8'></a>\n",
        "## 8. Evaluation & Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = FingeringMetrics()\n",
        "constraints = BiomechanicalConstraints()\n",
        "\n",
        "print('=' * 70)\n",
        "print('EVALUATION RESULTS')\n",
        "print('=' * 70)\n",
        "\n",
        "baseline_ifrs = []\n",
        "refined_ifrs = []\n",
        "\n",
        "for res in all_results:\n",
        "    if not res['assignments']: continue\n",
        "    asgns = res['assignments']\n",
        "    pitches = [a.midi_pitch for a in asgns]\n",
        "    fingers = [a.assigned_finger for a in asgns]\n",
        "    hl = [a.hand for a in asgns]\n",
        "\n",
        "    violations = constraints.validate_sequence(fingers, pitches, hl)\n",
        "    ifr = len(violations) / max(1, len(asgns) - 1)\n",
        "    baseline_ifrs.append(ifr)\n",
        "    mc = np.mean([a.confidence for a in asgns])\n",
        "\n",
        "    msg = f'  {res[\"sample_id\"]} - {len(asgns)} notes | Baseline IFR={ifr:.3f} | conf={mc:.3f}'\n",
        "\n",
        "    if 'refined_assignments' in res:\n",
        "        ref = res['refined_assignments']\n",
        "        rf = [a.assigned_finger for a in ref]\n",
        "        rv = constraints.validate_sequence(rf, pitches, hl)\n",
        "        ri = len(rv) / max(1, len(ref) - 1)\n",
        "        refined_ifrs.append(ri)\n",
        "        msg += f' | Refined IFR={ri:.3f}'\n",
        "\n",
        "    print(msg)\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "if baseline_ifrs:\n",
        "    print(f'BASELINE Mean IFR: {np.mean(baseline_ifrs):.3f} +/- {np.std(baseline_ifrs):.3f}')\n",
        "if refined_ifrs:\n",
        "    print(f'REFINED  Mean IFR: {np.mean(refined_ifrs):.3f} +/- {np.std(refined_ifrs):.3f}')\n",
        "    imp = np.mean(baseline_ifrs) - np.mean(refined_ifrs)\n",
        "    print(f'Improvement: {imp:+.3f}')\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test set evaluation\n",
        "print('Processing test split ...\\n')\n",
        "test_ds_eval = PianoVAMDataset(split='test', streaming=True, max_samples=5)\n",
        "test_results = []\n",
        "\n",
        "for i, samp in enumerate(test_ds_eval):\n",
        "    print(f'  Test {i+1}: {samp.id}')\n",
        "    res = process_sample_baseline(samp, test_ds_eval, config)\n",
        "    if res['error']:\n",
        "        print(f'    Error: {res[\"error\"][:80]}')\n",
        "    else:\n",
        "        n = len(res['assignments'])\n",
        "        if trained_model is not None and n > 0:\n",
        "            res['refined_assignments'] = refine_assignments(\n",
        "                trained_model, res['assignments'], feature_extractor, DEVICE)\n",
        "        print(f'    {n} notes assigned')\n",
        "    test_results.append(res)\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "print('TEST SET RESULTS')\n",
        "print('=' * 70)\n",
        "\n",
        "test_baseline_ifrs = []\n",
        "test_refined_ifrs = []\n",
        "\n",
        "for res in test_results:\n",
        "    if not res['assignments']: continue\n",
        "    asgns = res['assignments']\n",
        "    pitches = [a.midi_pitch for a in asgns]\n",
        "    fingers = [a.assigned_finger for a in asgns]\n",
        "    hl = [a.hand for a in asgns]\n",
        "\n",
        "    viols = constraints.validate_sequence(fingers, pitches, hl)\n",
        "    ifr = len(viols) / max(1, len(asgns) - 1)\n",
        "    test_baseline_ifrs.append(ifr)\n",
        "\n",
        "    msg = f'  {res[\"sample_id\"]} - {len(asgns)} notes | Baseline IFR={ifr:.3f}'\n",
        "\n",
        "    if 'refined_assignments' in res:\n",
        "        ref = res['refined_assignments']\n",
        "        rf = [a.assigned_finger for a in ref]\n",
        "        rv = constraints.validate_sequence(rf, pitches, hl)\n",
        "        ri = len(rv) / max(1, len(ref) - 1)\n",
        "        test_refined_ifrs.append(ri)\n",
        "        msg += f' | Refined IFR={ri:.3f}'\n",
        "    print(msg)\n",
        "\n",
        "if test_baseline_ifrs:\n",
        "    print(f'\\nTEST Baseline Mean IFR: {np.mean(test_baseline_ifrs):.3f}')\n",
        "if test_refined_ifrs:\n",
        "    print(f'TEST Refined  Mean IFR: {np.mean(test_refined_ifrs):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary figure\n",
        "if baseline_ifrs:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    x = np.arange(len(baseline_ifrs))\n",
        "    w = 0.35\n",
        "    axes[0].bar(x - w/2, baseline_ifrs, w, label='Baseline', color='steelblue')\n",
        "    if refined_ifrs:\n",
        "        axes[0].bar(x + w/2, refined_ifrs, w, label='Refined', color='coral')\n",
        "    axes[0].set_xlabel('Sample')\n",
        "    axes[0].set_ylabel('IFR (lower = better)')\n",
        "    axes[0].set_title('IFR Comparison (Train Samples)')\n",
        "    axes[0].legend()\n",
        "\n",
        "    if all_confs:\n",
        "        axes[1].hist(all_confs, bins=30, color='mediumseagreen', edgecolor='white')\n",
        "        axes[1].axvline(np.mean(all_confs), color='red', ls='--', label=f'mean={np.mean(all_confs):.3f}')\n",
        "        axes[1].set_title('Confidence Distribution')\n",
        "        axes[1].legend()\n",
        "\n",
        "    plt.suptitle('Piano Fingering Detection - Results Summary', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "output_dir = Path('/content/outputs' if IN_COLAB else './outputs')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "results_summary = {\n",
        "    'pipeline': 'piano-fingering-detection',\n",
        "    'baseline_method': 'Gaussian Assignment (x-only, both hands, max-distance gate)',\n",
        "    'refinement_method': 'BiLSTM + Attention + Constrained Viterbi',\n",
        "    'test_results': []\n",
        "}\n",
        "\n",
        "for i, res in enumerate(test_results):\n",
        "    entry = {'sample_id': res['sample_id'], 'num_assignments': len(res.get('assignments', []))}\n",
        "    if i < len(test_baseline_ifrs):\n",
        "        entry['baseline_ifr'] = float(test_baseline_ifrs[i])\n",
        "    if i < len(test_refined_ifrs):\n",
        "        entry['refined_ifr'] = float(test_refined_ifrs[i])\n",
        "    results_summary['test_results'].append(entry)\n",
        "\n",
        "if test_baseline_ifrs:\n",
        "    results_summary['mean_baseline_ifr'] = float(np.mean(test_baseline_ifrs))\n",
        "if test_refined_ifrs:\n",
        "    results_summary['mean_refined_ifr'] = float(np.mean(test_refined_ifrs))\n",
        "\n",
        "with open(output_dir / 'evaluation_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "if trained_model is not None:\n",
        "    torch.save(trained_model.state_dict(), output_dir / 'refinement_model.pt')\n",
        "\n",
        "print(f'Results saved to {output_dir}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
