{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automatic Piano Fingering Detection from Video\n",
        "\n",
        "**Computer Vision Final Project ‚Äî Sapienza University of Rome**\n",
        "\n",
        "---\n",
        "\n",
        "## Project Goal\n",
        "\n",
        "Given a video of a piano performance with synchronized MIDI data, automatically determine the finger assignment (1‚Äì5, thumb to pinky) for each played note using **only computer vision techniques** ‚Äî no manual annotations are used in the detection pipeline itself.\n",
        "\n",
        "**Input**: Video + MIDI ‚Üí **Output**: Per-note finger labels (L1‚ÄìL5 for left hand, R1‚ÄìR5 for right hand)\n",
        "\n",
        "## Primary Reference\n",
        "\n",
        "> **Moryossef et al. (2023)** ‚Äî *\"At Your Fingertips: Extracting Piano Fingering Instructions from Videos\"* ‚Äî [arXiv:2303.03745](https://arxiv.org/abs/2303.03745)\n",
        "\n",
        "Our pipeline follows the methodology proposed in this paper:\n",
        "\n",
        "| Paper Methodology | Our Implementation |\n",
        "|---|---|\n",
        "| Video-based pipeline (Video ‚Üí Keyboard ‚Üí Hands ‚Üí Assignment) | Same 4-stage architecture |\n",
        "| Keyboard detection from video via edge/line analysis | Canny + Hough + line clustering + black-key analysis (`AutoKeyboardDetector`) |\n",
        "| MediaPipe 21-keypoint hand pose estimation | Live detection on raw video (model_complexity=1, conf=0.3, video mode) |\n",
        "| **Gaussian probability assignment using x-distance only** | `P(finger‚Üíkey) = exp(‚àídx¬≤/2œÉ¬≤)` with auto-scaled œÉ |\n",
        "| Max-distance gate (reject when hand is far) | 4œÉ rejection threshold |\n",
        "| Both-hands evaluation per note | Try L & R, pick higher confidence |\n",
        "| Temporal smoothing of landmarks | Hampel + interpolation + Savitzky-Golay |\n",
        "\n",
        "## Pipeline Architecture\n",
        "\n",
        "```\n",
        "Video ‚îÄ‚îÄ‚ñ∫ Keyboard Detection ‚îÄ‚îÄ‚ñ∫ Hand Processing ‚îÄ‚îÄ‚ñ∫ Finger-Key Assignment ‚îÄ‚îÄ‚ñ∫ Neural Refinement ‚îÄ‚îÄ‚ñ∫ Fingering Labels\n",
        "           (Canny/Hough/           (MediaPipe +        (Gaussian x-only          (BiLSTM +\n",
        "            Clustering)             Temporal Filter)     Probability)              Viterbi)\n",
        "```\n",
        "\n",
        "| Stage | Method | Input | Output |\n",
        "|-------|--------|-------|--------|\n",
        "| 1. Keyboard Detection | Canny + Hough + Clustering + Black-Key Analysis | Video frames | 88 key bounding boxes (pixel space) |\n",
        "| 2. Hand Processing | MediaPipe (live) + Hampel + SavGol | Raw video frames | Filtered landmarks (T √ó 21 √ó 3) |\n",
        "| 3. Finger Assignment | Gaussian probability (x-only) | MIDI + fingertips + keys | FingerAssignment per note |\n",
        "| 4. Neural Refinement | BiLSTM + Attention + Viterbi | Initial assignments | Refined predictions |\n",
        "\n",
        "> **Full-CV approach**: The keyboard is detected automatically from raw video ‚Äî no dataset annotations are used in the pipeline. Corner annotations from PianoVAM are used **only for evaluation** (IoU metric).\n",
        "\n",
        "## Dataset\n",
        "\n",
        "**PianoVAM** (KAIST) ‚Äî 107 piano performances with synchronized video, audio, MIDI, and pre-extracted hand skeletons.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "0. [Environment Setup](#0)\n",
        "1. [Data Exploration](#1)\n",
        "2. [Stage 1: Keyboard Detection ‚Äî Automatic CV Pipeline](#2)\n",
        "3. [Stage 2: Hand Pose Estimation (MediaPipe)](#3)\n",
        "4. [Stage 3: Temporal Filtering](#4)\n",
        "5. [Stage 4: Finger-Key Assignment (Gaussian)](#5)\n",
        "6. [Baseline Pipeline on Multiple Samples](#6)\n",
        "7. [Stage 5: Neural Refinement (BiLSTM)](#7)\n",
        "8. [Evaluation & Results](#8)\n",
        "9. [Extended Evaluation & Validation](#9)\n",
        "    - 9.1 Baseline Comparisons\n",
        "    - 9.2 Hand Detection Validation (Live MP vs. Dataset Skeletons)\n",
        "    - 9.3 Ablation Study\n",
        "    - 9.4 Qualitative Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='0'></a>\n",
        "## 0. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, subprocess\n",
        "\n",
        "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
        "\n",
        "if IN_COLAB:\n",
        "    REPO_URL = 'https://github.com/esnylmz/computer-vision.git'\n",
        "    BRANCH = 'v4'\n",
        "    if not os.path.exists('computer-vision'):\n",
        "        subprocess.run(['git', 'clone', '--branch', BRANCH, '--single-branch', REPO_URL], check=True)\n",
        "    os.chdir('computer-vision')\n",
        "    subprocess.run(['git', 'fetch', 'origin', BRANCH], check=True)\n",
        "    subprocess.run(['git', 'checkout', BRANCH], check=True)\n",
        "    subprocess.run(['git', 'pull', '--ff-only', 'origin', BRANCH], check=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-e', '.'], check=True)\n",
        "    # mediapipe-numpy2 keeps mp.solutions API and works with numpy 2.x on Colab\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'mediapipe-numpy2'], check=True)\n",
        "    print('\\nColab environment ready')\n",
        "else:\n",
        "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "    if PROJECT_ROOT not in sys.path:\n",
        "        sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "    # make sure we have a compatible mediapipe (solutions API removed in 0.10.31+)\n",
        "    try:\n",
        "        import mediapipe as _mp\n",
        "        if not hasattr(_mp, 'solutions'):\n",
        "            print('WARNING: mediapipe version too new, reinstalling compatible version...')\n",
        "            subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'mediapipe-numpy2'], check=True)\n",
        "    except ImportError:\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'mediapipe-numpy2'], check=True)\n",
        "\n",
        "    print('Local environment ready')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import json, time, warnings\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(f'NumPy  : {np.__version__}')\n",
        "print(f'Pandas : {pd.__version__}')\n",
        "print(f'OpenCV : {cv2.__version__}')\n",
        "\n",
        "import torch\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device : {DEVICE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data.dataset import PianoVAMDataset, PianoVAMSample\n",
        "from src.data.midi_utils import MidiProcessor, MidiEvent\n",
        "from src.data.video_utils import VideoProcessor\n",
        "from src.utils.config import load_config, Config\n",
        "\n",
        "from src.keyboard.detector import KeyboardDetector, KeyboardRegion\n",
        "from src.keyboard.auto_detector import AutoKeyboardDetector, AutoDetectionResult\n",
        "from src.keyboard.homography import HomographyComputer\n",
        "from src.keyboard.key_localization import KeyLocalizer\n",
        "\n",
        "from src.hand.skeleton_loader import SkeletonLoader, HandLandmarks\n",
        "from src.hand.temporal_filter import TemporalFilter\n",
        "from src.hand.fingertip_extractor import FingertipExtractor, FingertipData\n",
        "from src.hand.live_detector import LiveHandDetector, LiveDetectionConfig\n",
        "\n",
        "from src.assignment.gaussian_assignment import GaussianFingerAssigner, FingerAssignment\n",
        "from src.assignment.midi_sync import MidiVideoSync\n",
        "from src.assignment.hand_separation import HandSeparator\n",
        "\n",
        "from src.refinement.model import FingeringRefiner, FeatureExtractor, SequenceDataset\n",
        "from src.refinement.constraints import BiomechanicalConstraints\n",
        "from src.refinement.decoding import constrained_viterbi_decode\n",
        "from src.refinement.train import train_refiner, collate_fn\n",
        "\n",
        "from src.evaluation.metrics import FingeringMetrics, EvaluationResult, aggregate_results\n",
        "from src.evaluation.visualization import ResultVisualizer\n",
        "\n",
        "from src.pipeline import FingeringPipeline\n",
        "\n",
        "config_path = 'configs/colab.yaml' if IN_COLAB else 'configs/default.yaml'\n",
        "config = load_config(config_path)\n",
        "print(f'All modules imported | Config: {config_path}')\n",
        "print(f'Project: {config.project_name} v{config.version}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='1'></a>\n",
        "## 1. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_EXPLORE = 20\n",
        "\n",
        "print('Loading PianoVAM dataset splits ...\\n')\n",
        "train_dataset = PianoVAMDataset(split='train', streaming=True, max_samples=MAX_EXPLORE)\n",
        "val_dataset = PianoVAMDataset(split='validation', streaming=True, max_samples=MAX_EXPLORE)\n",
        "test_dataset = PianoVAMDataset(split='test', streaming=True, max_samples=MAX_EXPLORE)\n",
        "\n",
        "sample = next(iter(train_dataset))\n",
        "print(f'\\nSample ID      : {sample.id}')\n",
        "print(f'Composer       : {sample.metadata[\"composer\"]}')\n",
        "print(f'Piece          : {sample.metadata[\"piece\"]}')\n",
        "print(f'Skill Level    : {sample.metadata[\"skill_level\"]}')\n",
        "print(f'Keyboard Corners: {sample.metadata[\"keyboard_corners\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Collecting dataset statistics ...')\n",
        "stats_ds = PianoVAMDataset(split='train', max_samples=None)\n",
        "\n",
        "composers, skill_levels = [], []\n",
        "for s in stats_ds:\n",
        "    composers.append(s.metadata['composer'])\n",
        "    skill_levels.append(s.metadata['skill_level'])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "pd.Series(skill_levels).value_counts().plot.bar(ax=axes[0], color='steelblue')\n",
        "axes[0].set_title(f'Skill Level Distribution (n={len(skill_levels)})')\n",
        "pd.Series(composers).value_counts().head(10).plot.barh(ax=axes[1], color='darkorange')\n",
        "axes[1].set_title('Top 10 Composers')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Train samples    : {len(skill_levels)}')\n",
        "print(f'Unique composers : {len(set(composers))}')\n",
        "print(f'Skill levels     : {dict(pd.Series(skill_levels).value_counts())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='2'></a>\n",
        "## 2. Stage 1 ‚Äî Keyboard Detection (Automatic CV Pipeline)\n",
        "\n",
        "We detect the piano keyboard from raw video using **only classical computer vision** ‚Äî no dataset annotations are used in the detection itself. This follows the video-based detection approach from Moryossef et al. (2023).\n",
        "\n",
        "### Detection Pipeline\n",
        "1. **Preprocessing** ‚Äî Grayscale ‚Üí CLAHE contrast enhancement ‚Üí Gaussian blur\n",
        "2. **Canny edge detection** ‚Äî Otsu-adaptive thresholds merged with fixed thresholds\n",
        "3. **Morphological closing** ‚Äî horizontal kernel to connect fragmented edges\n",
        "4. **Hough line transform** ‚Äî extract horizontal and vertical line segments\n",
        "5. **Line clustering** ‚Äî group horizontal lines by y-coordinate, select top/bottom keyboard edges\n",
        "6. **Black-key refinement** ‚Äî contour analysis to tighten x-boundaries\n",
        "7. **Multi-frame consensus** ‚Äî sample N frames, take median bbox for robustness\n",
        "8. **88-key layout** ‚Äî divide detected region into 52 white + 36 black keys in pixel space\n",
        "\n",
        "> **Evaluation only**: PianoVAM corner annotations are used solely to compute IoU (Intersection-over-Union) as a detection quality metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download a video frame from PianoVAM\n",
        "print(f'Downloading video for sample {sample.id} ...')\n",
        "video_path = train_dataset.download_file(sample.video_path)\n",
        "print(f'Video saved to: {video_path}')\n",
        "\n",
        "vp = VideoProcessor()\n",
        "vp.open(video_path)\n",
        "print(f'Resolution: {vp.info.width}x{vp.info.height}')\n",
        "print(f'FPS: {vp.info.fps}')\n",
        "print(f'Total frames: {vp.info.frame_count}')\n",
        "print(f'Duration: {vp.info.duration:.1f}s')\n",
        "\n",
        "# grab a frame from the middle of the video\n",
        "mid_frame_idx = vp.info.frame_count // 2\n",
        "frame_bgr = vp.get_frame(mid_frame_idx)\n",
        "frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.imshow(frame_rgb)\n",
        "plt.title(f'Raw Video Frame (frame {mid_frame_idx})')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "vp.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Image Preprocessing Pipeline ‚îÄ‚îÄ\n",
        "gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# CLAHE (Contrast-Limited Adaptive Histogram Equalisation) for lighting normalisation\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "enhanced = clahe.apply(gray)\n",
        "enhanced_blur = cv2.GaussianBlur(enhanced, (5, 5), 0)\n",
        "\n",
        "# Canny edge detection ‚Äî fixed thresholds for comparison\n",
        "edges_low = cv2.Canny(blurred, 30, 100)\n",
        "edges_mid = cv2.Canny(blurred, 50, 150)\n",
        "edges_high = cv2.Canny(blurred, 100, 200)\n",
        "\n",
        "# Otsu-based automatic threshold on CLAHE-enhanced image\n",
        "otsu_thresh, _ = cv2.threshold(enhanced_blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "otsu_low = max(10, int(otsu_thresh * 0.5))\n",
        "otsu_high = min(255, int(otsu_thresh * 1.0))\n",
        "edges_otsu = cv2.Canny(enhanced_blur, otsu_low, otsu_high)\n",
        "\n",
        "# Merge fixed + Otsu edges for robustness\n",
        "edges_merged = cv2.bitwise_or(edges_mid, edges_otsu)\n",
        "\n",
        "# Morphological closing with horizontal kernel to connect fragmented edges\n",
        "kernel_h = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 1))\n",
        "edges_closed = cv2.morphologyEx(edges_merged, cv2.MORPH_CLOSE, kernel_h)\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
        "\n",
        "axes[0, 0].imshow(frame_rgb);               axes[0, 0].set_title('Original (RGB)')\n",
        "axes[0, 1].imshow(gray, cmap='gray');        axes[0, 1].set_title('Grayscale')\n",
        "axes[0, 2].imshow(enhanced, cmap='gray');    axes[0, 2].set_title('CLAHE Enhanced')\n",
        "\n",
        "axes[1, 0].imshow(edges_low, cmap='gray');   axes[1, 0].set_title('Canny (30, 100)')\n",
        "axes[1, 1].imshow(edges_mid, cmap='gray');   axes[1, 1].set_title('Canny (50, 150)')\n",
        "axes[1, 2].imshow(edges_high, cmap='gray');  axes[1, 2].set_title('Canny (100, 200)')\n",
        "\n",
        "axes[2, 0].imshow(edges_otsu, cmap='gray');  axes[2, 0].set_title(f'Otsu-adaptive ({otsu_low}, {otsu_high})')\n",
        "axes[2, 1].imshow(edges_merged, cmap='gray');axes[2, 1].set_title('Merged (fixed + Otsu)')\n",
        "axes[2, 2].imshow(edges_closed, cmap='gray');axes[2, 2].set_title('After Morphological Close')\n",
        "\n",
        "for ax in axes.flat:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Image Preprocessing & Edge Detection Pipeline', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Otsu threshold: {otsu_thresh:.0f}  ‚Üí  Canny range: ({otsu_low}, {otsu_high})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hough Line Transform on the edge map\n",
        "edges = edges_mid\n",
        "\n",
        "lines = cv2.HoughLinesP(\n",
        "    edges, rho=1, theta=np.pi/180, threshold=100,\n",
        "    minLineLength=100, maxLineGap=10\n",
        ")\n",
        "\n",
        "print(f'Total lines detected: {len(lines) if lines is not None else 0}')\n",
        "\n",
        "# separate horizontal and vertical lines\n",
        "line_vis = frame_rgb.copy()\n",
        "horizontal_lines = []\n",
        "vertical_lines = []\n",
        "\n",
        "if lines is not None:\n",
        "    for line in lines:\n",
        "        x1, y1, x2, y2 = line[0]\n",
        "        angle = np.abs(np.arctan2(y2 - y1, x2 - x1))\n",
        "        if angle < np.pi / 18:  # within 10 degrees of horizontal\n",
        "            horizontal_lines.append((x1, y1, x2, y2))\n",
        "            cv2.line(line_vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        elif angle > np.pi / 2 - np.pi / 18:  # within 10 degrees of vertical\n",
        "            vertical_lines.append((x1, y1, x2, y2))\n",
        "            cv2.line(line_vis, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
        "\n",
        "print(f'Horizontal lines: {len(horizontal_lines)}')\n",
        "print(f'Vertical lines  : {len(vertical_lines)}')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "axes[0].imshow(edges, cmap='gray')\n",
        "axes[0].set_title('Canny Edge Map')\n",
        "axes[1].imshow(line_vis)\n",
        "axes[1].set_title('Hough Lines (green=horizontal, red=vertical)')\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Automatic Keyboard Detection (PRIMARY ‚Äî no annotations used) ‚îÄ‚îÄ\n",
        "auto_detector = AutoKeyboardDetector({\n",
        "    'canny_low': config.keyboard.canny_low,\n",
        "    'canny_high': config.keyboard.canny_high,\n",
        "    'hough_threshold': config.keyboard.hough_threshold,\n",
        "})\n",
        "\n",
        "# Run auto-detection on a single frame\n",
        "auto_result = auto_detector.detect_single_frame(frame_bgr, return_intermediates=True)\n",
        "\n",
        "if auto_result.success:\n",
        "    keyboard_region = auto_result.keyboard_region\n",
        "    print(f'‚úÖ Auto-detection succeeded')\n",
        "    print(f'  Bounding box    : {auto_result.consensus_bbox}')\n",
        "    print(f'  Keys detected   : {len(keyboard_region.key_boundaries)}')\n",
        "    print(f'  White key width : {keyboard_region.white_key_width:.1f} px')\n",
        "    print(f'  Horiz. lines    : {len(auto_result.horizontal_lines or [])}')\n",
        "    print(f'  Line clusters   : {len(auto_result.line_clusters or [])}')\n",
        "    print(f'  Black key cands : {len(auto_result.black_key_contours or [])}')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  Single-frame detection failed ‚Äî falling back to multi-frame consensus')\n",
        "    auto_result = auto_detector.detect_from_video(video_path, return_intermediates=True)\n",
        "    if auto_result.success:\n",
        "        keyboard_region = auto_result.keyboard_region\n",
        "        print(f'‚úÖ Multi-frame consensus succeeded: bbox={auto_result.consensus_bbox}')\n",
        "    else:\n",
        "        raise RuntimeError('Keyboard auto-detection failed on this video')\n",
        "\n",
        "# ‚îÄ‚îÄ Evaluation: IoU against corner annotations (annotations used ONLY here) ‚îÄ‚îÄ\n",
        "corners = sample.metadata['keyboard_corners']\n",
        "iou = auto_detector.evaluate_against_corners(auto_result, corners)\n",
        "print(f'\\nüìê IoU vs corner annotations (evaluation only): {iou:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Visualize Auto-Detection Intermediates ‚îÄ‚îÄ\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "# 1) Hough lines on original frame\n",
        "line_vis = auto_detector.visualize_lines(frame_bgr, auto_result)\n",
        "axes[0, 0].imshow(cv2.cvtColor(line_vis, cv2.COLOR_BGR2RGB))\n",
        "axes[0, 0].set_title(f'Hough Lines (green=horiz, red=vert)')\n",
        "\n",
        "# 2) Line clusters with selected top/bottom\n",
        "cluster_vis = auto_detector.visualize_clusters(frame_bgr, auto_result)\n",
        "axes[0, 1].imshow(cv2.cvtColor(cluster_vis, cv2.COLOR_BGR2RGB))\n",
        "axes[0, 1].set_title('Line Clusters & Selected Edges (cyan)')\n",
        "\n",
        "# 3) Black key contours\n",
        "bk_vis = auto_detector.visualize_black_keys(frame_bgr, auto_result)\n",
        "axes[1, 0].imshow(cv2.cvtColor(bk_vis, cv2.COLOR_BGR2RGB))\n",
        "axes[1, 0].set_title('Black-Key Contours (boundary refinement)')\n",
        "\n",
        "# 4) Final detection vs corner ground truth\n",
        "corner_det = KeyboardDetector()\n",
        "corner_region = corner_det.detect_from_corners(corners)\n",
        "det_vis = auto_detector.visualize_detection(frame_bgr, auto_result, corner_bbox=corner_region.bbox)\n",
        "axes[1, 1].imshow(cv2.cvtColor(det_vis, cv2.COLOR_BGR2RGB))\n",
        "axes[1, 1].set_title(f'Auto (green) vs Corner GT (red) ‚Äî IoU={iou:.3f}')\n",
        "\n",
        "for ax in axes.flat:\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Automatic Keyboard Detection Pipeline ‚Äî Intermediates', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Multi-Frame Consensus Detection ‚îÄ‚îÄ\n",
        "# Sample multiple frames across the video and take the median bbox.\n",
        "# This compensates for temporary occlusions (hands, page turns, etc.)\n",
        "\n",
        "multi_result = auto_detector.detect_from_video(video_path, return_intermediates=False)\n",
        "\n",
        "if multi_result.success:\n",
        "    print(f'‚úÖ Multi-frame consensus bbox: {multi_result.consensus_bbox}')\n",
        "    valid_bboxes = [b for b in (multi_result.per_frame_bboxes or []) if b is not None]\n",
        "    print(f'   Frames sampled: {len(multi_result.per_frame_bboxes or [])}')\n",
        "    print(f'   Successful detections: {len(valid_bboxes)}')\n",
        "\n",
        "    # Update keyboard_region to use multi-frame consensus (more robust)\n",
        "    keyboard_region = multi_result.keyboard_region\n",
        "\n",
        "    multi_iou = auto_detector.evaluate_against_corners(multi_result, corners)\n",
        "    print(f'   IoU vs corner GT: {multi_iou:.3f}')\n",
        "\n",
        "    # Show per-frame bboxes\n",
        "    if multi_result.per_frame_bboxes:\n",
        "        print('\\n   Per-frame detections:')\n",
        "        for i, bb in enumerate(multi_result.per_frame_bboxes):\n",
        "            status = f'bbox={bb}' if bb is not None else 'FAILED'\n",
        "            print(f'     Frame {i}: {status}')\n",
        "else:\n",
        "    print('Multi-frame consensus failed ‚Äî keeping single-frame result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Homography & Perspective Correction ‚îÄ‚îÄ\n",
        "# The auto-detected bbox defines the keyboard region in pixel space.\n",
        "# We compute a homography to warp it into a normalised rectangle for visualisation.\n",
        "\n",
        "H = keyboard_region.homography\n",
        "x1, y1, x2, y2 = keyboard_region.bbox\n",
        "kb_width = x2 - x1\n",
        "kb_height = y2 - y1\n",
        "\n",
        "warped = cv2.warpPerspective(frame_bgr, H, (kb_width, kb_height))\n",
        "warped_rgb = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Draw auto-detected keyboard boundary on the original frame\n",
        "bbox_vis = frame_rgb.copy()\n",
        "if keyboard_region.corners:\n",
        "    pts = [keyboard_region.corners[k] for k in ['LT', 'RT', 'RB', 'LB']]\n",
        "    for i in range(4):\n",
        "        p1 = pts[i]\n",
        "        p2 = pts[(i + 1) % 4]\n",
        "        cv2.line(bbox_vis, p1, p2, (0, 255, 0), 3)\n",
        "        cv2.circle(bbox_vis, p1, 8, (255, 0, 0), -1)\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
        "axes[0].imshow(bbox_vis)\n",
        "axes[0].set_title('Auto-Detected Keyboard Region')\n",
        "axes[1].imshow(warped_rgb)\n",
        "axes[1].set_title(f'Perspective-Corrected Keyboard ({kb_width}√ó{kb_height} px)')\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Keyboard bbox: ({x1}, {y1}) ‚Üí ({x2}, {y2})  |  {kb_width}√ó{kb_height} px')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Visualize 88-key layout in pixel space (from auto-detection) ‚îÄ‚îÄ\n",
        "localizer = KeyLocalizer(keyboard_region.key_boundaries)\n",
        "white_keys = localizer.get_white_keys()\n",
        "black_keys = localizer.get_black_keys()\n",
        "\n",
        "print(f'White keys: {len(white_keys)}  |  Black keys: {len(black_keys)}')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 3))\n",
        "for ki in white_keys:\n",
        "    kx1, ky1, kx2, ky2 = ki.bbox\n",
        "    ax.add_patch(plt.Rectangle((kx1, ky1), kx2 - kx1, ky2 - ky1, linewidth=0.8,\n",
        "                               edgecolor='black', facecolor='white'))\n",
        "for ki in black_keys:\n",
        "    kx1, ky1, kx2, ky2 = ki.bbox\n",
        "    ax.add_patch(plt.Rectangle((kx1, ky1), kx2 - kx1, ky2 - ky1, linewidth=0.5,\n",
        "                               edgecolor='black', facecolor='#333'))\n",
        "\n",
        "for note_name in ['A0', 'C4', 'C8']:\n",
        "    ki = localizer.get_key_by_name(note_name)\n",
        "    if ki:\n",
        "        ax.annotate(ki.note_name, xy=ki.center, fontsize=7, color='red', ha='center', va='bottom')\n",
        "\n",
        "bx1, by1, bx2, by2 = keyboard_region.bbox\n",
        "ax.set_xlim(bx1 - 10, bx2 + 10)\n",
        "ax.set_ylim(by2 + 10, by1 - 10)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_title('88-Key Layout in Pixel Space (Auto-Detected)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='3'></a>\n",
        "## 3. Stage 2 ‚Äî Hand Pose Estimation (Live MediaPipe)\n",
        "\n",
        "We run **MediaPipe Hands directly on the raw video** to detect 21 hand landmarks per hand ‚Äî no pre-extracted skeleton data from the dataset is used.\n",
        "\n",
        "Key parameters for robust detection:\n",
        "- **`model_complexity=1`** ‚Äî full model for higher accuracy\n",
        "- **`min_detection_confidence=0.3`** ‚Äî lower threshold catches partially-occluded and fast-moving hands\n",
        "- **`static_image_mode=False`** (video mode) ‚Äî enables temporal tracking across consecutive frames, dramatically improving detection when hands are in motion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "print(f'MediaPipe version: {mp.__version__}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Run MediaPipe hand detection on video frames ‚îÄ‚îÄ\n",
        "# Key improvements over default MediaPipe:\n",
        "#   - model_complexity=1 ‚Üí full model (more accurate keypoints)\n",
        "#   - min_detection_confidence=0.3 ‚Üí detect partially-occluded / fast-moving hands\n",
        "#   - min_tracking_confidence=0.3 ‚Üí maintain tracking through motion blur\n",
        "\n",
        "vp = VideoProcessor()\n",
        "vp.open(video_path)\n",
        "total_frames = int(vp.info.frame_count)\n",
        "\n",
        "# Pick frames spread evenly (skip first/last 5%)\n",
        "n_demo = 5\n",
        "margin = max(1, total_frames // 20)\n",
        "sample_frame_indices = np.linspace(margin, total_frames - margin, n_demo, dtype=int).tolist()\n",
        "sample_frames = []\n",
        "for idx in sample_frame_indices:\n",
        "    f = vp.get_frame(idx)\n",
        "    if f is not None:\n",
        "        sample_frames.append((idx, f))\n",
        "\n",
        "vp.close()\n",
        "\n",
        "hands_detector = mp_hands.Hands(\n",
        "    static_image_mode=True,       # True for individual frames\n",
        "    max_num_hands=2,\n",
        "    model_complexity=1,           # full model for higher accuracy\n",
        "    min_detection_confidence=0.3, # lower ‚Üí catches more hands\n",
        "    min_tracking_confidence=0.3\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, len(sample_frames), figsize=(5 * len(sample_frames), 6))\n",
        "if len(sample_frames) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, (fidx, frame) in enumerate(sample_frames):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = hands_detector.process(frame_rgb)\n",
        "\n",
        "    annotated = frame_rgb.copy()\n",
        "    n_hands = 0\n",
        "    if results.multi_hand_landmarks:\n",
        "        n_hands = len(results.multi_hand_landmarks)\n",
        "        for hand_lm in results.multi_hand_landmarks:\n",
        "            mp_drawing.draw_landmarks(\n",
        "                annotated, hand_lm, mp_hands.HAND_CONNECTIONS,\n",
        "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "                mp_drawing_styles.get_default_hand_connections_style()\n",
        "            )\n",
        "\n",
        "    axes[i].imshow(annotated)\n",
        "    axes[i].set_title(f'Frame {fidx} ({n_hands} hands)')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('MediaPipe Hand Detection (model_complexity=1, conf=0.3)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "hands_detector.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract landmarks from MediaPipe and visualize\n",
        "# Focus on one frame to show the 21-keypoint structure\n",
        "\n",
        "demo_frame_bgr = sample_frames[2][1] if len(sample_frames) > 2 else sample_frames[0][1]\n",
        "demo_frame_rgb = cv2.cvtColor(demo_frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "hands_detector = mp_hands.Hands(static_image_mode=True, max_num_hands=2, model_complexity=1,\n",
        "                                min_detection_confidence=0.3, min_tracking_confidence=0.3)\n",
        "results = hands_detector.process(demo_frame_rgb)\n",
        "hands_detector.close()\n",
        "\n",
        "h, w = demo_frame_rgb.shape[:2]\n",
        "annotated = demo_frame_rgb.copy()\n",
        "\n",
        "fingertip_indices = [4, 8, 12, 16, 20]\n",
        "finger_names = {4: 'Thumb', 8: 'Index', 12: 'Middle', 16: 'Ring', 20: 'Pinky'}\n",
        "\n",
        "if results.multi_hand_landmarks:\n",
        "    for hand_lm, hand_info in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
        "        label = hand_info.classification[0].label\n",
        "        mp_drawing.draw_landmarks(annotated, hand_lm, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "        # mark fingertips\n",
        "        for tip_idx in fingertip_indices:\n",
        "            lm = hand_lm.landmark[tip_idx]\n",
        "            px, py = int(lm.x * w), int(lm.y * h)\n",
        "            cv2.circle(annotated, (px, py), 8, (255, 0, 0), -1)\n",
        "            cv2.putText(annotated, finger_names[tip_idx], (px + 10, py - 5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
        "\n",
        "        print(f'{label} hand detected:')\n",
        "        for tip_idx in fingertip_indices:\n",
        "            lm = hand_lm.landmark[tip_idx]\n",
        "            print(f'  {finger_names[tip_idx]:6s} tip: ({lm.x:.4f}, {lm.y:.4f}, {lm.z:.4f})')\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.imshow(annotated)\n",
        "plt.title('MediaPipe 21-Keypoint Hand Skeleton with Fingertip Labels')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Run LIVE MediaPipe detection on the full video ‚îÄ‚îÄ\n",
        "# This is our primary hand-detection method ‚Äî no pre-extracted skeletons.\n",
        "# We use video mode (static_image_mode=False) for temporal tracking,\n",
        "# which dramatically improves detection across consecutive frames.\n",
        "\n",
        "print(f'Running live MediaPipe hand detection on {video_path} ...')\n",
        "print(f'  model_complexity=1, min_detection_conf=0.3, stride=2')\n",
        "\n",
        "live_cfg = LiveDetectionConfig(\n",
        "    model_complexity=1,\n",
        "    min_detection_confidence=0.3,\n",
        "    min_tracking_confidence=0.3,\n",
        "    frame_stride=2,            # process every 2nd frame for speed\n",
        "    static_image_mode=False,   # video mode ‚Üí temporal tracking\n",
        ")\n",
        "live_det = LiveHandDetector(config=live_cfg)\n",
        "left_raw, right_raw = live_det.detect_from_video(\n",
        "    video_path,\n",
        "    progress_callback=lambda cur, tot: print(f'  frame {cur}/{tot}', end='\\r')\n",
        ")\n",
        "\n",
        "left_rate = LiveHandDetector.detection_rate(left_raw)\n",
        "right_rate = LiveHandDetector.detection_rate(right_raw)\n",
        "\n",
        "print(f'\\nLive MediaPipe results:')\n",
        "print(f'  Left  hand: {left_raw.shape}, detection rate = {left_rate:.1%}')\n",
        "print(f'  Right hand: {right_raw.shape}, detection rate = {right_rate:.1%}')\n",
        "print(f'  Coordinates are normalised [0, 1] ‚Äî same format as SkeletonLoader')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Overlay live-detected landmarks on a demo frame ‚îÄ‚îÄ\n",
        "demo_fidx = sample_frames[2][0] if len(sample_frames) > 2 else sample_frames[0][0]\n",
        "demo_frame_bgr2 = sample_frames[2][1] if len(sample_frames) > 2 else sample_frames[0][1]\n",
        "comparison = cv2.cvtColor(demo_frame_bgr2, cv2.COLOR_BGR2RGB).copy()\n",
        "h, w = comparison.shape[:2]\n",
        "\n",
        "for hand_key, color in [('right', (0, 255, 0)), ('left', (0, 200, 255))]:\n",
        "    arr = right_raw if hand_key == 'right' else left_raw\n",
        "    if demo_fidx < len(arr) and not np.any(np.isnan(arr[demo_fidx])):\n",
        "        lm = arr[demo_fidx]\n",
        "        for j in range(21):\n",
        "            px = int(lm[j, 0] * w)\n",
        "            py = int(lm[j, 1] * h)\n",
        "            cv2.circle(comparison, (px, py), 4, color, -1)\n",
        "        connections = [(0,1),(1,2),(2,3),(3,4),(0,5),(5,6),(6,7),(7,8),\n",
        "                       (5,9),(9,10),(10,11),(11,12),(9,13),(13,14),(14,15),(15,16),\n",
        "                       (13,17),(17,18),(18,19),(19,20),(0,17)]\n",
        "        for c1, c2 in connections:\n",
        "            p1 = (int(lm[c1, 0] * w), int(lm[c1, 1] * h))\n",
        "            p2 = (int(lm[c2, 0] * w), int(lm[c2, 1] * h))\n",
        "            cv2.line(comparison, p1, p2, color, 2)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.imshow(comparison)\n",
        "plt.title(f'Live MediaPipe Detection Overlay (green=right, cyan=left) ‚Äî Frame {demo_fidx}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='4'></a>\n",
        "## 4. Stage 3 - Temporal Filtering\n",
        "\n",
        "MediaPipe landmarks are noisy. We apply a 3-stage filtering pipeline:\n",
        "1. Hampel filter (outlier detection via Median Absolute Deviation)\n",
        "2. Linear interpolation (fill gaps < 30 frames)\n",
        "3. Savitzky-Golay filter (smoothing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf = TemporalFilter(\n",
        "    hampel_window=config.hand.hampel_window,\n",
        "    hampel_threshold=config.hand.hampel_threshold,\n",
        "    max_interpolation_gap=config.hand.interpolation_max_gap,\n",
        "    savgol_window=config.hand.savgol_window,\n",
        "    savgol_order=config.hand.savgol_order\n",
        ")\n",
        "\n",
        "left_filtered = tf.process(left_raw) if left_raw.size > 0 else left_raw\n",
        "right_filtered = tf.process(right_raw) if right_raw.size > 0 else right_raw\n",
        "\n",
        "print('Filtering complete')\n",
        "print(f'Left  filtered shape: {left_filtered.shape}')\n",
        "print(f'Right filtered shape: {right_filtered.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize filtering effect: index fingertip x-coordinate\n",
        "hand_arr_raw = right_raw if right_raw.size > 0 else left_raw\n",
        "hand_arr_filt = right_filtered if right_filtered.size > 0 else left_filtered\n",
        "hand_label = 'Right' if right_raw.size > 0 else 'Left'\n",
        "\n",
        "lm_idx = 8  # index fingertip\n",
        "T = min(3000, len(hand_arr_raw))\n",
        "\n",
        "raw_signal = hand_arr_raw[:T, lm_idx, 0]\n",
        "filt_signal = hand_arr_filt[:T, lm_idx, 0]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 4))\n",
        "ax.plot(raw_signal, alpha=0.5, label='Raw', linewidth=0.5)\n",
        "ax.plot(filt_signal, label='Filtered', linewidth=1)\n",
        "ax.set_title(f'{hand_label} Hand - Index Fingertip X-Coordinate')\n",
        "ax.set_xlabel('Frame')\n",
        "ax.set_ylabel('X (normalized)')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optical flow: track fingertip motion between frames\n",
        "vp = VideoProcessor()\n",
        "vp.open(video_path)\n",
        "\n",
        "flow_start = 1000\n",
        "frame1_bgr = vp.get_frame(flow_start)\n",
        "frame2_bgr = vp.get_frame(flow_start + 5)\n",
        "vp.close()\n",
        "\n",
        "if frame1_bgr is not None and frame2_bgr is not None:\n",
        "    gray1 = cv2.cvtColor(frame1_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(frame2_bgr, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # crop to keyboard region for clearer visualization\n",
        "    y1, y2 = keyboard_region.bbox[1], keyboard_region.bbox[3]\n",
        "    x1, x2 = keyboard_region.bbox[0], keyboard_region.bbox[2]\n",
        "    gray1_crop = gray1[y1:y2, x1:x2]\n",
        "    gray2_crop = gray2[y1:y2, x1:x2]\n",
        "\n",
        "    flow = cv2.calcOpticalFlowFarneback(\n",
        "        gray1_crop, gray2_crop, None,\n",
        "        pyr_scale=0.5, levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.2, flags=0\n",
        "    )\n",
        "\n",
        "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    # HSV visualization\n",
        "    hsv = np.zeros((*gray1_crop.shape, 3), dtype=np.uint8)\n",
        "    hsv[..., 0] = angle * 180 / np.pi / 2\n",
        "    hsv[..., 1] = 255\n",
        "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    flow_vis = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    axes[0].imshow(cv2.cvtColor(frame1_bgr[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title(f'Frame {flow_start}')\n",
        "    axes[1].imshow(cv2.cvtColor(frame2_bgr[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
        "    axes[1].set_title(f'Frame {flow_start + 5}')\n",
        "    axes[2].imshow(flow_vis)\n",
        "    axes[2].set_title('Dense Optical Flow (Farneback)')\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "    plt.suptitle('Optical Flow: Hand Motion Between Frames', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Mean flow magnitude: {np.mean(magnitude):.2f} px/frame')\n",
        "    print(f'Max flow magnitude : {np.max(magnitude):.2f} px/frame')\n",
        "else:\n",
        "    print('Could not read frames for optical flow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fingertip extraction\n",
        "extractor = FingertipExtractor()\n",
        "\n",
        "sample_fidx = 500\n",
        "if sample_fidx < len(right_filtered) and not np.any(np.isnan(right_filtered[sample_fidx])):\n",
        "    ftips = extractor.extract(right_filtered[sample_fidx], frame_idx=sample_fidx, hand_type='right')\n",
        "    print(f'Frame {sample_fidx} - Right hand fingertips:')\n",
        "    for f_num in range(1, 6):\n",
        "        pos = ftips.get_position_2d(f_num)\n",
        "        if pos:\n",
        "            print(f'  {extractor.FINGER_NAMES[f_num]:6s}: ({pos[0]:.4f}, {pos[1]:.4f})')\n",
        "\n",
        "    span = extractor.compute_hand_span(ftips)\n",
        "    print(f'  Hand span: {span:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='5'></a>\n",
        "## 5. Stage 4 - Finger-Key Assignment\n",
        "\n",
        "Gaussian probability model in image-pixel space (not homography-warped space).\n",
        "Uses x-distance only to avoid y-bias from different finger lengths.\n",
        "Tries both hands for each key, picks the higher-confidence assignment.\n",
        "Max-distance gate rejects assignments when the hand is too far from the key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MIDI/TSV annotations\n",
        "print(f'Downloading TSV annotations for sample {sample.id} ...')\n",
        "tsv_df = train_dataset.load_tsv_annotations(sample)\n",
        "\n",
        "midi_events = []\n",
        "for _, row in tsv_df.iterrows():\n",
        "    midi_events.append({\n",
        "        'onset': float(row['onset']),\n",
        "        'offset': float(row['onset']) + 0.3,\n",
        "        'pitch': int(row['note']),\n",
        "        'velocity': int(row['velocity']) if 'velocity' in row and pd.notna(row['velocity']) else 64\n",
        "    })\n",
        "\n",
        "print(f'Total MIDI events: {len(midi_events)}')\n",
        "print(f'Pitch range: {min(e[\"pitch\"] for e in midi_events)} - {max(e[\"pitch\"] for e in midi_events)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synchronize MIDI events with video frames\n",
        "midi_sync = MidiVideoSync(fps=config.video_fps)\n",
        "synced_events = midi_sync.sync_events(midi_events)\n",
        "print(f'Synced events: {len(synced_events)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FRAME_W, FRAME_H = 1920, 1080\n",
        "\n",
        "# Auto-detected key boundaries are already in pixel space (no projection needed).\n",
        "# This is a key advantage of our full-CV approach:  the auto-detector computes\n",
        "# key positions directly in frame coordinates, matching the hand landmark space.\n",
        "key_boundaries_px = keyboard_region.key_boundaries\n",
        "\n",
        "# Scale hand landmarks from normalised [0,1] to pixel coordinates\n",
        "left_px = left_filtered.copy()\n",
        "left_px[:, :, 0] *= FRAME_W\n",
        "left_px[:, :, 1] *= FRAME_H\n",
        "\n",
        "right_px = right_filtered.copy()\n",
        "right_px[:, :, 0] *= FRAME_W\n",
        "right_px[:, :, 1] *= FRAME_H\n",
        "\n",
        "# Gaussian finger assigner (Moryossef et al. 2023 ‚Äî x-distance only)\n",
        "assigner = GaussianFingerAssigner(\n",
        "    key_boundaries=key_boundaries_px,\n",
        "    sigma=config.assignment.sigma,\n",
        "    candidate_range=config.assignment.candidate_keys\n",
        ")\n",
        "\n",
        "print(f'Key boundaries: {len(key_boundaries_px)} keys in pixel space')\n",
        "print(f'Sigma (auto): {assigner.sigma:.1f} px  (‚âà 1 white-key width)')\n",
        "print(f'Max distance: {assigner.max_distance_px:.0f} px ({assigner.max_distance_sigma}œÉ gate)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run assignment: try BOTH hands for every key, pick higher confidence\n",
        "assignments = []\n",
        "skipped = 0\n",
        "\n",
        "for event in synced_events:\n",
        "    frame_idx = event.frame_idx\n",
        "    key_idx = event.key_idx\n",
        "\n",
        "    if key_idx not in assigner.key_centers:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    asgn_right = None\n",
        "    if frame_idx < len(right_px):\n",
        "        lm = right_px[frame_idx]\n",
        "        if not np.any(np.isnan(lm)):\n",
        "            asgn_right = assigner.assign_from_landmarks(lm, key_idx, 'right', frame_idx, event.onset_time)\n",
        "\n",
        "    asgn_left = None\n",
        "    if frame_idx < len(left_px):\n",
        "        lm = left_px[frame_idx]\n",
        "        if not np.any(np.isnan(lm)):\n",
        "            asgn_left = assigner.assign_from_landmarks(lm, key_idx, 'left', frame_idx, event.onset_time)\n",
        "\n",
        "    candidates = [a for a in (asgn_right, asgn_left) if a is not None]\n",
        "    if candidates:\n",
        "        assignments.append(max(candidates, key=lambda a: a.confidence))\n",
        "    else:\n",
        "        skipped += 1\n",
        "\n",
        "print(f'Total events  : {len(synced_events)}')\n",
        "print(f'Assigned      : {len(assignments)}')\n",
        "print(f'Skipped       : {skipped}')\n",
        "print(f'Coverage      : {len(assignments)/max(1,len(synced_events))*100:.1f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assignment statistics\n",
        "if assignments:\n",
        "    fingers = [a.assigned_finger for a in assignments]\n",
        "    hands_list = [a.hand for a in assignments]\n",
        "    confs = [a.confidence for a in assignments]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "    finger_names_map = {1: 'Thumb', 2: 'Index', 3: 'Middle', 4: 'Ring', 5: 'Pinky'}\n",
        "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
        "    fc = pd.Series(fingers).value_counts().sort_index()\n",
        "    fc.plot.bar(ax=axes[0], color=[colors[i-1] for i in fc.index])\n",
        "    axes[0].set_xticklabels([finger_names_map[i] for i in fc.index], rotation=45)\n",
        "    axes[0].set_title('Finger Distribution')\n",
        "\n",
        "    pd.Series(hands_list).value_counts().plot.bar(ax=axes[1], color=['coral', 'skyblue'])\n",
        "    axes[1].set_title('Hand Distribution')\n",
        "\n",
        "    axes[2].hist(confs, bins=30, color='mediumseagreen', edgecolor='white')\n",
        "    axes[2].set_title('Assignment Confidence')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print('\\nSample assignments:')\n",
        "    for a in assignments[:10]:\n",
        "        print(f'  Frame {a.frame_idx:>5d} | {a.label} | MIDI {a.midi_pitch} ({a.finger_name:6s}) | conf={a.confidence:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='6'></a>\n",
        "## 6. Baseline Pipeline on Multiple Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_SAMPLE_CACHE = {'video': {}, 'keyboard': {}, 'tsv': {},\n",
        "                 'filtered_landmarks': {}, 'keys_px': {}}\n",
        "\n",
        "def process_sample_baseline(sample, dataset, config, max_duration_sec=60, cache=_SAMPLE_CACHE):\n",
        "    \"\"\"Full-CV baseline: auto-detect keyboard + live MediaPipe hands + Gaussian assignment.\n",
        "\n",
        "    NO pre-extracted skeletons from the dataset are used ‚Äî only raw video.\n",
        "    Corner annotations are used ONLY for IoU evaluation of keyboard detection.\n",
        "    \"\"\"\n",
        "    result = {'sample_id': sample.id, 'assignments': [], 'error': None, 'iou': None}\n",
        "    try:\n",
        "        # ‚îÄ‚îÄ Download video (shared by all stages) ‚îÄ‚îÄ\n",
        "        if sample.id not in cache['video']:\n",
        "            cache['video'][sample.id] = dataset.download_file(sample.video_path)\n",
        "        vid_path = cache['video'][sample.id]\n",
        "\n",
        "        # ‚îÄ‚îÄ Stage 1: Automatic keyboard detection (NO annotations) ‚îÄ‚îÄ\n",
        "        if sample.id not in cache['keyboard']:\n",
        "            det = AutoKeyboardDetector({\n",
        "                'canny_low': config.keyboard.canny_low,\n",
        "                'canny_high': config.keyboard.canny_high,\n",
        "                'hough_threshold': config.keyboard.hough_threshold,\n",
        "            })\n",
        "            auto_res = det.detect_from_video(vid_path)\n",
        "            if not auto_res.success:\n",
        "                result['error'] = 'Auto-detection failed'\n",
        "                return result\n",
        "            cache['keyboard'][sample.id] = (auto_res, det)\n",
        "\n",
        "        auto_res, det = cache['keyboard'][sample.id]\n",
        "        kb = auto_res.keyboard_region\n",
        "\n",
        "        if sample.id not in cache['keys_px']:\n",
        "            cache['keys_px'][sample.id] = kb.key_boundaries\n",
        "        kb_px = cache['keys_px'][sample.id]\n",
        "\n",
        "        # IoU evaluation against corner annotations (eval only)\n",
        "        corners = sample.metadata.get('keyboard_corners')\n",
        "        if corners:\n",
        "            result['iou'] = det.evaluate_against_corners(auto_res, corners)\n",
        "\n",
        "        # ‚îÄ‚îÄ Stage 2: Live hand detection (MediaPipe on raw video) ‚îÄ‚îÄ\n",
        "        if sample.id not in cache['filtered_landmarks']:\n",
        "            live_cfg = LiveDetectionConfig(\n",
        "                model_complexity=1,\n",
        "                min_detection_confidence=0.3,\n",
        "                min_tracking_confidence=0.3,\n",
        "                frame_stride=2,\n",
        "                static_image_mode=False,\n",
        "            )\n",
        "            live_det = LiveHandDetector(config=live_cfg)\n",
        "            max_vid_frames = int(max_duration_sec * config.video_fps) if max_duration_sec else None\n",
        "            la, ra = live_det.detect_from_video(vid_path, max_frames=max_vid_frames)\n",
        "\n",
        "            # Temporal filtering (Hampel ‚Üí interpolation ‚Üí Savitzky-Golay)\n",
        "            t = TemporalFilter(\n",
        "                hampel_window=config.hand.hampel_window,\n",
        "                hampel_threshold=config.hand.hampel_threshold,\n",
        "                max_interpolation_gap=config.hand.interpolation_max_gap,\n",
        "                savgol_window=config.hand.savgol_window,\n",
        "                savgol_order=config.hand.savgol_order\n",
        "            )\n",
        "            if la.size > 0: la = t.process(la)\n",
        "            if ra.size > 0: ra = t.process(ra)\n",
        "\n",
        "            # Scale from [0,1] to pixel space\n",
        "            if la.size > 0: la = la.copy(); la[:,:,0] *= FRAME_W; la[:,:,1] *= FRAME_H\n",
        "            if ra.size > 0: ra = ra.copy(); ra[:,:,0] *= FRAME_W; ra[:,:,1] *= FRAME_H\n",
        "            cache['filtered_landmarks'][sample.id] = (la, ra)\n",
        "\n",
        "        la, ra = cache['filtered_landmarks'][sample.id]\n",
        "        if max_duration_sec:\n",
        "            mf = int(max_duration_sec * config.video_fps)\n",
        "            if la.size > 0: la = la[:mf]\n",
        "            if ra.size > 0: ra = ra[:mf]\n",
        "\n",
        "        # ‚îÄ‚îÄ Stage 3: MIDI sync + Gaussian assignment ‚îÄ‚îÄ\n",
        "        if sample.id not in cache['tsv']:\n",
        "            cache['tsv'][sample.id] = dataset.load_tsv_annotations(sample)\n",
        "        tsv = cache['tsv'][sample.id]\n",
        "        if max_duration_sec:\n",
        "            tsv = tsv[tsv['onset'] <= float(max_duration_sec)].copy()\n",
        "\n",
        "        midi_evts = [{'onset': float(r['onset']), 'offset': float(r['onset'])+0.3,\n",
        "                      'pitch': int(r['note']),\n",
        "                      'velocity': int(r['velocity']) if 'velocity' in r and pd.notna(r['velocity']) else 64}\n",
        "                     for _, r in tsv.iterrows()]\n",
        "\n",
        "        sync = MidiVideoSync(fps=config.video_fps)\n",
        "        synced = sync.sync_events(midi_evts)\n",
        "\n",
        "        asgn = GaussianFingerAssigner(key_boundaries=kb_px, sigma=config.assignment.sigma,\n",
        "                                      candidate_range=config.assignment.candidate_keys)\n",
        "\n",
        "        for ev in synced:\n",
        "            fidx, kidx = ev.frame_idx, ev.key_idx\n",
        "            if kidx not in asgn.key_centers: continue\n",
        "            ar = None\n",
        "            if fidx < len(ra):\n",
        "                lm = ra[fidx]\n",
        "                if not np.any(np.isnan(lm)):\n",
        "                    ar = asgn.assign_from_landmarks(lm, kidx, 'right', fidx, ev.onset_time)\n",
        "            al = None\n",
        "            if fidx < len(la):\n",
        "                lm = la[fidx]\n",
        "                if not np.any(np.isnan(lm)):\n",
        "                    al = asgn.assign_from_landmarks(lm, kidx, 'left', fidx, ev.onset_time)\n",
        "            cands = [a for a in (ar, al) if a is not None]\n",
        "            if cands:\n",
        "                result['assignments'].append(max(cands, key=lambda a: a.confidence))\n",
        "    except Exception as e:\n",
        "        result['error'] = str(e)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_SAMPLES = 5          # Reduced for class project (faster training)\n",
        "MAX_DURATION_SEC = 60    # Reduced from 120s to 60s per sample\n",
        "\n",
        "all_results = []\n",
        "for i, samp in enumerate(train_dataset):\n",
        "    if i >= NUM_SAMPLES: break\n",
        "    print(f'Processing {i+1}/{NUM_SAMPLES}: {samp.id} - {samp.metadata[\"piece\"][:40]}')\n",
        "    res = process_sample_baseline(samp, train_dataset, config, max_duration_sec=MAX_DURATION_SEC)\n",
        "    if res['error']:\n",
        "        print(f'  Error: {res[\"error\"][:100]}')\n",
        "    else:\n",
        "        iou_str = f'IoU={res[\"iou\"]:.3f}' if res['iou'] is not None else 'IoU=N/A'\n",
        "        print(f'  Assigned {len(res[\"assignments\"])} notes  |  {iou_str}')\n",
        "    all_results.append(res)\n",
        "\n",
        "total_assigned = sum(len(r['assignments']) for r in all_results)\n",
        "ious = [r['iou'] for r in all_results if r['iou'] is not None]\n",
        "print(f'\\nTotal assignments: {total_assigned}')\n",
        "if ious:\n",
        "    print(f'Keyboard detection IoU: mean={np.mean(ious):.3f}, min={np.min(ious):.3f}, max={np.max(ious):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_fingers = [a.assigned_finger for r in all_results for a in r['assignments']]\n",
        "all_hands = [a.hand for r in all_results for a in r['assignments']]\n",
        "all_confs = [a.confidence for r in all_results for a in r['assignments']]\n",
        "\n",
        "if all_fingers:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "    colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
        "    fc = pd.Series(all_fingers).value_counts().sort_index()\n",
        "    fc.plot.bar(ax=axes[0], color=[colors[i-1] for i in fc.index])\n",
        "    axes[0].set_title(f'Finger Distribution (n={len(all_fingers)})')\n",
        "    pd.Series(all_hands).value_counts().plot.bar(ax=axes[1], color=['coral', 'skyblue'])\n",
        "    axes[1].set_title('Hand Distribution')\n",
        "    axes[2].hist(all_confs, bins=30, color='mediumseagreen', edgecolor='white')\n",
        "    axes[2].axvline(np.mean(all_confs), color='red', ls='--', label=f'mean={np.mean(all_confs):.3f}')\n",
        "    axes[2].set_title('Confidence Distribution')\n",
        "    axes[2].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Keyboard Detection IoU Across Samples ‚îÄ‚îÄ\n",
        "ious = [(r['sample_id'], r['iou']) for r in all_results if r['iou'] is not None]\n",
        "\n",
        "if ious:\n",
        "    labels, values = zip(*ious)\n",
        "    short_labels = [l[:12] for l in labels]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 4))\n",
        "    bars = ax.bar(range(len(values)), values, color='steelblue', edgecolor='white')\n",
        "    ax.axhline(np.mean(values), color='red', ls='--', label=f'Mean IoU = {np.mean(values):.3f}')\n",
        "    ax.set_xticks(range(len(values)))\n",
        "    ax.set_xticklabels(short_labels, rotation=45, ha='right', fontsize=8)\n",
        "    ax.set_ylabel('IoU')\n",
        "    ax.set_title('Keyboard Auto-Detection IoU vs Corner Annotations')\n",
        "    ax.set_ylim(0, 1.05)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('No IoU data available')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='7'></a>\n",
        "## 7. Stage 5 - Neural Refinement (BiLSTM)\n",
        "\n",
        "Architecture: Input(20) -> Linear(128) -> BiLSTM(128 x 2 layers) -> Self-Attention -> Linear(128) -> Linear(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Preparing training sequences from baseline assignments ...')\n",
        "\n",
        "MAX_TRAIN_SAMPLES = 20\n",
        "train_sequences = []\n",
        "train_ds_full = PianoVAMDataset(split='train', streaming=True, max_samples=MAX_TRAIN_SAMPLES)\n",
        "\n",
        "for i, samp in enumerate(train_ds_full):\n",
        "    if i >= MAX_TRAIN_SAMPLES: break\n",
        "    res = process_sample_baseline(samp, train_ds_full, config, max_duration_sec=60)\n",
        "    asgns = res['assignments']\n",
        "    if len(asgns) < 10: continue\n",
        "    seq = {\n",
        "        'pitches': [a.midi_pitch for a in asgns],\n",
        "        'fingers': [a.assigned_finger for a in asgns],\n",
        "        'onsets': [a.note_onset for a in asgns],\n",
        "        'hands': [a.hand for a in asgns],\n",
        "        'labels': [a.assigned_finger for a in asgns],\n",
        "    }\n",
        "    train_sequences.append(seq)\n",
        "\n",
        "print(f'Training sequences: {len(train_sequences)}')\n",
        "print(f'Total notes: {sum(len(s[\"pitches\"]) for s in train_sequences)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor(normalize_pitch=True)\n",
        "input_size = feature_extractor.get_input_size()\n",
        "\n",
        "trained_model = None\n",
        "if len(train_sequences) > 2:\n",
        "    split_idx = max(1, int(0.8 * len(train_sequences)))\n",
        "    train_seqs = train_sequences[:split_idx]\n",
        "    val_seqs = train_sequences[split_idx:]\n",
        "\n",
        "    train_torch_ds = SequenceDataset(train_seqs, feature_extractor, max_len=256)\n",
        "    val_torch_ds = SequenceDataset(val_seqs, feature_extractor, max_len=256)\n",
        "\n",
        "    model = FingeringRefiner(\n",
        "        input_size=input_size,\n",
        "        hidden_size=config.refinement.hidden_size,\n",
        "        num_layers=config.refinement.num_layers,\n",
        "        dropout=config.refinement.dropout,\n",
        "        bidirectional=config.refinement.bidirectional\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "    print(model)\n",
        "\n",
        "    training_config = {\n",
        "        'hidden_size': config.refinement.hidden_size,\n",
        "        'num_layers': config.refinement.num_layers,\n",
        "        'dropout': config.refinement.dropout,\n",
        "        'batch_size': min(config.refinement.batch_size, len(train_torch_ds)),\n",
        "        'learning_rate': config.refinement.learning_rate,\n",
        "        'epochs': config.refinement.epochs,\n",
        "        'early_stopping_patience': config.refinement.early_stopping_patience,\n",
        "        'device': DEVICE,\n",
        "        'checkpoint_dir': '/content/checkpoints' if IN_COLAB else './outputs/checkpoints'\n",
        "    }\n",
        "\n",
        "    print('\\nTraining BiLSTM refinement model ...')\n",
        "    trained_model = train_refiner(\n",
        "        train_dataset=train_torch_ds,\n",
        "        val_dataset=val_torch_ds if len(val_torch_ds) > 0 else None,\n",
        "        config=training_config\n",
        "    )\n",
        "    print('Training complete')\n",
        "else:\n",
        "    print('Not enough data for training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def refine_assignments(model, assignments, feature_extractor, device='cpu', use_constraints=True):\n",
        "    if not assignments or model is None:\n",
        "        return assignments\n",
        "\n",
        "    pitches = [a.midi_pitch for a in assignments]\n",
        "    fingers = [a.assigned_finger for a in assignments]\n",
        "    onsets = [a.note_onset for a in assignments]\n",
        "    hands = [a.hand for a in assignments]\n",
        "\n",
        "    x = feature_extractor.extract(pitches, fingers, onsets, hands)\n",
        "    x = x.unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    if use_constraints:\n",
        "        decoded = constrained_viterbi_decode(\n",
        "            probs=probs, pitches=pitches, hands=hands,\n",
        "            constraints=BiomechanicalConstraints(strict=False)\n",
        "        )\n",
        "        pred_fingers = decoded.fingers\n",
        "    else:\n",
        "        pred_fingers = (np.argmax(probs, axis=-1) + 1).tolist()\n",
        "\n",
        "    confs = [float(probs[i, f - 1]) for i, f in enumerate(pred_fingers)]\n",
        "\n",
        "    return [FingerAssignment(\n",
        "        note_onset=a.note_onset, frame_idx=a.frame_idx, midi_pitch=a.midi_pitch,\n",
        "        key_idx=a.key_idx, assigned_finger=int(pred_fingers[i]), hand=a.hand,\n",
        "        confidence=float(confs[i]), fingertip_position=a.fingertip_position\n",
        "    ) for i, a in enumerate(assignments)]\n",
        "\n",
        "\n",
        "if trained_model is not None and all_results:\n",
        "    print('Refining baseline predictions ...')\n",
        "    for res in all_results:\n",
        "        if res['assignments']:\n",
        "            original = res['assignments']\n",
        "            refined = refine_assignments(trained_model, original, feature_extractor, DEVICE)\n",
        "            res['refined_assignments'] = refined\n",
        "            changed = sum(1 for o, r in zip(original, refined) if o.assigned_finger != r.assigned_finger)\n",
        "            print(f'  {res[\"sample_id\"]}: {changed}/{len(original)} changed')\n",
        "    print('Refinement done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='8'></a>\n",
        "## 8. Evaluation & Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = FingeringMetrics()\n",
        "constraints = BiomechanicalConstraints()\n",
        "\n",
        "print('=' * 70)\n",
        "print('EVALUATION RESULTS')\n",
        "print('=' * 70)\n",
        "\n",
        "baseline_ifrs = []\n",
        "refined_ifrs = []\n",
        "\n",
        "for res in all_results:\n",
        "    if not res['assignments']: continue\n",
        "    asgns = res['assignments']\n",
        "    pitches = [a.midi_pitch for a in asgns]\n",
        "    fingers = [a.assigned_finger for a in asgns]\n",
        "    hl = [a.hand for a in asgns]\n",
        "\n",
        "    violations = constraints.validate_sequence(fingers, pitches, hl)\n",
        "    ifr = len(violations) / max(1, len(asgns) - 1)\n",
        "    baseline_ifrs.append(ifr)\n",
        "    mc = np.mean([a.confidence for a in asgns])\n",
        "\n",
        "    msg = f'  {res[\"sample_id\"]} - {len(asgns)} notes | Baseline IFR={ifr:.3f} | conf={mc:.3f}'\n",
        "\n",
        "    if 'refined_assignments' in res:\n",
        "        ref = res['refined_assignments']\n",
        "        rf = [a.assigned_finger for a in ref]\n",
        "        rv = constraints.validate_sequence(rf, pitches, hl)\n",
        "        ri = len(rv) / max(1, len(ref) - 1)\n",
        "        refined_ifrs.append(ri)\n",
        "        msg += f' | Refined IFR={ri:.3f}'\n",
        "\n",
        "    print(msg)\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "if baseline_ifrs:\n",
        "    print(f'BASELINE Mean IFR: {np.mean(baseline_ifrs):.3f} +/- {np.std(baseline_ifrs):.3f}')\n",
        "if refined_ifrs:\n",
        "    print(f'REFINED  Mean IFR: {np.mean(refined_ifrs):.3f} +/- {np.std(refined_ifrs):.3f}')\n",
        "    imp = np.mean(baseline_ifrs) - np.mean(refined_ifrs)\n",
        "    print(f'Improvement: {imp:+.3f}')\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test set evaluation (full-CV: auto-detection for each test sample)\n",
        "print('Processing test split ...\\n')\n",
        "test_ds_eval = PianoVAMDataset(split='test', streaming=True, max_samples=5)\n",
        "test_results = []\n",
        "\n",
        "for i, samp in enumerate(test_ds_eval):\n",
        "    print(f'  Test {i+1}: {samp.id}')\n",
        "    res = process_sample_baseline(samp, test_ds_eval, config)\n",
        "    if res['error']:\n",
        "        print(f'    Error: {res[\"error\"][:80]}')\n",
        "    else:\n",
        "        n = len(res['assignments'])\n",
        "        iou_str = f'IoU={res[\"iou\"]:.3f}' if res['iou'] is not None else ''\n",
        "        if trained_model is not None and n > 0:\n",
        "            res['refined_assignments'] = refine_assignments(\n",
        "                trained_model, res['assignments'], feature_extractor, DEVICE)\n",
        "        print(f'    {n} notes assigned  {iou_str}')\n",
        "    test_results.append(res)\n",
        "\n",
        "print('\\n' + '=' * 70)\n",
        "print('TEST SET RESULTS')\n",
        "print('=' * 70)\n",
        "\n",
        "test_baseline_ifrs = []\n",
        "test_refined_ifrs = []\n",
        "\n",
        "for res in test_results:\n",
        "    if not res['assignments']: continue\n",
        "    asgns = res['assignments']\n",
        "    pitches = [a.midi_pitch for a in asgns]\n",
        "    fingers = [a.assigned_finger for a in asgns]\n",
        "    hl = [a.hand for a in asgns]\n",
        "\n",
        "    viols = constraints.validate_sequence(fingers, pitches, hl)\n",
        "    ifr = len(viols) / max(1, len(asgns) - 1)\n",
        "    test_baseline_ifrs.append(ifr)\n",
        "\n",
        "    msg = f'  {res[\"sample_id\"]} - {len(asgns)} notes | Baseline IFR={ifr:.3f}'\n",
        "\n",
        "    if 'refined_assignments' in res:\n",
        "        ref = res['refined_assignments']\n",
        "        rf = [a.assigned_finger for a in ref]\n",
        "        rv = constraints.validate_sequence(rf, pitches, hl)\n",
        "        ri = len(rv) / max(1, len(ref) - 1)\n",
        "        test_refined_ifrs.append(ri)\n",
        "        msg += f' | Refined IFR={ri:.3f}'\n",
        "    print(msg)\n",
        "\n",
        "if test_baseline_ifrs:\n",
        "    print(f'\\nTEST Baseline Mean IFR: {np.mean(test_baseline_ifrs):.3f}')\n",
        "if test_refined_ifrs:\n",
        "    print(f'TEST Refined  Mean IFR: {np.mean(test_refined_ifrs):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary figure\n",
        "if baseline_ifrs:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    x = np.arange(len(baseline_ifrs))\n",
        "    w = 0.35\n",
        "    axes[0].bar(x - w/2, baseline_ifrs, w, label='Baseline', color='steelblue')\n",
        "    if refined_ifrs:\n",
        "        axes[0].bar(x + w/2, refined_ifrs, w, label='Refined', color='coral')\n",
        "    axes[0].set_xlabel('Sample')\n",
        "    axes[0].set_ylabel('IFR (lower = better)')\n",
        "    axes[0].set_title('IFR Comparison (Train Samples)')\n",
        "    axes[0].legend()\n",
        "\n",
        "    if all_confs:\n",
        "        axes[1].hist(all_confs, bins=30, color='mediumseagreen', edgecolor='white')\n",
        "        axes[1].axvline(np.mean(all_confs), color='red', ls='--', label=f'mean={np.mean(all_confs):.3f}')\n",
        "        axes[1].set_title('Confidence Distribution')\n",
        "        axes[1].legend()\n",
        "\n",
        "    plt.suptitle('Piano Fingering Detection - Results Summary', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "output_dir = Path('/content/outputs' if IN_COLAB else './outputs')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "results_summary = {\n",
        "    'pipeline': 'piano-fingering-detection',\n",
        "    'baseline_method': 'Gaussian Assignment (x-only, both hands, max-distance gate)',\n",
        "    'refinement_method': 'BiLSTM + Attention + Constrained Viterbi',\n",
        "    'test_results': []\n",
        "}\n",
        "\n",
        "for i, res in enumerate(test_results):\n",
        "    entry = {'sample_id': res['sample_id'], 'num_assignments': len(res.get('assignments', []))}\n",
        "    if i < len(test_baseline_ifrs):\n",
        "        entry['baseline_ifr'] = float(test_baseline_ifrs[i])\n",
        "    if i < len(test_refined_ifrs):\n",
        "        entry['refined_ifr'] = float(test_refined_ifrs[i])\n",
        "    results_summary['test_results'].append(entry)\n",
        "\n",
        "if test_baseline_ifrs:\n",
        "    results_summary['mean_baseline_ifr'] = float(np.mean(test_baseline_ifrs))\n",
        "if test_refined_ifrs:\n",
        "    results_summary['mean_refined_ifr'] = float(np.mean(test_refined_ifrs))\n",
        "\n",
        "with open(output_dir / 'evaluation_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "if trained_model is not None:\n",
        "    torch.save(trained_model.state_dict(), output_dir / 'refinement_model.pt')\n",
        "\n",
        "print(f'Results saved to {output_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='9'></a>\n",
        "## 9. Extended Evaluation & Validation\n",
        "\n",
        "### 9.1 Baseline Comparisons\n",
        "\n",
        "To validate that our pipeline produces **meaningful** predictions, we compare against trivial baselines.\n",
        "If our IFR metric is useful, there should be clear separation between intelligent methods and naive strategies.\n",
        "\n",
        "| Baseline | Strategy |\n",
        "|---|---|\n",
        "| **Random** | Assign a random finger (1‚Äì5) to every note |\n",
        "| **Always Finger 3** | Assign middle finger to every note |\n",
        "| **Pitch-Proportional** | Map the pitch range linearly to fingers 1‚Äì5 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ 9.1  Baseline Comparisons ‚îÄ‚îÄ\n",
        "import random as _random\n",
        "\n",
        "print('=' * 70)\n",
        "print('BASELINE COMPARISONS ‚Äî IFR  (lower = fewer impossible transitions = better)')\n",
        "print('=' * 70)\n",
        "\n",
        "method_names = ['Random', 'Always Finger 3', 'Pitch-Proportional',\n",
        "                'Gaussian Baseline', 'Refined (BiLSTM+Viterbi)']\n",
        "method_ifrs = {m: [] for m in method_names}\n",
        "\n",
        "eval_pool = [r for r in (all_results + test_results)\n",
        "             if r.get('assignments') and len(r['assignments']) >= 2]\n",
        "\n",
        "for res in eval_pool:\n",
        "    asgns = res['assignments']\n",
        "    pitches = [a.midi_pitch for a in asgns]\n",
        "    hl      = [a.hand       for a in asgns]\n",
        "    n = len(asgns)\n",
        "\n",
        "    # 1 ‚Äî Random\n",
        "    _random.seed(42)\n",
        "    rf = [_random.randint(1, 5) for _ in range(n)]\n",
        "    v = constraints.validate_sequence(rf, pitches, hl)\n",
        "    method_ifrs['Random'].append(len(v) / (n - 1))\n",
        "\n",
        "    # 2 ‚Äî Always finger 3 (middle)\n",
        "    v = constraints.validate_sequence([3] * n, pitches, hl)\n",
        "    method_ifrs['Always Finger 3'].append(len(v) / (n - 1))\n",
        "\n",
        "    # 3 ‚Äî Pitch-proportional  (map pitch range ‚Üí fingers 1-5)\n",
        "    pmin, pmax = min(pitches), max(pitches)\n",
        "    prange = max(1, pmax - pmin)\n",
        "    pf = [max(1, min(5, round(1 + 4 * (p - pmin) / prange))) for p in pitches]\n",
        "    v = constraints.validate_sequence(pf, pitches, hl)\n",
        "    method_ifrs['Pitch-Proportional'].append(len(v) / (n - 1))\n",
        "\n",
        "    # 4 ‚Äî Our Gaussian baseline\n",
        "    bf = [a.assigned_finger for a in asgns]\n",
        "    v = constraints.validate_sequence(bf, pitches, hl)\n",
        "    method_ifrs['Gaussian Baseline'].append(len(v) / (n - 1))\n",
        "\n",
        "    # 5 ‚Äî Refined (if available)\n",
        "    if 'refined_assignments' in res and res['refined_assignments']:\n",
        "        ref_f = [a.assigned_finger for a in res['refined_assignments']]\n",
        "        v = constraints.validate_sequence(ref_f, pitches, hl)\n",
        "        method_ifrs['Refined (BiLSTM+Viterbi)'].append(len(v) / (n - 1))\n",
        "\n",
        "# ‚îÄ‚îÄ Print table ‚îÄ‚îÄ\n",
        "print(f'\\n  {\"Method\":32s}  {\"Mean IFR\":>10s}  {\"Std\":>8s}  {\"n\":>4s}')\n",
        "print(f'  {\"‚îÄ\"*32}  {\"‚îÄ\"*10}  {\"‚îÄ\"*8}  {\"‚îÄ\"*4}')\n",
        "for m in method_names:\n",
        "    vals = method_ifrs[m]\n",
        "    if vals:\n",
        "        print(f'  {m:32s}  {np.mean(vals):>10.3f}  {np.std(vals):>8.3f}  {len(vals):>4d}')\n",
        "    else:\n",
        "        print(f'  {m:32s}  {\"N/A\":>10s}')\n",
        "\n",
        "# ‚îÄ‚îÄ Bar chart ‚îÄ‚îÄ\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "active = [m for m in method_names if method_ifrs[m]]\n",
        "means  = [np.mean(method_ifrs[m]) for m in active]\n",
        "stds   = [np.std(method_ifrs[m])  for m in active]\n",
        "palette = ['#d62728', '#ff7f0e', '#9467bd', '#2ca02c', '#1f77b4']\n",
        "\n",
        "bars = ax.bar(range(len(active)), means, yerr=stds, capsize=5,\n",
        "              color=palette[:len(active)], edgecolor='white', linewidth=1.5)\n",
        "ax.set_xticks(range(len(active)))\n",
        "ax.set_xticklabels(active, rotation=20, ha='right', fontsize=10)\n",
        "ax.set_ylabel('IFR  (lower = better)')\n",
        "ax.set_title('Irrational Fingering Rate ‚Äî Our Pipeline vs. Trivial Baselines')\n",
        "ax.set_ylim(0, None)\n",
        "\n",
        "for bar, m_val in zip(bars, means):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "            f'{m_val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚îÄ‚îÄ Relative improvements ‚îÄ‚îÄ\n",
        "if method_ifrs['Gaussian Baseline'] and method_ifrs['Random']:\n",
        "    base_m = np.mean(method_ifrs['Gaussian Baseline'])\n",
        "    rand_m = np.mean(method_ifrs['Random'])\n",
        "    print(f'\\nGaussian baseline reduces IFR by {(rand_m - base_m) / rand_m * 100:.1f}% vs random')\n",
        "if method_ifrs['Refined (BiLSTM+Viterbi)'] and method_ifrs['Gaussian Baseline']:\n",
        "    ref_m  = np.mean(method_ifrs['Refined (BiLSTM+Viterbi)'])\n",
        "    base_m = np.mean(method_ifrs['Gaussian Baseline'])\n",
        "    print(f'BiLSTM+Viterbi further reduces IFR by {(base_m - ref_m) / max(base_m, 1e-6) * 100:.1f}% vs Gaussian baseline')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Hand Detection Validation ‚Äî Live MediaPipe vs. Dataset Skeletons\n",
        "\n",
        "PianoVAM provides **pre-extracted hand skeletons** for every video. Our pipeline deliberately uses live MediaPipe detection on the raw video. Here we validate our detections against the dataset's reference skeletons using standard pose-estimation metrics.\n",
        "\n",
        "| Metric | What It Measures |\n",
        "|--------|-----------------|\n",
        "| **Detection Rate** | % of frames with a valid hand detection |\n",
        "| **PCK** (Percentage of Correct Keypoints) | % of keypoints within a normalised threshold of the reference (standard pose metric) |\n",
        "| **Trajectory Correlation** | Pearson *r* of fingertip x-coordinates over time ‚Äî captures motion-pattern similarity |\n",
        "| **Key Agreement** | % of frames where both methods place a fingertip on the **same piano key** |\n",
        "\n",
        "> **PCK** is the standard evaluation metric in pose estimation (Andriluka et al., 2014). Small spatial offsets still count as *correct* if they fall within the threshold.\n",
        "\n",
        "> **Note**: Differences are expected ‚Äî the dataset skeletons were extracted with a different MediaPipe version, different detection parameters, and potentially different hand labelling conventions. What matters is that both methods capture the same underlying hand motion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ 9.2  Hand Detection Validation ‚îÄ‚îÄ\n",
        "print(f'Downloading pre-extracted skeleton for sample {sample.id} ...\\n')\n",
        "\n",
        "try:\n",
        "    skeleton_data = train_dataset.load_skeleton(sample)\n",
        "    loader = SkeletonLoader(normalize=False)\n",
        "    parsed = loader._parse_json(skeleton_data)\n",
        "\n",
        "    T_live = max(len(left_raw), len(right_raw))\n",
        "    ds_left_arr  = loader.to_array(parsed.get('left',  []), fill_missing=True, total_frames=T_live)\n",
        "    ds_right_arr = loader.to_array(parsed.get('right', []), fill_missing=True, total_frames=T_live)\n",
        "\n",
        "    # ‚îÄ‚îÄ Normalise coordinates to [0, 1] if stored in pixel space ‚îÄ‚îÄ\n",
        "    for tag, arr in [('ds_left', ds_left_arr), ('ds_right', ds_right_arr)]:\n",
        "        valid = ~np.isnan(arr[:, 0, 0])\n",
        "        if valid.any():\n",
        "            xmax = np.nanmax(arr[valid, :, 0])\n",
        "            ymax = np.nanmax(arr[valid, :, 1])\n",
        "            if xmax > 2.0 or ymax > 2.0:          # pixel coordinates\n",
        "                print(f'  {tag}: pixel coords detected (max x={xmax:.0f}), normalising ...')\n",
        "                arr[:, :, 0] /= FRAME_W\n",
        "                arr[:, :, 1] /= FRAME_H\n",
        "\n",
        "    print(f'Shapes  ‚Äî Dataset  L:{ds_left_arr.shape}  R:{ds_right_arr.shape}')\n",
        "    print(f'          Live MP  L:{left_raw.shape}  R:{right_raw.shape}')\n",
        "\n",
        "    # ‚îÄ‚îÄ Detect hand-label swap ‚îÄ‚îÄ\n",
        "    # MediaPipe sometimes mirrors L/R labels. Try both matchings, keep the better one.\n",
        "    def _corr_tip(a, b, tip=8, min_n=30):\n",
        "        mn = min(len(a), len(b))\n",
        "        v = ~np.isnan(a[:mn, tip, 0]) & ~np.isnan(b[:mn, tip, 0])\n",
        "        if v.sum() < min_n:\n",
        "            return -1.0\n",
        "        x1, x2 = a[:mn, tip, 0][v], b[:mn, tip, 0][v]\n",
        "        if np.std(x1) < 1e-6 or np.std(x2) < 1e-6:\n",
        "            return -1.0\n",
        "        return float(np.corrcoef(x1, x2)[0, 1])\n",
        "\n",
        "    r_normal  = _corr_tip(right_raw, ds_right_arr) + _corr_tip(left_raw, ds_left_arr)\n",
        "    r_swapped = _corr_tip(right_raw, ds_left_arr)  + _corr_tip(left_raw, ds_right_arr)\n",
        "\n",
        "    if r_swapped > r_normal + 0.05:\n",
        "        print(f'\\n  Hand labels are SWAPPED between methods ‚Äî correcting '\n",
        "              f'(normal r={r_normal:.2f}, swapped r={r_swapped:.2f})')\n",
        "        ds_left_arr, ds_right_arr = ds_right_arr, ds_left_arr\n",
        "    else:\n",
        "        print(f'\\n  Hand labels consistent (normal r={r_normal:.2f}, swapped r={r_swapped:.2f})')\n",
        "\n",
        "    # ‚îÄ‚îÄ Temporal Alignment ‚îÄ‚îÄ\n",
        "    # Find the best frame offset by maximising index-fingertip correlation.\n",
        "    live_r_rate = LiveHandDetector.detection_rate(right_raw)\n",
        "    live_l_rate = LiveHandDetector.detection_rate(left_raw)\n",
        "    best_live = right_raw if live_r_rate >= live_l_rate else left_raw\n",
        "    best_ds   = ds_right_arr if live_r_rate >= live_l_rate else ds_left_arr\n",
        "    best_hand_label = 'Right' if live_r_rate >= live_l_rate else 'Left'\n",
        "\n",
        "    best_offset, best_r = 0, -1.0\n",
        "    for offset in range(-20, 21):\n",
        "        if offset >= 0:\n",
        "            lv = best_live[offset:]\n",
        "            ds = best_ds[:len(lv)]\n",
        "        else:\n",
        "            ds = best_ds[-offset:]\n",
        "            lv = best_live[:len(ds)]\n",
        "        mn = min(len(lv), len(ds))\n",
        "        lv, ds = lv[:mn], ds[:mn]\n",
        "        v = ~np.isnan(lv[:, 8, 0]) & ~np.isnan(ds[:, 8, 0])\n",
        "        if v.sum() < 30:\n",
        "            continue\n",
        "        x1, x2 = lv[v, 8, 0], ds[v, 8, 0]\n",
        "        if np.std(x1) < 1e-6 or np.std(x2) < 1e-6:\n",
        "            continue\n",
        "        r = float(np.corrcoef(x1, x2)[0, 1])\n",
        "        if r > best_r:\n",
        "            best_r, best_offset = r, offset\n",
        "\n",
        "    print(f'  Temporal alignment: offset = {best_offset} frames  (peak r = {best_r:.3f})')\n",
        "\n",
        "    def _align(a, b, off):\n",
        "        if off > 0:    a2, b2 = a[off:], b[:max(0, len(a)-off)]\n",
        "        elif off < 0:  b2, a2 = b[-off:], a[:max(0, len(b)+off)]\n",
        "        else:           a2, b2 = a, b\n",
        "        mn = min(len(a2), len(b2))\n",
        "        return a2[:mn], b2[:mn]\n",
        "\n",
        "    live_L_al, ds_L_al = _align(left_raw,  ds_left_arr,  best_offset)\n",
        "    live_R_al, ds_R_al = _align(right_raw, ds_right_arr, best_offset)\n",
        "\n",
        "    # ‚îÄ‚îÄ Detection Rate ‚îÄ‚îÄ\n",
        "    print(f'\\n{\"‚îÄ\"*55}')\n",
        "    print(f'  Detection Rates (after alignment)')\n",
        "    print(f'  {\"Hand\":8s}  {\"Dataset\":>10s}  {\"Live MP\":>10s}')\n",
        "    for label, ds_a, lv_a in [('Left', ds_L_al, live_L_al), ('Right', ds_R_al, live_R_al)]:\n",
        "        dr_ds = float(np.mean(~np.isnan(ds_a[:, 0, 0]))) if ds_a.size else 0\n",
        "        dr_lv = float(np.mean(~np.isnan(lv_a[:, 0, 0]))) if lv_a.size else 0\n",
        "        print(f'  {label:8s}  {dr_ds:>9.1%}  {dr_lv:>9.1%}')\n",
        "\n",
        "    # ‚îÄ‚îÄ PCK (Percentage of Correct Keypoints) ‚îÄ‚îÄ\n",
        "    thresholds = [0.02, 0.05, 0.10, 0.15]\n",
        "    print(f'\\n{\"‚îÄ\"*55}')\n",
        "    print(f'  PCK ‚Äî Percentage of Correct Keypoints')\n",
        "    for label, lv_a, ds_a in [('Right', live_R_al, ds_R_al), ('Left', live_L_al, ds_L_al)]:\n",
        "        mn = min(len(lv_a), len(ds_a))\n",
        "        valid = ~np.isnan(lv_a[:mn, 0, 0]) & ~np.isnan(ds_a[:mn, 0, 0])\n",
        "        n_mutual = int(valid.sum())\n",
        "        if n_mutual < 5:\n",
        "            print(f'  {label}: insufficient mutual frames ({n_mutual})')\n",
        "            continue\n",
        "        p, g = lv_a[:mn][valid], ds_a[:mn][valid]\n",
        "        dist = np.sqrt(np.sum((p[:, :, :2] - g[:, :, :2]) ** 2, axis=-1))\n",
        "        print(f'  {label} hand  ({n_mutual} mutual frames):')\n",
        "        for t in thresholds:\n",
        "            pck = float((dist < t).mean())\n",
        "            bar = '‚ñà' * int(pck * 40)\n",
        "            print(f'    PCK@{t:.2f} = {pck:6.1%}  {bar}')\n",
        "\n",
        "    # ‚îÄ‚îÄ Fingertip Trajectory Correlation ‚îÄ‚îÄ\n",
        "    tip_map = {4: 'Thumb', 8: 'Index', 12: 'Middle', 16: 'Ring', 20: 'Pinky'}\n",
        "    print(f'\\n{\"‚îÄ\"*55}')\n",
        "    print(f'  Fingertip X-Trajectory Pearson Correlation')\n",
        "    all_corrs = []\n",
        "    for label, lv_a, ds_a in [('Right', live_R_al, ds_R_al), ('Left', live_L_al, ds_L_al)]:\n",
        "        mn = min(len(lv_a), len(ds_a))\n",
        "        valid = ~np.isnan(lv_a[:mn, 0, 0]) & ~np.isnan(ds_a[:mn, 0, 0])\n",
        "        if valid.sum() < 10:\n",
        "            continue\n",
        "        for tidx, tname in tip_map.items():\n",
        "            x1 = lv_a[:mn, tidx, 0][valid]\n",
        "            x2 = ds_a[:mn, tidx, 0][valid]\n",
        "            if np.std(x1) < 1e-6 or np.std(x2) < 1e-6:\n",
        "                continue\n",
        "            r = float(np.corrcoef(x1, x2)[0, 1])\n",
        "            all_corrs.append(r)\n",
        "            print(f'    {label:5s} {tname:7s}:  r = {r:.3f}')\n",
        "\n",
        "    if all_corrs:\n",
        "        mean_r = np.mean(all_corrs)\n",
        "        strength = 'strong' if mean_r > 0.7 else 'moderate' if mean_r > 0.4 else 'weak'\n",
        "        print(f'\\n    Mean correlation:  r = {mean_r:.3f}  ({strength} agreement)')\n",
        "\n",
        "    # ‚îÄ‚îÄ Key Agreement ‚îÄ‚îÄ\n",
        "    # Do both methods place each fingertip over the same piano key?\n",
        "    kc_items = sorted(assigner.key_centers.items(), key=lambda kv: kv[1][0])\n",
        "    kc_ids = [k for k, _ in kc_items]\n",
        "    kc_xs  = np.array([cx for _, (cx, _) in kc_items])\n",
        "\n",
        "    def _nearest_key(x_norm, fw=FRAME_W):\n",
        "        return kc_ids[np.argmin(np.abs(kc_xs - x_norm * fw))]\n",
        "\n",
        "    print(f'\\n{\"‚îÄ\"*55}')\n",
        "    print(f'  Key Agreement (same piano key?)')\n",
        "    for label, lv_a, ds_a in [('Right', live_R_al, ds_R_al), ('Left', live_L_al, ds_L_al)]:\n",
        "        mn = min(len(lv_a), len(ds_a))\n",
        "        valid = ~np.isnan(lv_a[:mn, 0, 0]) & ~np.isnan(ds_a[:mn, 0, 0])\n",
        "        if valid.sum() < 5:\n",
        "            continue\n",
        "        total_t, agree_t = 0, 0\n",
        "        for tidx in [4, 8, 12, 16, 20]:\n",
        "            idxs = np.where(valid)[0]\n",
        "            k_lv = [_nearest_key(lv_a[f, tidx, 0]) for f in idxs]\n",
        "            k_ds = [_nearest_key(ds_a[f, tidx, 0]) for f in idxs]\n",
        "            matches = sum(a == b for a, b in zip(k_lv, k_ds))\n",
        "            total_t += len(k_lv)\n",
        "            agree_t += matches\n",
        "        pct = agree_t / total_t * 100 if total_t else 0\n",
        "        print(f'    {label} hand: {pct:.1f}% key agreement  ({agree_t}/{total_t} fingertip-frames)')\n",
        "\n",
        "    # ‚îÄ‚îÄ Trajectory Visualisation ‚îÄ‚îÄ\n",
        "    live_vis = live_R_al if best_hand_label == 'Right' else live_L_al\n",
        "    ds_vis   = ds_R_al   if best_hand_label == 'Right' else ds_L_al\n",
        "    T_vis    = min(3000, min(len(live_vis), len(ds_vis)))\n",
        "\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(16, 10), sharex=True)\n",
        "    for ax_i, (tidx, tname) in enumerate([(8, 'Index'), (4, 'Thumb'), (12, 'Middle')]):\n",
        "        axes[ax_i].plot(ds_vis[:T_vis, tidx, 0],   alpha=0.7, lw=0.8,\n",
        "                        color='tab:blue',   label='Dataset skeleton')\n",
        "        axes[ax_i].plot(live_vis[:T_vis, tidx, 0], alpha=0.7, lw=0.8,\n",
        "                        color='tab:orange', label='Live MediaPipe')\n",
        "        axes[ax_i].set_ylabel('X (norm)')\n",
        "        axes[ax_i].set_title(f'{best_hand_label} Hand ‚Äî {tname} Fingertip')\n",
        "        axes[ax_i].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "    axes[-1].set_xlabel('Frame')\n",
        "    plt.suptitle('Trajectory Comparison: Live MediaPipe vs. Dataset Skeletons (aligned)',\n",
        "                 fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ‚îÄ‚îÄ Summary PCK bar chart ‚îÄ‚îÄ\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    pck_vals = []\n",
        "    for label, lv_a, ds_a in [('Right', live_R_al, ds_R_al), ('Left', live_L_al, ds_L_al)]:\n",
        "        mn = min(len(lv_a), len(ds_a))\n",
        "        v = ~np.isnan(lv_a[:mn, 0, 0]) & ~np.isnan(ds_a[:mn, 0, 0])\n",
        "        if v.sum() < 5:\n",
        "            continue\n",
        "        p2, g2 = lv_a[:mn][v], ds_a[:mn][v]\n",
        "        d2 = np.sqrt(np.sum((p2[:, :, :2] - g2[:, :, :2]) ** 2, axis=-1))\n",
        "        for t in thresholds:\n",
        "            pck_vals.append({'Hand': label, 'Threshold': f'@{t:.2f}', 'PCK': float((d2 < t).mean())})\n",
        "\n",
        "    if pck_vals:\n",
        "        pck_df = pd.DataFrame(pck_vals)\n",
        "        for hi, hand in enumerate(pck_df['Hand'].unique()):\n",
        "            sub = pck_df[pck_df['Hand'] == hand]\n",
        "            x_pos = np.arange(len(sub)) + hi * 0.35\n",
        "            ax.bar(x_pos, sub['PCK'], 0.3, label=f'{hand} hand',\n",
        "                   color=['steelblue', 'coral'][hi], edgecolor='white')\n",
        "            for xp, yp in zip(x_pos, sub['PCK']):\n",
        "                ax.text(xp, yp + 0.01, f'{yp:.0%}', ha='center', fontsize=9, fontweight='bold')\n",
        "        ax.set_xticks(np.arange(len(thresholds)) + 0.15)\n",
        "        ax.set_xticklabels([f'PCK@{t:.2f}' for t in thresholds])\n",
        "        ax.set_ylabel('PCK')\n",
        "        ax.set_ylim(0, 1.1)\n",
        "        ax.set_title('Percentage of Correct Keypoints ‚Äî Live MP vs. Dataset Skeletons')\n",
        "        ax.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(f'‚ö†Ô∏è  Skeleton comparison failed: {e}')\n",
        "    traceback.print_exc()\n",
        "    print('\\nThe pipeline does NOT depend on pre-extracted skeletons ‚Äî this is validation only.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Ablation Study\n",
        "\n",
        "We evaluate the contribution of **each pipeline component** by progressively enabling stages and measuring the effect on IFR. This is run on the demo sample whose landmarks are already cached.\n",
        "\n",
        "| Config | Active Components |\n",
        "|--------|------------------|\n",
        "| **A** | Raw MediaPipe landmarks + Gaussian assignment |\n",
        "| **B** | + Temporal filtering (Hampel / Interp / SavGol) |\n",
        "| **C** | + Max-distance gate (4œÉ rejection) |\n",
        "| **D** | + BiLSTM neural refinement |\n",
        "| **E** | + Constrained Viterbi decoding |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ 9.3  Ablation Study ‚îÄ‚îÄ\n",
        "print('Ablation Study ‚Äî Contribution of Each Pipeline Component')\n",
        "print(f'Sample: {sample.id}\\n')\n",
        "\n",
        "# Scale raw (unfiltered) landmarks to pixel space\n",
        "left_raw_px  = left_raw.copy();   left_raw_px[:, :, 0]  *= FRAME_W; left_raw_px[:, :, 1]  *= FRAME_H\n",
        "right_raw_px = right_raw.copy();  right_raw_px[:, :, 0] *= FRAME_W; right_raw_px[:, :, 1] *= FRAME_H\n",
        "\n",
        "# right_px / left_px were defined earlier (filtered + scaled to pixel space)\n",
        "\n",
        "ablation_configs = [\n",
        "    # (name, right_arr, left_arr, disable_gate, do_refine, use_viterbi)\n",
        "    ('A: Raw + Gaussian',                     right_raw_px, left_raw_px, True,  False, False),\n",
        "    ('B: + Temporal filtering',               right_px,     left_px,     True,  False, False),\n",
        "    ('C: + Max-distance gate (4sig)',         right_px,     left_px,     False, False, False),\n",
        "    ('D: + BiLSTM refinement',                right_px,     left_px,     False, True,  False),\n",
        "    ('E: + Constrained Viterbi',              right_px,     left_px,     False, True,  True),\n",
        "]\n",
        "\n",
        "ablation_results = []\n",
        "\n",
        "for name, r_arr, l_arr, disable_gate, do_refine, use_viterbi in ablation_configs:\n",
        "    # Build assigner\n",
        "    a = GaussianFingerAssigner(\n",
        "        key_boundaries=key_boundaries_px,\n",
        "        sigma=config.assignment.sigma,\n",
        "        candidate_range=config.assignment.candidate_keys\n",
        "    )\n",
        "    if disable_gate:\n",
        "        a.max_distance_px = 1e9        # effectively no rejection\n",
        "\n",
        "    # Assignment pass\n",
        "    asgns_abl = []\n",
        "    for ev in synced_events:\n",
        "        fidx, kidx = ev.frame_idx, ev.key_idx\n",
        "        if kidx not in a.key_centers:\n",
        "            continue\n",
        "        ar, al = None, None\n",
        "        if fidx < len(r_arr):\n",
        "            lm = r_arr[fidx]\n",
        "            if not np.any(np.isnan(lm)):\n",
        "                ar = a.assign_from_landmarks(lm, kidx, 'right', fidx, ev.onset_time)\n",
        "        if fidx < len(l_arr):\n",
        "            lm = l_arr[fidx]\n",
        "            if not np.any(np.isnan(lm)):\n",
        "                al = a.assign_from_landmarks(lm, kidx, 'left', fidx, ev.onset_time)\n",
        "        cands = [x for x in (ar, al) if x is not None]\n",
        "        if cands:\n",
        "            asgns_abl.append(max(cands, key=lambda x: x.confidence))\n",
        "\n",
        "    # Optionally refine\n",
        "    if do_refine and trained_model is not None and len(asgns_abl) > 0:\n",
        "        asgns_abl = refine_assignments(\n",
        "            trained_model, asgns_abl, feature_extractor, DEVICE,\n",
        "            use_constraints=use_viterbi\n",
        "        )\n",
        "\n",
        "    # IFR\n",
        "    if len(asgns_abl) >= 2:\n",
        "        ps  = [x.midi_pitch      for x in asgns_abl]\n",
        "        fs  = [x.assigned_finger for x in asgns_abl]\n",
        "        hs  = [x.hand            for x in asgns_abl]\n",
        "        viols = constraints.validate_sequence(fs, ps, hs)\n",
        "        ifr = len(viols) / (len(asgns_abl) - 1)\n",
        "    else:\n",
        "        ifr = float('nan')\n",
        "\n",
        "    ablation_results.append((name, len(asgns_abl), ifr))\n",
        "    print(f'  {name:42s}  notes={len(asgns_abl):>5d}  IFR={ifr:.4f}')\n",
        "\n",
        "# ‚îÄ‚îÄ Ablation bar chart ‚îÄ‚îÄ\n",
        "fig, ax = plt.subplots(figsize=(11, 5))\n",
        "labels_abl = [r[0] for r in ablation_results]\n",
        "ifrs_abl   = [r[2] for r in ablation_results]\n",
        "colors_abl = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4', '#9467bd']\n",
        "\n",
        "bars = ax.bar(range(len(ifrs_abl)), ifrs_abl,\n",
        "              color=colors_abl[:len(ifrs_abl)], edgecolor='white', linewidth=1.5)\n",
        "ax.set_xticks(range(len(ifrs_abl)))\n",
        "ax.set_xticklabels(labels_abl, rotation=30, ha='right', fontsize=9)\n",
        "ax.set_ylabel('IFR  (lower = better)')\n",
        "ax.set_title('Ablation Study ‚Äî Effect of Each Pipeline Component')\n",
        "\n",
        "for bar, v in zip(bars, ifrs_abl):\n",
        "    if not np.isnan(v):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
        "                f'{v:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚îÄ‚îÄ Summary ‚îÄ‚îÄ\n",
        "if len(ablation_results) >= 3:\n",
        "    a_ifr, b_ifr, c_ifr = [r[2] for r in ablation_results[:3]]\n",
        "    print(f'\\nComponent contributions (IFR reduction):')\n",
        "    if a_ifr > 0:\n",
        "        print(f'  Temporal filtering:     {a_ifr:.4f} ‚Üí {b_ifr:.4f}  '\n",
        "              f'({(a_ifr - b_ifr) / a_ifr * 100:+.1f}%)')\n",
        "        print(f'  Max-distance gate:      {b_ifr:.4f} ‚Üí {c_ifr:.4f}  '\n",
        "              f'({(b_ifr - c_ifr) / max(b_ifr, 1e-6) * 100:+.1f}%)')\n",
        "    if len(ablation_results) >= 5:\n",
        "        e_ifr = ablation_results[4][2]\n",
        "        if not np.isnan(e_ifr) and a_ifr > 0:\n",
        "            print(f'  Full pipeline (A‚ÜíE):    {a_ifr:.4f} ‚Üí {e_ifr:.4f}  '\n",
        "                  f'({(a_ifr - e_ifr) / a_ifr * 100:+.1f}% total)')\n",
        "    # Also note coverage changes\n",
        "    a_n, c_n = ablation_results[0][1], ablation_results[2][1]\n",
        "    if a_n != c_n:\n",
        "        print(f'\\n  Note: Max-distance gate reduced coverage from {a_n} to {c_n} notes '\n",
        "              f'({(a_n - c_n) / a_n * 100:.1f}% rejected as too far)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.4 Qualitative Analysis\n",
        "\n",
        "We overlay fingertip positions and predicted finger labels on actual video frames to visually assess the system.\n",
        "\n",
        "- **Coloured dots** mark each fingertip (1=red, 2=green, 3=blue, 4=purple, 5=cyan).\n",
        "- **Green rectangle** shows the auto-detected keyboard boundary.\n",
        "- For each frame we list the notes being played and the assigned finger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ 9.4  Qualitative Analysis ‚îÄ‚îÄ\n",
        "vp = VideoProcessor()\n",
        "vp.open(video_path)\n",
        "\n",
        "finger_colors_bgr = {1: (0,0,255), 2: (0,200,0), 3: (255,0,0), 4: (200,0,200), 5: (0,200,200)}\n",
        "finger_names_q     = {1: 'Thumb', 2: 'Index', 3: 'Middle', 4: 'Ring', 5: 'Pinky'}\n",
        "\n",
        "# Select 4 frames that have active assignments\n",
        "assigned_frames = sorted(set(a.frame_idx for a in assignments))\n",
        "n_qual = min(4, len(assigned_frames))\n",
        "qual_idxs = [assigned_frames[int(i)]\n",
        "             for i in np.linspace(0, len(assigned_frames) - 1, n_qual)]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "axes = axes.flat\n",
        "\n",
        "for i, fidx in enumerate(qual_idxs):\n",
        "    frame_bgr = vp.get_frame(int(fidx))\n",
        "    if frame_bgr is None:\n",
        "        continue\n",
        "    vis = frame_bgr.copy()\n",
        "    h, w = vis.shape[:2]\n",
        "\n",
        "    # Draw keyboard boundary\n",
        "    bx1, by1, bx2, by2 = keyboard_region.bbox\n",
        "    cv2.rectangle(vis, (bx1, by1), (bx2, by2), (0, 255, 0), 2)\n",
        "\n",
        "    # Draw hand skeletons + fingertip markers\n",
        "    connections = [\n",
        "        (0,1),(1,2),(2,3),(3,4),(0,5),(5,6),(6,7),(7,8),\n",
        "        (5,9),(9,10),(10,11),(11,12),(9,13),(13,14),(14,15),(15,16),\n",
        "        (13,17),(17,18),(18,19),(19,20),(0,17)\n",
        "    ]\n",
        "    for hand_key, arr, col_base in [('right', right_filtered, (0,255,0)),\n",
        "                                     ('left',  left_filtered,  (255,200,0))]:\n",
        "        fidx_int = int(fidx)\n",
        "        if fidx_int >= len(arr) or np.any(np.isnan(arr[fidx_int])):\n",
        "            continue\n",
        "        lm = arr[fidx_int]\n",
        "        for c1, c2 in connections:\n",
        "            p1 = (int(lm[c1, 0] * w), int(lm[c1, 1] * h))\n",
        "            p2 = (int(lm[c2, 0] * w), int(lm[c2, 1] * h))\n",
        "            cv2.line(vis, p1, p2, col_base, 1)\n",
        "        for f_num, tip_idx in [(1,4),(2,8),(3,12),(4,16),(5,20)]:\n",
        "            px = int(lm[tip_idx, 0] * w)\n",
        "            py = int(lm[tip_idx, 1] * h)\n",
        "            cv2.circle(vis, (px, py), 7, finger_colors_bgr[f_num], -1)\n",
        "            cv2.circle(vis, (px, py), 7, (255,255,255), 1)\n",
        "            cv2.putText(vis, str(f_num), (px+9, py-4),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), 1)\n",
        "\n",
        "    # Highlight assigned keys\n",
        "    frame_asgns = [a for a in assignments if a.frame_idx == int(fidx)]\n",
        "    for a in frame_asgns:\n",
        "        kb = key_boundaries_px.get(a.key_idx)\n",
        "        if kb:\n",
        "            kx1, ky1, kx2, ky2 = kb\n",
        "            cv2.rectangle(vis, (int(kx1), int(ky1)), (int(kx2), int(ky2)),\n",
        "                          finger_colors_bgr.get(a.assigned_finger, (255,255,0)), 2)\n",
        "\n",
        "    vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)\n",
        "    axes[i].imshow(vis_rgb)\n",
        "    title = f'Frame {int(fidx)} ‚Äî {len(frame_asgns)} note(s)'\n",
        "    if frame_asgns:\n",
        "        labels = [f'{a.hand[0].upper()}{a.assigned_finger}' for a in frame_asgns[:4]]\n",
        "        title += '\\n' + '  '.join(labels) + ('...' if len(frame_asgns) > 4 else '')\n",
        "    axes[i].set_title(title, fontsize=10)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "vp.close()\n",
        "plt.suptitle('Qualitative: Finger-Key Assignments on Video Frames\\n'\n",
        "             '(dots: 1=red 2=green 3=blue 4=purple 5=cyan  |  '\n",
        "             'green rect = keyboard  |  coloured key rect = assigned key)',\n",
        "             fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚îÄ‚îÄ Confidence Analysis ‚îÄ‚îÄ\n",
        "if assignments:\n",
        "    low_conf  = [a for a in assignments if a.confidence < 0.3]\n",
        "    high_conf = [a for a in assignments if a.confidence > 0.8]\n",
        "    print(f'Confidence breakdown ({len(assignments)} total assignments):')\n",
        "    print(f'  High confidence  (>0.8) : {len(high_conf):>5d}  '\n",
        "          f'({len(high_conf)/len(assignments)*100:.1f}%)')\n",
        "    print(f'  Low confidence   (<0.3) : {len(low_conf):>5d}  '\n",
        "          f'({len(low_conf)/len(assignments)*100:.1f}%)')\n",
        "\n",
        "    # Error analysis: when does the system struggle?\n",
        "    if low_conf:\n",
        "        print(f'\\n  Low-confidence examples (potential failure cases):')\n",
        "        for a in low_conf[:5]:\n",
        "            print(f'    Frame {a.frame_idx:>5d}  MIDI {a.midi_pitch}  '\n",
        "                  f'{a.hand:5s} {finger_names_q[a.assigned_finger]:6s}  '\n",
        "                  f'conf={a.confidence:.3f}')\n",
        "        print(f'\\n  Common causes of low confidence:')\n",
        "        print(f'    - Hand far from key (weak Gaussian signal)')\n",
        "        print(f'    - Fast motion / motion blur (noisy landmarks)')\n",
        "        print(f'    - Hand occlusion (fingers overlapping)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
